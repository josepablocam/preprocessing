[
  {
    "code": "def netgroup(base_dn, cn, triples=(), members=()):\n    attr_list = [('objectClass', [b'top', b'nisNetgroup'])]\n    if triples:\n        triples = [triple.encode('utf-8') for triple in triples]\n        attr_list.append(('nisNetgroupTriple', triples))\n    if members:\n        members = [member.encode('utf-8') for member in members]\n        attr_list.append(('memberNisNetgroup', members))\n    return (((('cn=' + cn) + ',ou=Netgroups,') + base_dn), attr_list)",
    "nl": "Generate an RFC2307bis netgroup add-modlist for passing to ldap.add*.",
    "original_nl": "Generate an RFC2307bis netgroup add-modlist for passing to ldap.add*."
  },
  {
    "code": "def testWells(self):\n    return self.test_case.wells()",
    "nl": "Will return a list of wells keys in the test case.",
    "original_nl": "Will return a list of wells keys in the test case."
  },
  {
    "code": "def do_action(self, action: Action):\n    action.do()\n    self._logging(action, 'Last action: ')\n    self._undo.append(action)\n    self._redo.clear()\n    self.action_done.emit(action)",
    "nl": "Execute the action, and add it the `undo` stack.\n\nWhen an action is executed:\n * is logged\n * is appended to the `undo` stack\n * the `redo` stack is cleared to maintain consistency\n * the `action_done` signal is emitted",
    "original_nl": "Execute the action, and add it the `undo` stack.\n\nWhen an action is executed:\n * is logged\n * is appended to the `undo` stack\n * the `redo` stack is cleared to maintain consistency\n * the `action_done` signal is emitted"
  },
  {
    "code": "def cleanup(self):\n    pass",
    "nl": "Backend cleanup. Is run by\n:class:`celery.task.DeleteExpiredTaskMetaTask`.",
    "original_nl": "Backend cleanup. Is run by\n:class:`celery.task.DeleteExpiredTaskMetaTask`."
  },
  {
    "code": "@staticmethod\ndef _search_cases(test_case_path, case_filter=None):\n    with open(test_case_path, 'r') as f:\n        raw_data = yaml.load(f)\n    test_cases = raw_data['test cases']\n    if case_filter:\n        for key in case_filter:\n            filtered_cases = []\n            for case in test_cases:\n                try:\n                    if isinstance(case[key], str):\n                        _value = case[key].lower()\n                    else:\n                        _value = case[key]\n                    if (_value in case_filter[key]):\n                        filtered_cases.append(case)\n                except KeyError:\n                    filtered_cases.append(case)\n            test_cases = filtered_cases\n    return test_cases",
    "nl": "For unit test case, we don't search for test functions.\nThe unit test cases is stored in a yaml file which is created in job build-idf-test.",
    "original_nl": "For unit test case, we don't search for test functions.\nThe unit test cases is stored in a yaml file which is created in job build-idf-test."
  },

  {
    "code": "def txp_bin_counter_callback(self, counter):\n    self.log.info('Mcast address %s packets %u', counter.mcast, counter.tx_packets[0])",
    "nl": "Counters callback.",
    "original_nl": "Counters callback."
  },
  {
    "code": "def predict(self, premise: str, hypothesis: str) -> JsonDict:\n    return self.predict_json({\n        'premise': premise,\n        'hypothesis': hypothesis,\n    })",
    "nl": "Predicts whether the hypothesis is entailed by the premise text.\n",
    "original_nl": "Predicts whether the hypothesis is entailed by the premise text.\n\nParameters\n----------\npremise : ``str``\n    A passage representing what is assumed to be true.\n\nhypothesis : ``str``\n    A sentence that may be entailed by the premise.\n\nReturns\n-------\nA dictionary where the key \"label_probs\" determines the probabilities of each of\n[entailment, contradiction, neutral]."
  },
  {
    "code": "def isSameObject(self, obj1, obj2):\n    if (obj1 == obj2):\n        return True\n    elif ((not obj1) or (not obj2)):\n        return False\n    elif ((obj1.name != obj2.name) or (obj1.childCount != obj2.childCount)):\n        return False\n    if (obj1.getRole() == obj2.getRole() == pyatspi.ROLE_LABEL):\n        try:\n            ext1 = obj1.queryComponent().getExtents(0)\n            ext2 = obj2.queryComponent().getExtents(0)\n        except:\n            pass\n        else:\n            if ((ext1.x == ext2.x) and (ext1.y == ext2.y) and (ext1.width == ext2.width) and (ext1.height == ext2.height)):\n                return True\n    try:\n        parent1 = obj1\n        parent2 = obj2\n        while (parent1 and parent2 and (parent1.getRole() == pyatspi.ROLE_LABEL) and (parent2.getRole() == pyatspi.ROLE_LABEL)):\n            if (parent1.getIndexInParent() != parent2.getIndexInParent()):\n                return False\n            parent1 = parent1.parent\n            parent2 = parent2.parent\n        if (parent1 and parent2 and (parent1 == parent2)):\n            return True\n    except:\n        pass\n    return script_utilities.Utilities.isSameObject(self, obj1, obj2)",
    "nl": "Compares two objects to determine if they are functionally\nthe same object. This is needed because some applications and\ntoolkits kill and replace accessibles.",
    "original_nl": "Compares two objects to determine if they are functionally\nthe same object. This is needed because some applications and\ntoolkits kill and replace accessibles."
  },
  {
    "code": "def _GenerateColLoadAggregateStore(emitter, registers, lanes_count, elements_count, aggregators, input_address, stride, output):\n    emitter.EmitNewline()\n    emitter.EmitComment(('Load Aggregate Store - column major %dx%d' % (lanes_count, elements_count)))\n    block = [registers.DoubleRegister() for unused_i in range(lanes_count)]\n    if (elements_count is not 8):\n        _GenerateClear(emitter, 'i8', block)\n    block = emitter.EmitLoadColBlock(registers, 8, lanes_count, elements_count, block, input_address, stride)\n    for (aggregator, row) in zip(aggregators, block):\n        emitter.EmitVAddw('u8', aggregator, aggregator, row)\n    emitter.EmitVStoreAE(8, (8 * lanes_count), block, output, _AlignForLanes(lanes_count))\n    registers.FreeRegisters(block)",
    "nl": "Emit inner loop code for reading N col lanes and interweaving them.",
    "original_nl": "Emit inner loop code for reading N col lanes and interweaving them."
  },
  {
    "code": "def check(**kwargs):\n    jdata = kwargs['jdata']\n    logger = kwargs['logger']\n    env.gateway = jdata['data']['gateway']\n    env.host_string = jdata['data']['host_string']\n    env.user = jdata['data']['username']\n    env.key = jdata['data']['sshkey']\n    env.shell = '/bin/sh -c'\n    env.disable_known_hosts = True\n    env.warn_only = True\n    env.abort_on_prompts = True\n    results = run_cmd('uname -a')\n    if results.succeeded:\n        if ('FreeBSD' in results):\n            cmd = 'vmstat 2 2'\n            results = run_cmd(cmd)\n            if results.succeeded:\n                lines = results.splitlines()\n                vmstat_info = lines[(- 1)].split()\n                cpu_idle = float(vmstat_info[(- 1)])\n            else:\n                return None\n        else:\n            cmd = 'vmstat 2 2'\n            results = run_cmd(cmd)\n            if results.succeeded:\n                lines = results.splitlines()\n                vmstat_info = lines[(- 1)].split()\n                cpu_idle = float(vmstat_info[(- 3)])\n            else:\n                return None\n    else:\n        return None\n    threshold = float(jdata['data']['threshold'])\n    logger.debug('cpu-idle: Idle {0} Threshold {1}'.format(cpu_idle, threshold))\n    if (cpu_idle > threshold):\n        return True\n    else:\n        return False",
    "nl": "Login over SSH and execute shell command",
    "original_nl": "Login over SSH and execute shell command"
  },
  {
    "code": "@skipUnless((_ssh.ssh_version >= (6, 5)), 'ED25519 not available in OpenSSH < 6.5')\ndef test_18(self):\n    with tempfile.NamedTemporaryFile() as tmp:\n        filename = tmp.name\n    obj = CbSSHKeygen()\n    obj.algorithm = 'ed25519'\n    obj.keylength = 256\n    obj.keygen(filename=filename, passphrase='secret')\n    self.assertTrue((os.path.isfile(filename) and os.path.isfile('.'.join((filename, 'pub')))))\n    self.assertEqual(obj.return_code, os.EX_OK)\n    os.unlink(filename)\n    os.unlink('.'.join((filename, 'pub')))",
    "nl": "Test Case 18:\nTry creating a public/private key pair using the ED25519 algorithm.\n\nTest is passed if two files, ``filename`` and ``filename.pub`` are created during the test and the",
    "original_nl": "Test Case 18:\nTry creating a public/private key pair using the ED25519 algorithm.\n\nTest is passed if two files, ``filename`` and ``filename.pub`` are created during the test and the\nreturn value of ssh-keygen is zero."
  },


  {
    "code": "def parse(self, html):\n    html = self.__fix_html(html)\n    self.reset()\n    try:\n        self.feed(html)\n        self.close()\n    finally:\n        for tag in self.__tag_stack[1:]:\n            self.__close_tag(tag, True)",
    "nl": "Parses the specified HTML page.",
    "original_nl": "Parses the specified HTML page."
  },
  {
    "code": "def testGetFormatStringAttributeNames(self):\n    event_formatter = windows.WindowsVolumeCreationEventFormatter()\n    expected_attribute_names = ['device_path', 'serial_number', 'origin']\n    self._TestGetFormatStringAttributeNames(event_formatter, expected_attribute_names)",
    "nl": "Tests the GetFormatStringAttributeNames function.",
    "original_nl": "Tests the GetFormatStringAttributeNames function."
  },
  {
    "code": "def op(name, audio, sample_rate, labels=None, max_outputs=3, encoding=None, display_name=None, description=None, collections=None):\n    if (display_name is None):\n        display_name = name\n    if (encoding is None):\n        encoding = 'wav'\n    if (encoding == 'wav'):\n        encoding = metadata.Encoding.Value('WAV')\n        encoder = functools.partial(tf.contrib.ffmpeg.encode_audio, samples_per_second=sample_rate, file_format='wav')\n    else:\n        raise ValueError(('Unknown encoding: %r' % encoding))\n    with tf.name_scope(name), tf.control_dependencies([tf.assert_rank(audio, 3)]):\n        limited_audio = audio[:max_outputs]\n        encoded_audio = tf.map_fn(encoder, limited_audio, dtype=tf.string, name='encode_each_audio')\n        if (labels is None):\n            limited_labels = tf.tile([''], tf.shape(limited_audio)[:1])\n        else:\n            limited_labels = labels[:max_outputs]\n        tensor = tf.transpose(tf.stack([encoded_audio, limited_labels]))\n        summary_metadata = metadata.create_summary_metadata(display_name=display_name, description=description, encoding=encoding)\n        return tf.summary.tensor_summary(name='audio_summary', tensor=tensor, collections=collections, summary_metadata=summary_metadata)",
    "nl": "Create an audio summary op for use in a TensorFlow graph.\n\nArguments:\n  name: A unique name for the generated summary node.\n  audio: A `Tensor` representing audio data with shape `[k, t, c]`,\n    where `k` is the number of audio clips, `t` is the number of\n    frames, and `c` is the number of channels. Elements should be\n    floating-point values in `[-1.0, 1.0]`. Any of the dimensions may\n    be statically unknown (i.e., `None`).\n  sample_rate: An `int` or rank-0 `int32` `Tensor` that represents the\n    sample rate, in Hz. Must be positive.\n  labels: Optional `string` `Tensor`, a vector whose length is the\n    first dimension of `audio`, where `labels[i]` contains arbitrary\n    textual information about `audio[i]`. (For instance, this could be\n    some text that a TTS system was supposed to produce.) Markdown is\n    supported. Contents should be UTF-8.\n  max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n    many audio clips will be emitted at each step. When more than\n    `max_outputs` many clips are provided, the first `max_outputs`\n    many clips will be used and the rest silently discarded.\n  encoding: A constant `str` (not string tensor) indicating the\n    desired encoding. You can choose any format you like, as long as\n    it's \"wav\". Please see the \"API compatibility note\" below.\n  display_name: Optional name for this summary in TensorBoard, as a\n    constant `str`. Defaults to `name`.\n  description: Optional long-form description for this summary, as a\n    constant `str`. Markdown is supported. Defaults to empty.\n  collections: Optional list of graph collections keys. The new\n    summary op is added to these collections. Defaults to\n    `[Graph Keys.SUMMARIES]`.\n",
    "original_nl": "Create an audio summary op for use in a TensorFlow graph.\n\nArguments:\n  name: A unique name for the generated summary node.\n  audio: A `Tensor` representing audio data with shape `[k, t, c]`,\n    where `k` is the number of audio clips, `t` is the number of\n    frames, and `c` is the number of channels. Elements should be\n    floating-point values in `[-1.0, 1.0]`. Any of the dimensions may\n    be statically unknown (i.e., `None`).\n  sample_rate: An `int` or rank-0 `int32` `Tensor` that represents the\n    sample rate, in Hz. Must be positive.\n  labels: Optional `string` `Tensor`, a vector whose length is the\n    first dimension of `audio`, where `labels[i]` contains arbitrary\n    textual information about `audio[i]`. (For instance, this could be\n    some text that a TTS system was supposed to produce.) Markdown is\n    supported. Contents should be UTF-8.\n  max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n    many audio clips will be emitted at each step. When more than\n    `max_outputs` many clips are provided, the first `max_outputs`\n    many clips will be used and the rest silently discarded.\n  encoding: A constant `str` (not string tensor) indicating the\n    desired encoding. You can choose any format you like, as long as\n    it's \"wav\". Please see the \"API compatibility note\" below.\n  display_name: Optional name for this summary in TensorBoard, as a\n    constant `str`. Defaults to `name`.\n  description: Optional long-form description for this summary, as a\n    constant `str`. Markdown is supported. Defaults to empty.\n  collections: Optional list of graph collections keys. The new\n    summary op is added to these collections. Defaults to\n    `[Graph Keys.SUMMARIES]`.\n\nReturns:\n  A TensorFlow summary op.\n\nAPI compatibility note: The default value of the `encoding`\nargument is _not_ guaranteed to remain unchanged across TensorBoard\nversions. In the future, we will by default encode as FLAC instead of\nas WAV. If the specific format is important to you, please provide a\nfile format explicitly."
  },
  {
    "code": "def visit_For(self, node):\n    newnode = node\n    if (node in self.analysis.loopsToBeConverted):\n        newbody = ([ast.Assign(targets=[node.target], value=ast.Yield(value=None))] + node.body)\n        whileNode = ast.While(test=ast.Name(id=self.moreValuesAvailableId, ctx=ast.Load()), body=newbody, orelse=[])\n        newnode = self._tryExceptGeneratorExit([whileNode], [self._moreValuesAvailableAssignmentNode('False')])\n    self.generic_visit(newnode)\n    return ast.copy_location(newnode, node)",
    "nl": "Change iteration into while-yield statements",
    "original_nl": "Change iteration into while-yield statements"
  },
  {
    "code": "def CompileFilter(self, filter_expression):\n    if (not os.path.isfile(filter_expression)):\n        raise errors.WrongPlugin('ObjectFilterList requires an YAML file to be passed on, this filter string is not a file.')\n    yaml.add_constructor('!include', self._IncludeKeyword, Loader=yaml.loader.SafeLoader)\n    results = None\n    with open(filter_expression, 'rb') as file_object:\n        try:\n            results = yaml.safe_load(file_object)\n        except (yaml.scanner.ScannerError, IOError) as exception:\n            raise errors.WrongPlugin('Unable to parse YAML file with error: {0!s}.'.format(exception))\n    self.filters = []\n    results_type = type(results)\n    if (results_type is dict):\n        self._ParseEntry(results)\n    elif (results_type is list):\n        for result in results:\n            if (not isinstance(result, dict)):\n                raise errors.WrongPlugin('Wrong format of YAML file, entry not a dict ({0:s})'.format(results_type))\n            self._ParseEntry(result)\n    else:\n        raise errors.WrongPlugin('Wrong format of YAML file, entry not a dict ({0:s})'.format(results_type))\n    self._filter_expression = filter_expression",
    "nl": "Compiles the filter expression.\n\nThe filter expression contains the name of a YAML file.\n\nArgs:\n  filter_expression: string that contains the filter expression.\n\nRaises:\n  WrongPlugin: if the filter could not be compiled.",
    "original_nl": "Compiles the filter expression.\n\nThe filter expression contains the name of a YAML file.\n\nArgs:\n  filter_expression: string that contains the filter expression.\n\nRaises:\n  WrongPlugin: if the filter could not be compiled."
  },
  {
    "code": "@login_required\ndef delete(request, obj_id=None):\n    data = (request.DELETE or json.loads(request.body))\n    guids = data.get('guids').split(',')\n    objects = getObjectsFromGuids(guids)\n    gallery = Gallery.objects.get(pk=obj_id)\n    LOGGER.info('{} removed {} from {}'.format(request.user.email, guids, gallery))\n    for o in objects:\n        if isinstance(o, Image):\n            gallery.images.remove(o)\n        elif isinstance(o, Video):\n            gallery.videos.remove(o)\n    res = Result()\n    return JsonResponse(res.asDict())",
    "nl": "Removes ImageVideo objects from Gallery",
    "original_nl": "Removes ImageVideo objects from Gallery"
  },
  {
    "code": "def count(self):\n    (resp, page) = self.request('GET', self.uri)\n    return page['total']",
    "nl": "Return the number of instance resources contained in this list resource",
    "original_nl": "Return the number of instance resources contained in this list resource"
  },
  {
    "code": "def test_get_logout_screen(self):\n    url = (reverse('django-pam:logout') + '?next=home-page')\n    response = self.client.get(url)\n    msg = 'response status: {}, should be 302'.format(response.status_code)\n    self.assertEqual(response.status_code, 302, msg)\n    self._login_form()\n    url = (reverse('django-pam:logout') + '?next=home-page')\n    response = self.client.get(url)\n    msg = 'response status: {}, should be 200'.format(response.status_code)\n    self.assertEqual(response.status_code, 200, msg)\n    content = response.content.decode('utf-8')\n    msg = 'content: {}'.format(content)\n    self.assertTrue(('csrfmiddlewaretoken' in content), msg)\n    self.assertTrue(('next' in content), msg)",
    "nl": "Test that the logout screen returns properly.",
    "original_nl": "Test that the logout screen returns properly."
  },
  {
    "code": "def getDistDir(packageDir):\n    if isfile(join(packageDir, 'setup.cfg')):\n        p = configparser.ConfigParser()\n        p.read(join(packageDir, 'setup.cfg'))\n        return normpath(join(packageDir, p.get('bdist_egg', 'dist_dir')))\n    else:\n        return dirname(script)",
    "nl": "Return the distribution directory corresponding to the given package.",
    "original_nl": "Return the distribution directory corresponding to the given package."
  },
  {
    "code": "def print_formatted(query_result):\n    for (cluster, cluster_usage) in query_result['clusters'].items():\n        if ('usage' in cluster_usage):\n            usage_map = cluster_usage['usage']\n            share_map = cluster_usage['share']\n            print_info(colors.bold(cluster))\n            print_info(format_share(share_map))\n            print_info(format_usage(usage_map))\n            applications = cluster_usage['applications']\n            if applications:\n                print_info('Applications:')\n            else:\n                print_info(colors.waiting('Nothing Running'))\n            for (application, application_usage) in applications.items():\n                usage_map = application_usage['usage']\n                print_info(f\"- {colors.running((application if application else '[no application defined]'))}\")\n                print_info(f'  {format_usage(usage_map)}')\n                print_info('  Job Groups:')\n                for (group, group_usage) in application_usage['groups'].items():\n                    usage_map = group_usage['usage']\n                    jobs = group_usage['jobs']\n                    print_info(f\"\t- {colors.bold((group if group else '[ungrouped]'))}\")\n                    print_info(f'\t  {format_usage(usage_map)}')\n                    print_info(f'\t  Jobs: {len(jobs)}')\n                    print_info('')\n            print_info('')",
    "nl": "Prints the query result as a hierarchical set of bullets",
    "original_nl": "Prints the query result as a hierarchical set of bullets"
  },
  {
    "code": "@abstractmethod\ndef create_margin(self, window_render_info, width, height):\n    return []",
    "nl": "Creates a margin.\nThis should return a list of (style_str, text) tuples.\n\n",
    "original_nl": "Creates a margin.\nThis should return a list of (style_str, text) tuples.\n\n:param window_render_info:\n    :class:`~prompt_toolkit.layout.containers.WindowRenderInfo`\n    instance, generated after rendering and copying the visible part of\n    the :class:`~prompt_toolkit.layout.controls.UIControl` into the\n    :class:`~prompt_toolkit.layout.containers.Window`.\n:param width: The width that's available for this margin. (As reported\n    by :meth:`.get_width`.)\n:param height: The height that's available for this margin. (The height\n    of the :class:`~prompt_toolkit.layout.containers.Window`.)"
  },
  {
    "code": "def post_save_update_cache(sender, instance, created, raw, **kwargs):\n    if raw:\n        return\n    name = sender.__name__\n    if ((name == 'User') and created):\n        return\n    delay_cache = getattr(instance, '_delay_cache', False)\n    if (not delay_cache):\n        update_cache_for_instance(name, instance.pk, instance)",
    "nl": "Invalidate the cache when an instance is created or updated.",
    "original_nl": "Invalidate the cache when an instance is created or updated."
  },
  {
    "code": "def get_addresses(self):\n    return self.http_get('/wallet/addresses').get('addresses')",
    "nl": "Returns a list of addresses from the wallet.",
    "original_nl": "Returns a list of addresses from the wallet."
  },
  {
    "code": "def test_filter_assign():\n\n    class TestFilter(Filter):\n        pass\n\n    def _assert(list, length):\n        'Confirm that everything in the list is a filter instance, and\\n        that the list as the required length.'\n        assert (len(list) == length)\n        assert bool([f for f in list if isinstance(f, Filter)])\n    b = Bundle(filters='jsmin,cssutils')\n    _assert(b.filters, 2)\n    b = Bundle(filters=['jsmin', 'cssutils'])\n    _assert(b.filters, 2)\n    assert_raises(ValueError, Bundle, filters=['jsmin,cssutils'])\n    b = Bundle(filters=TestFilter)\n    _assert(b.filters, 1)\n    b = Bundle(filters=[TestFilter, TestFilter, TestFilter])\n    _assert(b.filters, 3)\n    b = Bundle(filters=TestFilter())\n    _assert(b.filters, 1)\n    b = Bundle(filters=[TestFilter(), TestFilter(), TestFilter()])\n    _assert(b.filters, 3)\n    b = Bundle(filters=[TestFilter, TestFilter()])\n    _assert(b.filters, 2)\n    assert_raises(ValueError, Bundle, filters='notreallyafilter')\n    assert_raises(ValueError, Bundle, filters=object())\n    Bundle().filters = None\n    b = Bundle()\n    assert (b.filters is None)\n    b.filters = TestFilter\n    _assert(b.filters, 1)\n    old_filters = b.filters\n    b.filters = b.filters\n    assert (b.filters == old_filters)",
    "nl": "Test the different ways we can assign filters to the bundle.",
    "original_nl": "Test the different ways we can assign filters to the bundle."
  },
  {
    "code": "def wait_until(pred, timeout=30, interval=5):\n    if timeout:\n        finish = (datetime.now() + timedelta(seconds=timeout))\n    else:\n        finish = None\n    while True:\n        result = pred()\n        if (result or quit_running):\n            break\n        if (finish and (datetime.now() >= finish)):\n            break\n        time.sleep(interval)\n    return result",
    "nl": "Wait, retrying a predicate until it is True, or the \ntimeout value has been exceeded.",
    "original_nl": "Wait, retrying a predicate until it is True, or the \ntimeout value has been exceeded."
  },

  {
    "code": "def North_East_vectors_target(ra_xallarap, dec_xallarap):\n    target_angles_in_the_sky = [((ra_xallarap * np.pi) / 180), ((dec_xallarap * np.pi) / 180)]\n    source_center_of_mass = np.array([(np.cos(target_angles_in_the_sky[1]) * np.cos(target_angles_in_the_sky[0])), (np.cos(target_angles_in_the_sky[1]) * np.sin(target_angles_in_the_sky[0])), np.sin(target_angles_in_the_sky[1])])\n    East = np.array([(- np.sin(target_angles_in_the_sky[0])), np.cos(target_angles_in_the_sky[0]), 0.0])\n    North = np.cross(source_center_of_mass, East)\n    return (North, East)",
    "nl": "This function define the North and East vectors projected on the sky plane\nperpendicular to the line\nof sight (i.e the line define by ra,dec of the center of mass of the source binary).\n\n",
    "original_nl": "This function define the North and East vectors projected on the sky plane\nperpendicular to the line\nof sight (i.e the line define by ra,dec of the center of mass of the source binary).\n\n:param float ra_xallarap : the right acsension of the source binary center of mass in degree.\n:param float dec_xallarap : the declinaision of the source binary center of mass in degree.\n:return: the North and East vectors projected in the sky plan for this trajectory\n:rtype: array_like, array_like"
  },

  {
    "code": "def test_get_licenses():\n    expected_matches = [('mit', 'mit'), ('mti', 'mit'), ('MIT', 'mit'), ('bit', 'mit'), ('nit', 'mit'), ('ics', 'isc'), ('isc', 'isc'), ('allrightsreserved', 'all-rights-reserved'), ('gpl2', 'gpl-2.0'), ('gpl-2', 'gpl-2.0'), ('apache', 'apache-2.0'), ('apache-2', 'apache-2.0'), ('mpl-2', 'mpl-2.0'), ('bsd2clause', 'bsd-2-clause'), ('agpl3', 'agpl-3.0'), ('agpl', 'agpl-3.0')]\n    for pair in expected_matches:\n        match = licenser.get_license(pair[0])\n        assert (match == pair[1]), '{a} != {b}, test: {pair}'.format(a=match, b=pair[1], pair=pair)",
    "nl": "calls get_license to test matching functionality",
    "original_nl": "calls get_license to test matching functionality"
  },
  {
    "code": "def resident(since=0):\n    return (_VmB('VmRSS:') - since)",
    "nl": "Return resident memory usage in kilobytes.",
    "original_nl": "Return resident memory usage in kilobytes."
  },
  {
    "code": "def reset_status(self, cluster):\n    body = {\n        'reset-status': {\n            \n        },\n    }\n    self._action(cluster, body)",
    "nl": "Reset the status of a cluster\n\n",
    "original_nl": "Reset the status of a cluster\n\n:param cluster: The cluster to reset"
  },
  {
    "code": "def readSubjectivityClues(postag, datafile=None):\n    A = {\n        \n    }\n    if (datafile == None):\n        return None\n    if (postag == 'a'):\n        posname = 'adj'\n    elif (postag == 'v'):\n        posname = 'verb'\n    elif (postag == 'n'):\n        posname = 'noun'\n    else:\n        posname = 'anypos'\n    Wfile = open(datafile, 'r', 1024000)\n    for line in Wfile:\n        entry = line.split()\n        posval = 0\n        negval = 0\n        pos_type = entry[3].split('=')[1]\n        polarity = entry[5].split('=')[1]\n        term = entry[2].split('=')[1]\n        if (polarity == 'negative'):\n            posval = 0\n            negval = 1\n        elif (polarity == 'positive'):\n            posval = 1\n            negval = 0\n        if (((pos_type == posname) or (pos_type == 'anypos')) and (polarity != 'neutral')):\n            if (term not in A):\n                A[term] = []\n            A[term].append((term, posval, negval))\n    return A",
    "nl": "Reads Wiebe's subjectivity clues into a dictionary\n\nTypical line read from file:\ntype=weaksubj len=1 word1=wrestle pos1=verb stemmed1=y priorpolarity=negative",
    "original_nl": "Reads Wiebe's subjectivity clues into a dictionary\n\nTypical line read from file:\ntype=weaksubj len=1 word1=wrestle pos1=verb stemmed1=y priorpolarity=negative"
  },
  {
    "code": "def cross_validation(model, horizon, period=None, initial=None):\n    te = model.history['ds'].max()\n    ts = model.history['ds'].min()\n    horizon = pd.Timedelta(horizon)\n    period = ((0.5 * horizon) if (period is None) else pd.Timedelta(period))\n    initial = ((3 * horizon) if (initial is None) else pd.Timedelta(initial))\n    k = int(np.ceil((((te - horizon) - (ts + initial)) / period)))\n    if (k < 1):\n        raise ValueError('Not enough data for specified horizon, period, and initial.')\n    return simulated_historical_forecasts(model, horizon, k, period)",
    "nl": "Cross-Validation for time series.\n\nComputes forecasts from historical cutoff points. Beginning from initial,\nmakes cutoffs with a spacing of period up to (end - horizon).\n\nWhen period is equal to the time interval of the data, this is the\ntechnique described in https://robjhyndman.com/hyndsight/tscv/ .\n",
    "original_nl": "Cross-Validation for time series.\n\nComputes forecasts from historical cutoff points. Beginning from initial,\nmakes cutoffs with a spacing of period up to (end - horizon).\n\nWhen period is equal to the time interval of the data, this is the\ntechnique described in https://robjhyndman.com/hyndsight/tscv/ .\n\nParameters\n----------\nmodel: Prophet class object. Fitted Prophet model\nhorizon: string with pd.Timedelta compatible style, e.g., '5 days',\n    '3 hours', '10 seconds'.\nperiod: string with pd.Timedelta compatible style. Simulated forecast will\n    be done at every this period. If not provided, 0.5 * horizon is used.\ninitial: string with pd.Timedelta compatible style. The first training\n    period will begin here. If not provided, 3 * horizon is used.\n\nReturns\n-------\nA pd.DataFrame with the forecast, actual value and cutoff."
  },
  {
    "code": "def _testXDailyDownload(self):\n    self.data_dir = 'c:/temp/_daily_test_data/'\n    catalog = IceCat.IceCatCatalog(log=self.log, data_dir=self.data_dir, auth=self.auth)\n    detail_keys = ['ProductDescription[@LongDesc]', 'ShortSummaryDescription', 'LongSummaryDescription', 'ProductDescription[@ShortDesc]']\n    catalog.add_product_details_parallel(keys=detail_keys, connections=100)\n    file = 'test.large.json'\n    if os.path.exists(file):\n        os.remove(file)\n    catalog.dump_to_file(file)\n    self.assertEqual(os.path.isfile(file), True)",
    "nl": "load all data files from Ice Cat. \nthis test check that data is parsed correctly to the dictionary\nproduct details are tested. parallel download tested\nit's normal for this test to run long, several minutes",
    "original_nl": "load all data files from Ice Cat. \nthis test check that data is parsed correctly to the dictionary\nproduct details are tested. parallel download tested\nit's normal for this test to run long, several minutes"
  },
  {
    "code": "def Main():\n    argument_parser = argparse.ArgumentParser(description='Extracts information from timezone information files.')\n    argument_parser.add_argument('-d', '--debug', dest='debug', action='store_true', default=False, help='enable debug output.')\n    argument_parser.add_argument('source', nargs='?', action='store', metavar='PATH', default=None, help='path of the timezone information file.')\n    options = argument_parser.parse_args()\n    if (not options.source):\n        print('Source file missing.')\n        print('')\n        argument_parser.print_help()\n        print('')\n        return False\n    logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n    output_writer = output_writers.StdoutWriter()\n    try:\n        output_writer.Open()\n    except IOError as exception:\n        print('Unable to open output writer with error: {0!s}'.format(exception))\n        print('')\n        return False\n    tzif_file = tzif.TimeZoneInformationFile(debug=options.debug, output_writer=output_writer)\n    tzif_file.Open(options.source)\n    output_writer.WriteText('Timezone information:\\n')\n    tzif_file.Close()\n    output_writer.Close()\n    return True",
    "nl": "The main program function.\n",
    "original_nl": "The main program function.\n\nReturns:\n  bool: True if successful or False if not."
  },
  {
    "code": "def defuse_xml_libs():\n    from defusedxml import defuse_stdlib\n    defuse_stdlib()\n    import lxml\n    import lxml.etree\n    from . import etree as safe_etree\n    lxml.etree = safe_etree",
    "nl": "Monkey patch and defuse all stdlib xml packages and lxml.",
    "original_nl": "Monkey patch and defuse all stdlib xml packages and lxml."
  },
  {
    "code": "def getTempF(self):\n    tempC = self.getTemp()\n    return (((tempC * 9.0) / 5) + 32)",
    "nl": "Reads and returns the current ambient temperature in fahrenheit\n\nReceived value is checked against a CRC and an AssertionError is thrown if \nit is invalid.",
    "original_nl": "Reads and returns the current ambient temperature in fahrenheit\n\nReceived value is checked against a CRC and an AssertionError is thrown if \nit is invalid."
  },
  {
    "code": "def cmd_status(args):\n    opts = {\n        'bynode': '-n',\n        'inactive': '-r',\n        'ops': '-o',\n        'timing': '-t',\n        'failcounts': '-f',\n        'verbose': '-V',\n        'quiet': '-Q',\n        'html': '--as-html',\n        'xml': '--as-xml',\n        'simple': '-s',\n        'tickets': '-c',\n        'noheaders': '-D',\n        'detail': '-R',\n        'brief': '-b',\n        'full': '-ncrft',\n    }\n    extra = ' '.join((opts.get(arg, arg) for arg in args))\n    if (not args):\n        extra = '-r'\n    (rc, s) = crm_mon(extra)\n    if (rc != 0):\n        raise IOError(('crm_mon (rc=%d): %s' % (rc, s)))\n    utils.page_string(CrmMonFilter()(s))\n    return True",
    "nl": "Calls crm_mon -1, passing optional extra arguments.\nDisplays the output, paging if necessary.\nRaises IOError if crm_mon fails.",
    "original_nl": "Calls crm_mon -1, passing optional extra arguments.\nDisplays the output, paging if necessary.\nRaises IOError if crm_mon fails."
  },
  {
    "code": "def handle_sigint(self, signum, frame):\n    self.finish()\n    self.original_handler(signum, frame)",
    "nl": "Call self.finish() before delegating to the original SIGINT handler.\n\nThis handler should only be in place while the progress display is\nactive.",
    "original_nl": "Call self.finish() before delegating to the original SIGINT handler.\n\nThis handler should only be in place while the progress display is\nactive."
  },
  {
    "code": "def test_optionsInvalidArguments(self):\n    self.patchExit()\n    Twist.options(['twist', '--bogus-bagels'])\n    self.assertIdentical(self.exit.status, ExitStatus.EX_USAGE)\n    self.assertTrue(self.exit.message.startswith('Error: '))\n    self.assertTrue(self.exit.message.endswith('\\n\\n{}'.format(TwistOptions())))",
    "nl": "L{Twist.options} given invalid arguments exits with\nL{ExitStatus.EX_USAGE} and an error/usage message.",
    "original_nl": "L{Twist.options} given invalid arguments exits with\nL{ExitStatus.EX_USAGE} and an error/usage message."
  },
  {
    "code": "def test_success_test_method(self):\n    self._run_test_class(testcases.SuccessfulTestCase, 'successful_test_case')",
    "nl": "Check the XML output of a test class with a successful test method.",
    "original_nl": "Check the XML output of a test class with a successful test method."
  },
  {
    "code": "@sigint.setter\ndef sigint(self, handler):\n    self._sigint = _normalize_handler(handler, self._default_handler)",
    "nl": "Normalizes and sets sigint.",
    "original_nl": "Normalizes and sets sigint."
  },
  {
    "code": "def test_init_seed_bad_genome():\n    np.random.seed(4303423)\n    seed_genome = np.random.randint(0, 256, 10000)\n    seed_genome[0:2] = np.array([42, 213])\n    seed_genome[(- 10):(- 8)] = np.array([42, 213])\n    test_mn = MarkovNetwork(num_input_states=4, num_memory_states=5, num_output_states=6, probabilistic=False, genome=seed_genome)\n    assert np.all((test_mn.genome == seed_genome))\n    assert (len(test_mn.markov_gates) == 1)",
    "nl": "MarkovNetwork initializer with bad seeded genome",
    "original_nl": "MarkovNetwork initializer with bad seeded genome"
  },

  {
    "code": "def get_nicklist(self):\n    if (not self.nicklist):\n        self.sendLine(('NAMES %s' % self.channel))",
    "nl": "Retrieve name list from the channel. The return\nis handled by the catch methods below.",
    "original_nl": "Retrieve name list from the channel. The return\nis handled by the catch methods below."
  },
  {
    "code": "def tags(self):\n    for item in self._prop_list:\n        (yield Tag(item))",
    "nl": "Get a generator of Tag within the TagsCollectionPage\n\nYields:\n    :class:`Tag<onedrivesdk.model.tag.Tag>`:\n        The next Tag in the collection",
    "original_nl": "Get a generator of Tag within the TagsCollectionPage\n\nYields:\n    :class:`Tag<onedrivesdk.model.tag.Tag>`:\n        The next Tag in the collection"
  },
  {
    "code": "def test_discover_depth_treantdepth(tmpdir):\n    with tmpdir.as_cwd():\n        ghosts = ('inky', 'inky/blinky', 'pinky', 'inky/blinky/nothing/clyde')\n        for name in ghosts:\n            dtr.Treant(name)\n        assert (len(discover('.', treantdepth=0, depth=0)) == 0)\n        assert (len(discover('.', treantdepth=0, depth=1)) == 2)\n        assert (len(discover('pinky', treantdepth=0, depth=0)) == 1)\n        assert (len(discover('inky', treantdepth=0, depth=2)) == 1)\n        assert (len(discover('.', treantdepth=1, depth=1)) == 2)\n        assert (len(discover('inky', treantdepth=1, depth=1)) == 2)\n        assert (len(discover('inky', treantdepth=1, depth=0)) == 1)\n        assert (len(discover('inky', treantdepth=2)) == 3)\n        assert (len(discover('inky', treantdepth=2, depth=2)) == 2)\n        assert (len(discover('inky', treantdepth=2, depth=3)) == 3)",
    "nl": "Check that using `treantdepth` and `depth` parameters together gives\nexpected result.",
    "original_nl": "Check that using `treantdepth` and `depth` parameters together gives\nexpected result."
  },
  {
    "code": "def useNotifyByWriteFile(fileObj: TextIO=None, prefix: str=None, publisher: Publisher=None, all: bool=True, **kwargs):\n    notifHandler = NotifyByWriteFile(fileObj, prefix)\n    if (publisher is None):\n        from .. import pub\n        publisher = pub.getDefaultPublisher()\n    publisher.addNotificationHandler(notifHandler)\n    publisher.setNotificationFlags(all=all, **kwargs)",
    "nl": "Will cause all pubsub notifications of pubsub \"actions\" (such as new topic created, message sent, listener died\netc) to be written to specified file (or stdout if none given). The fileObj need only provide a 'write(string)'\nmethod.\n\nThe first two arguments are the same as those of NotifyByWriteFile constructor. The 'all' and kwargs arguments\nare those of pubsub's setNotificationFlags(), except that 'all' defaults to True.  See useNotifyByPubsubMessage()\nfor an explanation of pubModule (typically only if pubsub inside wxPython's wx.lib)",
    "original_nl": "Will cause all pubsub notifications of pubsub \"actions\" (such as new topic created, message sent, listener died\netc) to be written to specified file (or stdout if none given). The fileObj need only provide a 'write(string)'\nmethod.\n\nThe first two arguments are the same as those of NotifyByWriteFile constructor. The 'all' and kwargs arguments\nare those of pubsub's setNotificationFlags(), except that 'all' defaults to True.  See useNotifyByPubsubMessage()\nfor an explanation of pubModule (typically only if pubsub inside wxPython's wx.lib)"
  },
  {
    "code": "def bin_search(binary):\n    result = None\n    mode = (os.R_OK | os.X_OK)\n    for p in bin_search_path:\n        path = os.path.join(p, binary)\n        if (os.access(path, mode) == 1):\n            result = path\n            break\n    else:\n        raise MissingBinary(('Unable to find binary \"%s\"' % binary))\n    return result",
    "nl": "search the bin_search_path  for a given binary",
    "original_nl": "search the bin_search_path  for a given binary\nreturning its fullname or None"
  },
  {
    "code": "def test_get_by_region_name(self):\n    for reg in list(RegionCode.Region):\n        reg_str = reg.name.replace('_', '-')\n        self.assertEquals(self.REGIONS[reg], RegionCode[reg_str])",
    "nl": "Code can be correctly retrieved from Region string with dashes using RegionCode.",
    "original_nl": "Code can be correctly retrieved from Region string with dashes using RegionCode."
  },
  {
    "code": "def read_history(self, num=10, segment=0):\n    if (num < 0):\n        num = 0\n    return self._builder.read_history(num, segment)",
    "nl": "Outputs the last `num` rows that were appended either by `append` or\n`append_multiple`.\n",
    "original_nl": "Outputs the last `num` rows that were appended either by `append` or\n`append_multiple`.\n\nReturns\n-------\nout : list[list]"
  },
  {
    "code": "def run_command(self, command, timeout=(- 1)):\n    cmd_lines = [l for l in command.splitlines() if (l and (not l.startswith('//')))]\n    cmd = re.sub('\\\\s{2,}', ' ', ' '.join(cmd_lines))\n    logger.debug('Command length: {} chars'.format(len(cmd)))\n    logger.debug('Command: {}'.format(cmd))\n    if (len(cmd) > 1024):\n        error = 'Code too long. Please commands with less than 1024 effective chracters.\\nIndentation spaces/tabs don\\'t count towards \"effective\" characters.'\n        logger.error(error)\n        raise ValueError(error.replace('\\n', ' '))\n    self._send_line(cmd)\n    match = self._expect_prompt(timeout=timeout)\n    logger.debug('Prompt type: {}'.format(match))\n    logger.debug('Iterating over message')\n    response = []\n    while (not self._isbufferempty()):\n        response.append(self.child.before)\n        logger.debug('Buffer not empty, sending blank line')\n        match = self._expect_prompt(timeout=timeout)\n        if (match == 1):\n            error = 'Code incomplete. Please enter valid and complete code.\\nContinuation prompt functionality not implemented yet.'\n            logger.error(error.replace('\\n', ' '))\n            raise ValueError(error)\n        self._send_line('')\n    response.append(self.child.before)\n    response = self._filter_response(''.join(response))\n    logger.debug('Response: {}'.format(response))\n    return response",
    "nl": "Send a command to the REPL, wait for and return output.\n\n",
    "original_nl": "Send a command to the REPL, wait for and return output.\n\n:param str command: The command to send. Trailing newlines are not needed.\n  This should be a complete block of input that will trigger execution;\n  if a continuation prompt is found after sending input, :exc:`ValueError`\n  will be raised.\n:param int timeout: How long to wait for the next prompt. -1 means the\n  default from the :class:`pexpect.spawn` object (default 30 seconds).\n  None means to wait indefinitely."
  },
  {
    "code": "@classmethod\ndef is_collected_outdated(cls, block_structure):\n    outdated_transformers = []\n    for transformer in TransformerRegistry.get_registered_transformers():\n        version_in_block_structure = block_structure._get_transformer_data_version(transformer)\n        if (transformer.VERSION != version_in_block_structure):\n            outdated_transformers.append(transformer)\n    if outdated_transformers:\n        logger.debug(\"Collected Block Structure data for the following transformers is outdated: '%s'.\", [(transformer.name(), transformer.VERSION) for transformer in outdated_transformers])\n    return bool(outdated_transformers)",
    "nl": "Returns whether the collected data in the block structure is outdated.",
    "original_nl": "Returns whether the collected data in the block structure is outdated."
  },
  {
    "code": "def fetch_as_element(self, **kw):\n    clone = self.copy()\n    clone.format.field_format('id')\n    for custom_field in ['field_ids', 'field_names']:\n        clone.format.data.pop(custom_field, None)\n    for list_of_results in clone.fetch_raw(**kw):\n        for entry in list_of_results:\n            (yield Connection(**entry))",
    "nl": "Fetch the results and return as a Connection element. The original\nquery is not modified.\n\n",
    "original_nl": "Fetch the results and return as a Connection element. The original\nquery is not modified.\n\n:return: generator of elements\n:rtype: :class:`.Connection`"
  },
  {
    "code": "def ci_width(self, alpha=0.05):\n    half_width = self.std_dev.copy()\n    n = self.num_realizations\n    if (n > 1):\n        half_width *= scipy.stats.t.ppf((1 - (alpha / 2)), (n - 1))\n    return half_width",
    "nl": "Confidence interval half-width based on a Student t distribution\n",
    "original_nl": "Confidence interval half-width based on a Student t distribution\n\nParameters\n----------\nalpha : float\n    Significance level (one minus the confidence level!)\n\nReturns\n-------\nfloat\n    Half-width of a two-sided (1 - :math:`alpha`) confidence interval"
  },
  {
    "code": "def read(self, request, id):\n    resource = get_object_or_404(BootResource, id=id)\n    stream = json_object(boot_resource_to_dict(resource, with_sets=True), request)\n    return HttpResponse(stream, content_type='application/json; charset=utf-8', status=int(http.client.OK))",
    "nl": "Read a boot resource.",
    "original_nl": "Read a boot resource."
  },
  {
    "code": "def url_fix(url, charset='UTF-8'):\n    if isinstance(url, unicode):\n        url = url.encode(charset)\n    return quote(url, safe=\"%/:=&?~#+!$,;'@()*[]\")",
    "nl": "Normalize the URL if it contains Non-ASCII chars",
    "original_nl": "Normalize the URL if it contains Non-ASCII chars"
  },
  {
    "code": "def usernames_in(message):\n    if _command_re.match(message.split(' ', 1)[0]):\n        message = message.split(' ', 1)[1]\n    message = strip_urls(message)\n    results = []\n    for result in _username_re.finditer(message):\n        if result.group(1):\n            results.append(result.group(1))\n    return results",
    "nl": "Return all the matched usernames in the message",
    "original_nl": "Return all the matched usernames in the message"
  },
  {
    "code": "@classmethod\ndef deserialize(cls, uid, trusted=True, registry=None, **kwargs):\n    if (registry is None):\n        raise ValueError('no registry provided')\n    if (uid not in registry):\n        raise ValueError('uid not found: {}'.format(uid))\n    if (not isinstance(registry[uid], UidModel)):\n        date_created = registry[uid]['date_created']\n        date_modified = registry[uid]['date_modified']\n        kwargs.update({\n            'verbose': False,\n        })\n        new_model = super(UidModel, cls).deserialize(value=registry[uid], registry=registry, trusted=trusted, **kwargs)\n        new_model._backend.update({\n            'uid': properties.Uuid.from_json(uid),\n            'date_created': properties.DateTime.from_json(date_created),\n            'date_modified': properties.DateTime.from_json(date_modified),\n        })\n        registry.update({\n            uid: new_model,\n        })\n    return registry[uid]",
    "nl": "Deserialize nested UidModels from flat pointer dictionary",
    "original_nl": "Deserialize nested UidModels from flat pointer dictionary"
  },
  {
    "code": "@home_position.setter\ndef home_position(self, value: np.ndarray) -> None:\n    self._home_position = value",
    "nl": "Set home position.",
    "original_nl": "Set home position."
  },
  {
    "code": "def test_svg_seaborn():\n    draw_format_method('svg', 'seaborn')",
    "nl": "Write .svg graphics with seaborn",
    "original_nl": "Write .svg graphics with seaborn"
  },
  {
    "code": "def GetMessages(self, unused_formatter_mediator, event):\n    if (self.DATA_TYPE != event.data_type):\n        raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(event.data_type))\n    event_values = event.CopyToDict()\n    file_reference = event_values.get('file_reference', None)\n    if file_reference:\n        event_values['file_reference'] = '{0:d}-{1:d}'.format((file_reference & 281474976710655), (file_reference >> 48))\n    parent_file_reference = event_values.get('parent_file_reference', None)\n    if parent_file_reference:\n        event_values['parent_file_reference'] = '{0:d}-{1:d}'.format((parent_file_reference & 281474976710655), (parent_file_reference >> 48))\n    update_reason_flags = event_values.get('update_reason_flags', 0)\n    update_reasons = []\n    for (bitmask, description) in sorted(self._USN_REASON_FLAGS.items()):\n        if (bitmask & update_reason_flags):\n            update_reasons.append(description)\n    event_values['update_reason'] = ', '.join(update_reasons)\n    update_source_flags = event_values.get('update_source_flags', 0)\n    update_sources = []\n    for (bitmask, description) in sorted(self._USN_SOURCE_FLAGS.items()):\n        if (bitmask & update_source_flags):\n            update_sources.append(description)\n    event_values['update_source'] = ', '.join(update_sources)\n    return self._ConditionalFormatMessages(event_values)",
    "nl": "Determines the formatted message strings for an event object.\n\nArgs:\n  formatter_mediator (FormatterMediator): mediates the interactions between\n      formatters and other components, such as storage and Windows EventLog\n      resources.\n  event (EventObject): event.\n",
    "original_nl": "Determines the formatted message strings for an event object.\n\nArgs:\n  formatter_mediator (FormatterMediator): mediates the interactions between\n      formatters and other components, such as storage and Windows EventLog\n      resources.\n  event (EventObject): event.\n\nReturns:\n  tuple(str, str): formatted message string and short message string.\n\nRaises:\n  WrongFormatter: if the event object cannot be formatted by the formatter."
  },
  {
    "code": "def cartesian_to_spherical_azimuthal(x, y):\n    y = (float(y) if isinstance(y, int) else y)\n    phi = numpy.arctan2(y, x)\n    return (phi % (2 * numpy.pi))",
    "nl": "Calculates the azimuthal angle in spherical coordinates from Cartesian\ncoordinates. The azimuthal angle is in [0,2*pi].\n",
    "original_nl": "Calculates the azimuthal angle in spherical coordinates from Cartesian\ncoordinates. The azimuthal angle is in [0,2*pi].\n\nParameters\n----------\nx : {numpy.array, float}\n    X-coordinate.\ny : {numpy.array, float}\n    Y-coordinate.\n\nReturns\n-------\nphi : {numpy.array, float}\n    The azimuthal angle."
  },
  {
    "code": "def settingsDeploy(self, action='start', options={\n    \n}):\n    os.environ['DJANGO_SETTINGS_MODULE'] = TEST_SETTINGS\n    options.update({\n        'settings': TEST_SETTINGS,\n    })\n    return self.deploy(action, options)",
    "nl": "Use the hendrix test project to test the bash deployment flow path",
    "original_nl": "Use the hendrix test project to test the bash deployment flow path"
  },
  {
    "code": "def test_update_airspeed_estimate(mpstate):\n    loadedModule = cuav_check.init(mpstate)\n    m = common.MAVLink_global_position_int_message(2000, 230100000, (- 344560000), 110000, 100000, 3000, 200, 1, 9000)\n    loadedModule.cuav_settings.wind_speed = 10\n    loadedModule.cuav_settings.wind_direction = 88\n    loadedModule.update_airspeed_estimate(m)\n    loadedModule.unload()",
    "nl": "Test the airspeed updater",
    "original_nl": "Test the airspeed updater"
  },
  {
    "code": "def _find_event(self, clz):\n    return ((k, v) for (k, v) in enumerate(self.bot.cron.events) if isinstance(v, clz)).next()",
    "nl": "Find an event of a given class in cron.",
    "original_nl": "Find an event of a given class in cron."
  },
  {
    "code": "def json_deep_search(iterable, field):\n    fields_found = []\n    if isinstance(iterable, dict):\n        for (key, value) in iterable.items():\n            if (key == field):\n                fields_found.append(value)\n            elif isinstance(value, (dict, list)):\n                fields_found.extend(json_deep_search(value, field))\n    elif isinstance(iterable, list):\n        for item in iterable:\n            fields_found.extend(json_deep_search(item, field))\n    return fields_found",
    "nl": "Takes a JSON like data-structure and search for the occurrences\nof the given field/key.",
    "original_nl": "Takes a JSON like data-structure and search for the occurrences\nof the given field/key."
  },
  {
    "code": "def make_call_side_effect(text):\n\n    def side_effect(*args, **kwargs):\n        log = kwargs['stdout']\n        log.write(text)\n    return side_effect",
    "nl": "Intended for mocking the subprocess.call() in testing\n_release_tools.up_to_date(). Return a side effect that\nprints text to mocked function's stdout argument.",
    "original_nl": "Intended for mocking the subprocess.call() in testing\n_release_tools.up_to_date(). Return a side effect that\nprints text to mocked function's stdout argument."
  },
  {
    "code": "def sampling(orig, dest, rand, format, conf):\n    return ShellCommand('\\n                        count=$({tool} view -Sc {input[sam]})\\n                        ## judge mapped reads number less than sampling number\\n                        if [ $count -le {param[random_number]} ]\\n                        then\\n                            ln -f {input[sam]} {input[sam]}.{param[random_number]}\\n                            {tool} view -bS {input[sam]}.{param[random_number]} > {output[samp]}\\n                        else\\n                            sampling_pe_sam.py {input[sam]} {param[random_number]}\\n                            {tool} view -bS {input[sam]}.{param[random_number]} > {output[samp]}\\n                        fi\\n                        ', tool='samtools', input={\n        'sam': orig,\n    }, output={\n        'samp': dest,\n    }, param={\n        'random_number': rand,\n    }, name='sampling bam')",
    "nl": "prepare sampling fastq files for library contamination and fastqc\nrand: the number of random selected fastq reads\nuse lh3's https://github.com/lh3/seqtk/ to sample fastq and fastq.gz",
    "original_nl": "prepare sampling fastq files for library contamination and fastqc\nrand: the number of random selected fastq reads\nuse lh3's https://github.com/lh3/seqtk/ to sample fastq and fastq.gz"
  },
  {
    "code": "def targets(self, node):\n    node = self.node(node)\n    nodes = [conn.target for conn in self.connections if (conn.target == node)]\n    return nodes",
    "nl": "Return nodes that `node` passes data into.",
    "original_nl": "Return nodes that `node` passes data into."
  },
  {
    "code": "def linkQualityCB(self, percentage):\n    q = self.linkQuality.addMeasurementCount(percentage)\n    if (q is not None):\n        self.sig_flieLink.emit(percentage)",
    "nl": "Called when the link driver updates the link quality measurement",
    "original_nl": "Called when the link driver updates the link quality measurement"
  },
  {
    "code": "def test_eval_running_mode3(self):\n    self.root.database.prepare_to_run()\n    self.root.database.set_value('root', 'val1', 10.0)\n    test = 'cm.sqrt({val1}/{val2})'\n    formatted = self.root.format_and_eval_string(test)\n    assert (formatted == (1 + 0j))\n    assert self.root._eval_cache\n    assert (test in self.root._eval_cache)\n    self.root.database.set_value('root', 'val1', 40.0)\n    formatted = self.root.format_and_eval_string(test)\n    assert (formatted == (2 + 0j))",
    "nl": "Test eval expression containing a cmath function.",
    "original_nl": "Test eval expression containing a cmath function."
  },
  {
    "code": "def test_layermap_strict(self):\n    with self.assertRaises(InvalidDecimal):\n        lm = LayerMapping(Interstate, inter_shp, inter_mapping)\n        lm.save(silent=True, strict=True)\n    Interstate.objects.all().delete()\n    lm = LayerMapping(Interstate, inter_shp, inter_mapping)\n    lm.save(silent=True)\n    self.assertEqual(2, Interstate.objects.count())\n    ds = DataSource(inter_shp)\n    valid_feats = ds[0][:2]\n    for feat in valid_feats:\n        istate = Interstate.objects.get(name=feat['Name'].value)\n        if (feat.fid == 0):\n            self.assertEqual(Decimal(str(feat['Length'])), istate.length)\n        elif (feat.fid == 1):\n            self.assertAlmostEqual(feat.get('Length'), float(istate.length), 2)\n        for (p1, p2) in zip(feat.geom, istate.path):\n            self.assertAlmostEqual(p1[0], p2[0], 6)\n            self.assertAlmostEqual(p1[1], p2[1], 6)",
    "nl": "Testing the `strict` keyword, and import of a LineString shapefile.",
    "original_nl": "Testing the `strict` keyword, and import of a LineString shapefile."
  },
  {
    "code": "def calculate_base_skill_roll(pc, skill):\n    trait = skill.trait\n    trait_value = api.character.modified_trait_rank(trait)\n    skill_value = api.character.skills.get_skill_rank(skill.id)\n    return DicePool().from_values(roll=(skill_value + trait_value), keep=trait_value)",
    "nl": "calculate the base skill roll for a given skill",
    "original_nl": "calculate the base skill roll for a given skill"
  },
  {
    "code": "def check_switch_vendor(old_vendor, name, urls, _depth=0):\n    if (_depth > 3):\n        return ''\n    new_name = check_for_launchpad(old_vendor, name, urls)\n    if new_name:\n        return ('launchpad', new_name)\n    return ('', '')",
    "nl": "Check if the project should switch vendors. E.g\nproject pushed on pypi, but changelog on launchpad.\n\n",
    "original_nl": "Check if the project should switch vendors. E.g\nproject pushed on pypi, but changelog on launchpad.\n\n:param name: str, name of the project\n:param urls: set, urls to check.\n:return: tuple, (str(new vendor name), str(new project name))"
  },
  {
    "code": "def total_number_of_data_points(self):\n    n_data = 0.0\n    for telescope in self.telescopes:\n        n_data = (n_data + telescope.n_data('flux'))\n    return n_data",
    "nl": "Compute the parallax displacement for all the telescopes, if this is desired in\nthe second order parameter.\n",
    "original_nl": "Compute the parallax displacement for all the telescopes, if this is desired in\nthe second order parameter.\n:return: n_data, the total number of points\n:rtype: float"
  },
  {
    "code": "def request_headers(request):\n    return CaseInsensitiveDict(((key[5:].lower().replace('_', '-'), value) for (key, value) in request.META.items() if key.startswith('HTTP_')))",
    "nl": "Return a dict with headers from a request.\n\nHeader keys are case insensitive.",
    "original_nl": "Return a dict with headers from a request.\n\nHeader keys are case insensitive."
  },
  {
    "code": "def test_load_geetest_js():\n    jsfile = os.path.join(os.getcwd(), 'gsxt', 'geetest.5.10.10.js')\n    print(jsfile)\n    js_context = JSRUNTIME.compile(load_filetext(jsfile))\n    print(js_context)",
    "nl": "load javascript text from file, compile, return context object",
    "original_nl": "load javascript text from file, compile, return context object"
  },
  {
    "code": "def copy(self):\n    return ContinuousFactor(self.scope(), self.distribution.copy())",
    "nl": "Return a copy of the distribution.\n",
    "original_nl": "Return a copy of the distribution.\n\nReturns\n-------\nContinuousFactor object: copy of the distribution\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.special import beta\n>>> from pgmpy.factors.continuous import ContinuousFactor\n# Two variable dirichlet distribution with alpha = (1,2)\n>>> def dirichlet_pdf(x, y):\n...     return (np.power(x, 1) * np.power(y, 2)) / beta(x, y)\n>>> dirichlet_factor = ContinuousFactor(['x', 'y'], dirichlet_pdf)\n>>> dirichlet_factor.variables\n['x', 'y']\n>>> copy_factor = dirichlet_factor.copy()\n>>> copy_factor.variables\n['x', 'y']"
  },
  {
    "code": "def _tidy(self, work, file_path):\n    output_file = os.path.join(self._output_dir, work)\n    self._logger.info('Tidying file {} into {}'.format(file_path, output_file))\n    try:\n        tei_doc = etree.parse(file_path)\n    except etree.XMLSyntaxError as err:\n        self._logger.error('XML file \"{}\" is invalid: {}'.format(file_path, err))\n        raise\n    return self.transform(tei_doc).getroot()",
    "nl": "Transforms the file at `file_path` into simpler XML and returns\nthat.",
    "original_nl": "Transforms the file at `file_path` into simpler XML and returns\nthat."
  },
  {
    "code": "def GetScript(self, source, line, col, filename):\n    return jedi.Script(source, line, col, filename)",
    "nl": "Return Jedi script object suitable to AutoComplete and ShowCallTip",
    "original_nl": "Return Jedi script object suitable to AutoComplete and ShowCallTip"
  },
  {
    "code": "def file_convert(mib_txt_path, mib_py_path):\n    mod_logger_snmp = loggers.module_logger(name=__name__)\n    smidump = Popen(['smidump', '-k', '-f', 'python', mib_txt_path], stdout=PIPE)\n    list_stdout = smidump.communicate()[0]\n    if (len(list_stdout) == 0):\n        return 'Fail'\n    mib_path_tmp = os.path.join(mib_py_path, 'tmp')\n    if (not os.path.exists(mib_path_tmp)):\n        os.makedirs(mib_path_tmp)\n    sys.path.append(mib_path_tmp)\n    file_name = os.path.splitext(os.path.basename(mib_txt_path))[0]\n    temp_file_name = '{0}.py'.format(file_name)\n    temp_file_path = os.path.join(mib_path_tmp, temp_file_name)\n    with open(temp_file_path, 'ab') as a:\n        a.write(list_stdout)\n    temp_module = __import__(os.path.splitext(os.path.basename(mib_txt_path))[0])\n    if (('moduleName' in list(temp_module.MIB.keys())) and ('nodes' in list(temp_module.MIB.keys()))):\n        helpers.MIBS_DICT.update({\n            temp_module.MIB['moduleName']: list(temp_module.MIB['nodes'].keys()),\n        })\n    sys.path.remove(mib_path_tmp)\n    os.remove(temp_file_path)\n    pipe = Popen(['libsmi2pysnmp', '--no-text'], stdout=PIPE, stdin=PIPE)\n    stdout = pipe.communicate(input=list_stdout)\n    mib_name = '{0}.py'.format(temp_module.MIB['moduleName'])\n    mib_py_path = os.path.join(mib_py_path, mib_name)\n    mod_logger_snmp.debug(('Convert %s to %s' % (file_name, temp_file_name)))\n    with open(mib_py_path, 'a') as py_file:\n        for string in stdout:\n            if (string is not None):\n                str_dict = string.decode('utf-8').split('\\n')\n                for each_str in str_dict:\n                    if ('ModuleCompliance' in each_str):\n                        if ('ObjectGroup' in each_str):\n                            py_file.write((each_str + '\\n'))\n                    elif ('Compliance)' in each_str):\n                        pass\n                    else:\n                        py_file.write((each_str + '\\n'))\n    return mib_name",
    "nl": "Convert .txt MIB to .py.\n\nArgs:\n    mib_txt_path(str):  Full path to .txt MIB.\n    mib_py_path(str):  Full path to .py MIB\n\nExamples::\n\n    file_convert(mib_txt_path, mib_py_path)",
    "original_nl": "Convert .txt MIB to .py.\n\nArgs:\n    mib_txt_path(str):  Full path to .txt MIB.\n    mib_py_path(str):  Full path to .py MIB\n\nExamples::\n\n    file_convert(mib_txt_path, mib_py_path)"
  },
  {
    "code": "def _modify_command(command):\n    if isinstance(command, list):\n        for i in xrange(len(command)):\n            if (command[i] == 'configtest'):\n                command[i] = '-t'\n    else:\n        command = command.replace('configtest', '-t')\n    return command",
    "nl": "Modifies command so configtest works inside the docker image",
    "original_nl": "Modifies command so configtest works inside the docker image"
  },
  {
    "code": "def do_info_threads(self):\n    return self.execute_command(self.GS_INFO_THREADS)",
    "nl": "Perform 'info threads' command. Returns output.",
    "original_nl": "Perform 'info threads' command. Returns output."
  },
  {
    "code": "def replace_nodes(self, node1, node2):\n    (node1[PARENT][DIRECTION1], node2[PARENT][DIRECTION2]) = (node2[PARENT][DIRECTION2], node1[PARENT][DIRECTION1])\n    (node1[PARENT], node2[PARENT]) = (node2[PARENT], node1[PARENT])\n    for direction in [LEFT, RIGHT]:\n        (node1[direction][PARENT], node2[direction][PARENT]) = (node2[direction][PARENT], node1[direction][PARENT])\n        (node1[direction], node2[direction]) = (node2[direction], node1[direction])",
    "nl": "Replaces node1 and node2 by rewiring the pointers (parent, left\nand right) of the two nodes.\n\nArgs:\n    node1: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]\n    node2: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]",
    "original_nl": "Replaces node1 and node2 by rewiring the pointers (parent, left\nand right) of the two nodes.\n\nArgs:\n    node1: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]\n    node2: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]"
  },
  {
    "code": "def getfield(self, pkt, s):\n    if s.startswith('\\r\\n'):\n        s = s.lstrip('\\r\\n')\n        if (s == ''):\n            return ('', '')\n    self.myresult = ''\n    for c in s:\n        self.myresult = (self.myresult + base64.standard_b64encode(c))\n    return ('', self.myresult)",
    "nl": "this method will get the packet, takes what does need to be\ntaken and let the remaining go, so it returns two values.\nfirst value which belongs to this field and the second is\nthe remaining which does need to be dissected with\nother \"field classes\".\n",
    "original_nl": "this method will get the packet, takes what does need to be\ntaken and let the remaining go, so it returns two values.\nfirst value which belongs to this field and the second is\nthe remaining which does need to be dissected with\nother \"field classes\".\n@param pkt: holds the whole packet\n@param s: holds only the remaining data which is not dissected yet."
  },
  {
    "code": "def assert_is_instance(value, types, message=None, extra=None):\n    assert isinstance(value, types), _assert_fail_message(message, value, types, 'is not an instance of', extra)",
    "nl": "Raises an AssertionError if value is not an instance of type(s).",
    "original_nl": "Raises an AssertionError if value is not an instance of type(s)."
  },
  {
    "code": "@classmethod\ndef expand(cls, input):\n\n    def parameterized_expand_wrapper(f):\n        stack = inspect.stack()\n        frame = stack[1]\n        frame_locals = frame[0].f_locals\n        base_name = f.__name__\n        get_input = cls.input_as_callable(input)\n        for (num, args) in enumerate(get_input()):\n            p = param.from_decorator(args)\n            name_suffix = ('_%s' % (num,))\n            if ((len(p.args) > 0) and isinstance(p.args[0], basestring)):\n                name_suffix += ('_' + cls.to_safe_name(p.args[0]))\n            name = (base_name + name_suffix)\n            frame_locals[name] = cls.param_as_standalone_func(p, f, name)\n        return nottest(f)\n    return parameterized_expand_wrapper",
    "nl": "A \"brute force\" method of parameterizing test cases. Creates new\ntest cases and injects them into the namespace that the wrapped\nfunction is being defined in. Useful for parameterizing tests in\nsubclasses of 'UnitTest', where Nose test generators don't work.\n\n>>> ",
    "original_nl": "A \"brute force\" method of parameterizing test cases. Creates new\ntest cases and injects them into the namespace that the wrapped\nfunction is being defined in. Useful for parameterizing tests in\nsubclasses of 'UnitTest', where Nose test generators don't work.\n\n>>> @parameterized.expand([(\"foo\", 1, 2)])\n... def test_add1(name, input, expected):\n...     actual = add1(input)\n...     assert_equal(actual, expected)\n...\n>>> locals()\n... 'test_add1_foo_0': <function ...> ...\n>>>"
  },
  {
    "code": "def test_properties():\n    telescope_obj = pyuvdata.Telescope()\n    prop_dict = dict(zip(required_properties, required_parameters))\n    for (k, v) in prop_dict.iteritems():\n        rand_num = np.random.rand()\n        setattr(telescope_obj, k, rand_num)\n        this_param = getattr(telescope_obj, v)\n        try:\n            nt.assert_equal(rand_num, this_param.value)\n        except AssertionError:\n            print('setting {prop_name} to a random number failed'.format(prop_name=k))\n            raise",
    "nl": "Test that properties can be get and set properly.",
    "original_nl": "Test that properties can be get and set properly."
  },
  {
    "code": "def get_task(self, task, view=False):\n    infos = self.get_task_infos(task)\n    if (infos is None):\n        answer = (None if (not view) else (None, None))\n        return answer\n    return (infos.cls if (not view) else (infos.cls, infos.view))",
    "nl": "Access a given task class.\n",
    "original_nl": "Access a given task class.\n\nParameters\n----------\ntask : unicode\n    Id of the task class for which to return the actual class.\n\nview : bool, optional\n    Whether or not to return the view assoicated with the task.\n\nReturns\n-------\ntask_cls : type or None\n    Class associated to the requested task or None if the task was not\n    found.\n\ntask_view : EnamlDefMeta or None, optional\n    Associated view if requested."
  },
  {
    "code": "def clear(self):\n    self.values = [0 for x in xrange(len(self.values))]\n    self.count = 0",
    "nl": "Clears the sample, setting all values to zero.",
    "original_nl": "Clears the sample, setting all values to zero."
  },
  {
    "code": "def contained_expr(container, ast_expr):\n    res = set()\n    for elmt in container:\n        if matches_expr(elmt, ast_expr):\n            res.add(elmt)\n    return list(res)",
    "nl": "Given a container, returns structural AST expression matches.",
    "original_nl": "Given a container, returns structural AST expression matches."
  },
  {
    "code": "def get_sitemap(base_url, filename, out_file_path, compressed=True):\n    response = urllib2.urlopen((base_url + filename))\n    compressed_file = StringIO.StringIO()\n    compressed_file.write(response.read())\n    compressed_file.seek(0)\n    if compressed:\n        decompressed_file = gzip.GzipFile(fileobj=compressed_file, mode='rb')\n        with open(out_file_path, 'w') as outfile:\n            outfile.write(decompressed_file.read())\n    else:\n        with open(out_file_path, 'w') as outfile:\n            outfile.write(compressed_file.read())",
    "nl": "Get the sitemap XML.\n",
    "original_nl": "Get the sitemap XML.\n\nParameters\n----------\nbase_url : str\n  Base url of file location\nfilename : str\n  Filename on web\nout_file_path : str\n  Desire file path to save to\n\nReturns\n-------\nNone (file is downloaded)"
  },
  {
    "code": "@app.route('/project/<pid>/import', methods=['GET', 'POST'])\ndef import_scan(pid):\n    project = get_project_db(pid)\n    db = database.ScanDatabase(project['dbfile'])\n    if (flask.request.method == 'GET'):\n        files = db.importdb.get_imported_files()\n        return flask.render_template('import.html', pid=pid, files=files, name=project['name'])\n    else:\n        i = importscan.Import(project['dbfile'])\n        scans = flask.request.files.getlist('scans[]')\n        for scan in scans:\n            res = i.import_scan(scan.read())\n            if (res is True):\n                db.importdb.add_import_file(scan.filename)\n        a = attacks.Attack(project['dbfile'])\n        a.find_attacks()\n        return flask.redirect(flask.url_for('get_project', pid=pid))",
    "nl": "Import scan data into the database associated with the pid.",
    "original_nl": "Import scan data into the database associated with the pid."
  },
  {
    "code": "def operon_map(self):\n    if (not self.__operon_mappings):\n        pairs = mo.get_operon_pairs(self.__microbes_online_db, self)\n        synonyms = self.thesaurus()\n        self.__operon_mappings = {synonyms[gene]: synonyms[head] for (head, gene) in pairs}\n    return self.__operon_mappings",
    "nl": "Returns the operon map for this particular organism.\nMicrobes Online works on VNG names, but RSAT is working on\nfeature ids, so this function also maps VNG names to feature ids",
    "original_nl": "Returns the operon map for this particular organism.\nMicrobes Online works on VNG names, but RSAT is working on\nfeature ids, so this function also maps VNG names to feature ids"
  },
  {
    "code": "def initialize_tree(self):\n    self.quadtree = QuadTree(0.0, 0.0, max(self.width, self.height), self.quadtree_threshold)\n    self.started = True",
    "nl": "Initialize the Quad Tree implementation.",
    "original_nl": "Initialize the Quad Tree implementation."
  },
  {
    "code": "def ready(self):\n    connection_created.connect(self.activate_pragmas_per_connection)\n    self.activate_pragmas_on_start()\n    logger.info('Running Kolibri with the following settings: {settings}'.format(settings=os.environ['DJANGO_SETTINGS_MODULE']))",
    "nl": "Sets up PRAGMAs.",
    "original_nl": "Sets up PRAGMAs."
  },
  {
    "code": "def _setting(self, key, default):\n    env = 'STATSD_{}'.format(key).upper()\n    return self._settings.get(key, os.environ.get(env, default))",
    "nl": "Return the setting, checking config, then the appropriate\nenvironment variable, falling back to the default.\n\n",
    "original_nl": "Return the setting, checking config, then the appropriate\nenvironment variable, falling back to the default.\n\n:param str key: The key to get\n:param any default: The default value if not set\n:return: str"
  },
  {
    "code": "@abc.abstractmethod\ndef get_allowed_network_types(self, agent=None):\n    pass",
    "nl": "Return the agent's or driver's allowed network types.\n\nFor example: return ('flat', ...). You can also refer to the\nconfiguration the given agent exposes.",
    "original_nl": "Return the agent's or driver's allowed network types.\n\nFor example: return ('flat', ...). You can also refer to the\nconfiguration the given agent exposes."
  },
  {
    "code": "@pytest.fixture\ndef minmax_zoom():\n    path = os.path.join(TESTDATA_DIR, 'minmax_zoom.mapchete')\n    return ExampleConfig(path=path, dict=_dict_from_mapchete(path))",
    "nl": "Fixture for minmax_zoom.mapchete.",
    "original_nl": "Fixture for minmax_zoom.mapchete."
  },
  {
    "code": "def test_clear(self):\n    cart = Cart()\n    cart.add('rabbit')\n    cart.modified = False\n    cart.clear()\n    self.assertEqual(len(cart), 0)\n    self.assertEqual(cart.modified, True)",
    "nl": "Cart.clear() clears the cart and marks it as modified",
    "original_nl": "Cart.clear() clears the cart and marks it as modified"
  },
  {
    "code": "@classmethod\ndef count_stats(self, xs):\n    stats = {\n        'DIB': 0,\n        'EIB': 0,\n    }\n    for x in xs:\n        if (float(x.name) > 0):\n            stats['EIB'] += 1\n        else:\n            stats['DIB'] += 1\n    return stats",
    "nl": "xs is a bedtool instance where the name field holds the bin score",
    "original_nl": "xs is a bedtool instance where the name field holds the bin score"
  },
  {
    "code": "def print_exceptions(exceptions, max_print=10):\n    exceptions = sorted(exceptions, key=(lambda n: (n[2], n[1])), reverse=True)[:max_print]\n    for (url, stroke_count, _) in exceptions:\n        logging.info('\\t%s - %i strokes', url, stroke_count)",
    "nl": "Print the exceptions, but not too many.\n",
    "original_nl": "Print the exceptions, but not too many.\n\nParameters\n----------\nexceptions : list\n    Triplets (url, stroke_count, dist to closest mode)\nmax_print : int\n    Print not more then max_print lines"
  },
  {
    "code": "@manager.option('text')\ndef send(text):\n    data = current_app.config['TEST_DATA']\n    uri = current_app.config['SLACK_CALLBACK']\n    client = current_app.test_client()\n    data['text'] = text\n    rv = client.post(uri, data=data)\n    if (rv.status_code == 200):\n        body = rv.data\n        if (not body):\n            print('Response body is empty!')\n            return\n        obj = json.loads(body)\n        if (not obj.get('attachments')):\n            obj = obj['text']\n            print(obj)\n        else:\n            pprint(obj)\n    else:\n        print(('Error!\\nstatus code: %s\\nbody: %s' % (rv.status_code, rv.data)))",
    "nl": "Send text to slack callback url",
    "original_nl": "Send text to slack callback url"
  },
  {
    "code": "def ar1(func, method=ar_nitime):\n    func_centered = (func - func.mean(0))\n    ar_vals = np.apply_along_axis(method, 0, func_centered)\n    return ar_vals",
    "nl": "Apply the 'ar_nitime' function across the centered functional \ntimeseries.\n\n:type func: Nibabel data\n",
    "original_nl": "Apply the 'ar_nitime' function across the centered functional \ntimeseries.\n\n:type func: Nibabel data\n:param func: The functional timeseries data.\n:type method: Python function\n:param method: (default: ar_nitime) The algorithm to use to calculate AR1.\n:rtype: NumPy array\n:return: The vector of AR1 values."
  },
  {
    "code": "def create_badge(slug: str, label: str, image_filename: str, *, brand_id: Optional[BrandID]=None, description: Optional[str]=None, featured: bool=False) -> BadgeTuple:\n    badge = Badge(slug, label, image_filename, brand_id=brand_id, description=description, featured=featured)\n    db.session.add(badge)\n    db.session.commit()\n    return badge.to_tuple()",
    "nl": "Introduce a new badge.",
    "original_nl": "Introduce a new badge."
  },
  {
    "code": "def CompileReport(self, mediator):\n    lines_of_text = ['Listing file paths and hashes']\n    for (pathspec, hashes) in sorted(self._paths_with_hashes.items(), key=(lambda tuple: tuple[0].comparable)):\n        path_string = self._GeneratePathString(mediator, pathspec, hashes)\n        lines_of_text.append(path_string)\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)",
    "nl": "Compiles an analysis report.\n\nArgs:\n  mediator (AnalysisMediator): mediates interactions between analysis\n      plugins and other components, such as storage and dfvfs.\n",
    "original_nl": "Compiles an analysis report.\n\nArgs:\n  mediator (AnalysisMediator): mediates interactions between analysis\n      plugins and other components, such as storage and dfvfs.\n\nReturns:\n  AnalysisReport: report."
  },
  {
    "code": "@staticmethod\ndef complete_template(lazy_values):\n\n    def _template(self, text, line, start_index, end_index):\n        try:\n            values = lazy_values()\n        except TypeError:\n            values = (lazy_values or [])\n        return [v for v in values if v.startswith((text or '').strip())]\n    return _template",
    "nl": "Template method for handling auto-completion.",
    "original_nl": "Template method for handling auto-completion."
  },
  {
    "code": "def _client_receive(self):\n    try:\n        return self._client.readline()\n    except socket.error as e:\n        raise Error(self._ad, ('Encountered socket error reading RPC response \"%s\"' % e))",
    "nl": "Receives the server's response of an Rpc message.\n",
    "original_nl": "Receives the server's response of an Rpc message.\n\nReturns:\n    Raw byte string of the response.\n\nRaises:\n    Error: a socket error occurred during the read."
  },
  {
    "code": "def delete_files(log_printer, identifiers):\n    error_files = []\n    result = True\n    for identifier in identifiers:\n        try:\n            file_path = get_data_path(None, identifier)\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            else:\n                result = False\n        except (OSError, TypeError) as e:\n            error_files.append(hash_id(identifier))\n    if (len(error_files) > 0):\n        error_files = ', '.join(error_files)\n        logging.warning(\"There was a problem deleting the following files: {}. Please delete them manually from '{}'.\".format(error_files, Constants.USER_DATA_DIR))\n        result = False\n    return result",
    "nl": "Delete the given identifiers from the user's coala data directory.\n\n",
    "original_nl": "Delete the given identifiers from the user's coala data directory.\n\n:param log_printer: A LogPrinter object to use for logging.\n:param identifiers: The list of files to be deleted.\n:return:            True if all the given files were successfully deleted.\n                    False otherwise."
  },
  {
    "code": "def guess_ext(code):\n    lexer = guess_lexer(code)\n    mime = lexer.mimetypes[0]\n    ext = mimetypes.guess_extension(mime)[1:]\n    return ext",
    "nl": "Guess file ext with code",
    "original_nl": "Guess file ext with code"
  },
  {
    "code": "def test_open_file_object(self):\n    if (not unittest.source):\n        return\n    file_object = open(unittest.source, 'rb')\n    bde_volume = pybde.volume()\n    if unittest.password:\n        bde_volume.set_password(unittest.password)\n    if unittest.recovery_password:\n        bde_volume.set_recovery_password(unittest.recovery_password)\n    bde_volume.open_file_object(file_object)\n    bde_volume.close()\n    with self.assertRaises(IOError):\n        bde_volume.open_file_object(None)\n    with self.assertRaises(ValueError):\n        bde_volume.open_file_object(file_object, mode='w')",
    "nl": "Tests the open_file_object function.",
    "original_nl": "Tests the open_file_object function."
  },
  {
    "code": "def __init__(self, jids, _id=None):\n    super(GetStatusesIqProtocolEntity, self).__init__(self.__class__.XMLNS, _id, _type='get', to=YowConstants.WHATSAPP_SERVER)\n    self.setGetStatusesProps(jids)",
    "nl": "Request the statuses of users. Should be sent once after login.\n\nArgs:\n    - jids: A list of jids representing the users whose statuses you are\n        trying to get.",
    "original_nl": "Request the statuses of users. Should be sent once after login.\n\nArgs:\n    - jids: A list of jids representing the users whose statuses you are\n        trying to get."
  },
  {
    "code": "def _parse_backtrace_frame(self, match_obj):\n    fredutil.fred_assert(False, 'Must be implemented in subclass.')",
    "nl": "Return a BacktraceFrame from the given re Match object.\nThe Match object should be a tuple (result of gre_backtrace_frame.)",
    "original_nl": "Return a BacktraceFrame from the given re Match object.\nThe Match object should be a tuple (result of gre_backtrace_frame.)"
  },
  {
    "code": "def union(self, other):\n    meet = self.concept.meet(other.concept)\n    return self._sibling(meet.index)",
    "nl": "Return the closest subsumed neighbor (unification, meet).",
    "original_nl": "Return the closest subsumed neighbor (unification, meet)."
  },
  {
    "code": "def show_storing_credentials_failed(self):\n    dialog = xbmcgui.Dialog()\n    dialog.ok(self.utils.get_addon_data().get('plugin'), self.utils.get_local_string(32008))",
    "nl": "Shows \"storing credentials failed\" modal\n\n",
    "original_nl": "Shows \"storing credentials failed\" modal\n\n:returns:  bool - Dialog shown"
  },
  {
    "code": "def _get_converted_filename(self, filename):\n    splitted_filename = list(os.path.splitext(filename))\n    splitted_filename.insert(1, '.converted')\n    logger.debug('converted file name')\n    return ''.join(splitted_filename)",
    "nl": "Returns the audio converted name associated to the standard audio filename\n* Example: /var/www/myproject/media/audio/picture_1.wav\n    will return /var/www/myproject/media/audio/picture_1.converted.wav",
    "original_nl": "Returns the audio converted name associated to the standard audio filename\n* Example: /var/www/myproject/media/audio/picture_1.wav\n    will return /var/www/myproject/media/audio/picture_1.converted.wav"
  },
  {
    "code": "def addLineAndNewlineIfNecessary(line, output):\n    output.write(line)\n    if (len(line) < 1):\n        return\n    if (not line.endswith('\\n')):\n        output.write('\\n')",
    "nl": "Add the line and if the line does not end with a newline add a newline.",
    "original_nl": "Add the line and if the line does not end with a newline add a newline."
  },
  {
    "code": "def url_replace_param(url, name, value):\n    url_components = urlparse(url)\n    query_params = parse_qs(url_components.query)\n    query_params[name] = value\n    query = urlencode(query_params, doseq=True)\n    return urlunparse([url_components.scheme, url_components.netloc, url_components.path, url_components.params, query, url_components.fragment])",
    "nl": "Replace a GET parameter in an URL",
    "original_nl": "Replace a GET parameter in an URL"
  },
  {
    "code": "def list(self):\n    response = self._client.get('groups')\n    return GroupList.from_json(response.text)",
    "nl": "Return the group list.\n\n:raise TenableIOApiException:  When API error is encountered.\n",
    "original_nl": "Return the group list.\n\n:raise TenableIOApiException:  When API error is encountered.\n:return: An instance of :class:`tenable_io.api.models.Grouplist`."
  },
  {
    "code": "def stringize(data):\n    for field in data.keys():\n        if (field == 'birth_date'):\n            data[field] = data[field].strftime('%d/%m/%Y')\n            data[field] = '/'.join([str(int(f)) for f in data[field].split('/')])\n        else:\n            data[field] = str(data[field])\n    return data",
    "nl": "Given data for a citizen where integers are really integers\nand such, make them all into strings.",
    "original_nl": "Given data for a citizen where integers are really integers\nand such, make them all into strings."
  },
  {
    "code": "@property\ndef signature(self):\n    assert (len(self.signatures) == 1)\n    return self.signatures[0]",
    "nl": "Get a singleton signature.\n\n:rtype: `signature_cls`",
    "original_nl": "Get a singleton signature.\n\n:rtype: `signature_cls`"
  },
  {
    "code": "def color_hex_to_tuple(hex: str):\n    return tuple((int(hex.replace('#', '')[i:(i + 2)], 16) for i in (0, 2, 4)))",
    "nl": "Convert a hex color value to an RGB tuple",
    "original_nl": "Convert a hex color value to an RGB tuple"
  },
  {
    "code": "@micropython.native\ndef p(c='X8'):\n    p = pyb.Pin(c, pyb.Pin.OUT_PP, pull=pyb.Pin.PULL_NONE)\n    p.low()\n    print(p.value())\n    print('time get a pin value')\n    u = pyb.micros()\n    p.value()\n    t = (pyb.micros() - u)\n    print(t)",
    "nl": "time get a pin value\n13",
    "original_nl": "time get a pin value\n13"
  },
  {
    "code": "@property\ndef floating(self):\n    return [a for a in self.attendees if (a.is_unassigned and (a.paid == c.PAID_BY_GROUP))]",
    "nl": "Returns the list of paid-by-group unassigned badges for this group.\nThis is a separate property from the \"Group.unassigned\" property\nbecause when automatically adding or removing unassigned badges, we\ncare specifically about paid-by-group badges rather than all unassigned\nbadges.",
    "original_nl": "Returns the list of paid-by-group unassigned badges for this group.\nThis is a separate property from the \"Group.unassigned\" property\nbecause when automatically adding or removing unassigned badges, we\ncare specifically about paid-by-group badges rather than all unassigned\nbadges."
  },
  {
    "code": "def _check_chol(self, chol):\n    chol = ops.convert_to_tensor(chol, name='chol')\n    if (not self.verify_pd):\n        return chol\n    shape = array_ops.shape(chol)\n    rank = array_ops.rank(chol)\n    is_matrix = check_ops.assert_rank_at_least(chol, 2)\n    is_square = check_ops.assert_equal(array_ops.gather(shape, (rank - 2)), array_ops.gather(shape, (rank - 1)))\n    deps = [is_matrix, is_square]\n    diag = array_ops.batch_matrix_diag_part(chol)\n    deps.append(check_ops.assert_positive(diag))\n    return control_flow_ops.with_dependencies(deps, chol)",
    "nl": "Verify that `chol` is proper.",
    "original_nl": "Verify that `chol` is proper."
  },
  {
    "code": "def computeRMS(v):\n    v = numpy.array(v)\n    assert (len(v) > 0)\n    return math.sqrt(((v ** 2).sum() / len(v)))",
    "nl": "The root mean square (RMS) of a list of values\n\nArgs:\n    `v` (array-like)\n        Values for which we compute the RMS.\n        ",
    "original_nl": "The root mean square (RMS) of a list of values\n\nArgs:\n    `v` (array-like)\n        Values for which we compute the RMS.\n        \nReturns:\n    The RMS of the values.\n\n>>> v = [1.2, 3.5, 6.8, 1.1]\n>>> numpy.allclose(computeRMS(v), 3.9096, atol=1e-4)\nTrue"
  },
  {
    "code": "def flushdb(self):\n    return self.__db.flushdb()",
    "nl": "Delete all keys in the current database",
    "original_nl": "Delete all keys in the current database"
  },
  {
    "code": "@atomic.action_timer('zaqar.create_queue')\ndef _queue_create(self, **kwargs):\n    name = self.generate_random_name()\n    return self.clients('zaqar').queue(name, **kwargs)",
    "nl": "Create a Zaqar queue with random name.\n\n",
    "original_nl": "Create a Zaqar queue with random name.\n\n:param kwargs: other optional parameters to create queues like\n               \"metadata\"\n:returns: Zaqar queue instance"
  },
  {
    "code": "def update(self):\n    self._logger.info('{\"event\":\"updating_updater\"}')\n    self._pull_products()",
    "nl": "Update should be called before any firmware loading is done, to ensure the\nmost up-to-date information is being used.",
    "original_nl": "Update should be called before any firmware loading is done, to ensure the\nmost up-to-date information is being used."
  },
  {
    "code": "def _include_paths_from_environ(env_prefix=''):\n    paths = os.environ.get((env_prefix + 'WSGI_AUTH_PATHS'))\n    if (not paths):\n        return []\n    return paths.split(';')",
    "nl": "Environment value via `/login;/register`",
    "original_nl": "Environment value via `/login;/register`"
  },
  {
    "code": "def test_NP_Mixture_Smoother(self):\n    mix = m_s.NP_Mixture_Smoother(self.e, self.b)\n    np.testing.assert_array_almost_equal(mix.r, np.array([0.10982278, 0.03445531, 0.11018404, 0.11018604]))\n    np.testing.assert_array_almost_equal(mix.category, np.array([1, 0, 1, 1]))\n    (left, right) = mix.getSeed()\n    np.testing.assert_array_almost_equal(left, np.array([0.5, 0.5]))\n    np.testing.assert_array_almost_equal(right, np.array([0.03333333, 0.15]))\n    d = mix.mixalg()\n    np.testing.assert_array_almost_equal(d['mix_den'], np.array([0.0, 0.0, 0.0, 0.0]))\n    np.testing.assert_array_almost_equal(d['gradient'], np.array([0.0]))\n    np.testing.assert_array_almost_equal(d['p'], np.array([1.0]))\n    np.testing.assert_array_almost_equal(d['grid'], np.array([11.27659574]))\n    self.assertEqual(d['k'], 1)\n    self.assertEqual(d['accuracy'], 1.0)\n    (left, right) = mix.getRateEstimates()\n    np.testing.assert_array_almost_equal(left, np.array([0.0911574, 0.0911574, 0.0911574, 0.0911574]))\n    np.testing.assert_array_almost_equal(right, np.array([1, 1, 1, 1]))",
    "nl": "Test the main class",
    "original_nl": "Test the main class"
  },
  {
    "code": "def gemset_copy(source, destination, runas=None):\n    return _rvm('gemset copy {0} {1}'.format(source, destination), runas=runas)",
    "nl": "Copy all gems from one gemset to another.\n\nsource\n    The name of the gemset to copy, complete with ruby version.\ndestination\n    The destination gemset.\nrunas : None\n    The user to run rvm as.\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' rvm.gemset_copy foobar bazquo",
    "original_nl": "Copy all gems from one gemset to another.\n\nsource\n    The name of the gemset to copy, complete with ruby version.\ndestination\n    The destination gemset.\nrunas : None\n    The user to run rvm as.\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' rvm.gemset_copy foobar bazquo"
  },
  {
    "code": "def attach_ide_drive(self, vm_name, path, ctrller_addr, drive_addr, drive_type=constants.IDE_DISK):\n    vm = self._lookup_vm_check(vm_name)\n    ctrller_path = self._get_vm_ide_controller(vm, ctrller_addr)\n    if (drive_type == constants.IDE_DISK):\n        res_sub_type = self._DISK_RES_SUB_TYPE\n    elif (drive_type == constants.IDE_DVD):\n        res_sub_type = self._DVD_RES_SUB_TYPE\n    drive = self._get_new_resource_setting_data(res_sub_type)\n    drive.Parent = ctrller_path\n    drive.Address = drive_addr\n    drive.AddressOnParent = drive_addr\n    new_resources = self._add_virt_resource(drive, vm.path_())\n    drive_path = new_resources[0]\n    if (drive_type == constants.IDE_DISK):\n        res_sub_type = self._IDE_DISK_RES_SUB_TYPE\n    elif (drive_type == constants.IDE_DVD):\n        res_sub_type = self._IDE_DVD_RES_SUB_TYPE\n    res = self._get_new_resource_setting_data(res_sub_type, self._STORAGE_ALLOC_SETTING_DATA_CLASS)\n    res.Parent = drive_path\n    res.HostResource = [path]\n    self._add_virt_resource(res, vm.path_())",
    "nl": "Create an IDE drive and attach it to the vm.",
    "original_nl": "Create an IDE drive and attach it to the vm."
  },
  {
    "code": "def chStatus(cid, status, rdb, r_server, logger):\n    success = False\n    cacheonly = False\n    try:\n        results = r.table('monitors').get(cid).update({\n            'status': status,\n        }).run(rdb)\n        if (results['replaced'] == 1):\n            success = True\n            cacheonly = False\n        else:\n            success = False\n    except (RqlDriverError, RqlRuntimeError) as e:\n        success = False\n        cacheonly = True\n        line = ('chstatus: RethinkDB is inaccessible cannot change status for %s' % cid)\n        logger.info(line)\n        line = ('chstatus: RethinkDB Error: %s' % e.message)\n        logger.info(line)\n    try:\n        r_server.set((('monitor:' + cid) + ':status'), status)\n        success = True\n    except:\n        line = ('chstatus: Redis is inaccessible cannot change status for %s' % cid)\n        logger.info(line)\n        success = False\n    return success",
    "nl": "This method will be called to change a users status in the db",
    "original_nl": "This method will be called to change a users status in the db"
  },
  {
    "code": "def test_to_xml_element():\n    C = nuclide.Nuclide()\n    C.name = 'C'\n    C.half_life = 0.123\n    C.decay_modes = [nuclide.DecayTuple('beta-', 'B', 0.99), nuclide.DecayTuple('alpha', 'D', 0.01)]\n    C.reactions = [nuclide.ReactionTuple('fission', None, 200000000.0, 1.0), nuclide.ReactionTuple('(n,gamma)', 'A', 0.0, 1.0)]\n    C.yield_energies = [0.0253]\n    C.yield_data = {\n        0.0253: [('A', 0.0292737), ('B', 0.002566345)],\n    }\n    element = C.to_xml_element()\n    assert (element.get('half_life') == '0.123')\n    decay_elems = element.findall('decay')\n    assert (len(decay_elems) == 2)\n    assert (decay_elems[0].get('type') == 'beta-')\n    assert (decay_elems[0].get('target') == 'B')\n    assert (decay_elems[0].get('branching_ratio') == '0.99')\n    assert (decay_elems[1].get('type') == 'alpha')\n    assert (decay_elems[1].get('target') == 'D')\n    assert (decay_elems[1].get('branching_ratio') == '0.01')\n    rx_elems = element.findall('reaction')\n    assert (len(rx_elems) == 2)\n    assert (rx_elems[0].get('type') == 'fission')\n    assert (float(rx_elems[0].get('Q')) == 200000000.0)\n    assert (rx_elems[1].get('type') == '(n,gamma)')\n    assert (rx_elems[1].get('target') == 'A')\n    assert (float(rx_elems[1].get('Q')) == 0.0)\n    assert (element.find('neutron_fission_yields') is not None)",
    "nl": "Test writing nuclide data to an XML element.",
    "original_nl": "Test writing nuclide data to an XML element."
  },
  {
    "code": "def should_restart(self, config):\n    net_ids = set([net.id for net in config.networks if net.is_tenant_network])\n    try:\n        config_dict = json.load(open(CONF_PATH))\n    except:\n        return True\n    orchestrator_addr = config_dict.get('orchestrator_metadata_address')\n    orchestrator_port = config_dict.get('orchestrator_metadata_port')\n    return ((net_ids != set(config_dict.get('networks', {\n        \n    }).keys())) or (orchestrator_addr != config.metadata_address) or (orchestrator_port != config.metadata_port))",
    "nl": "This function determines if the networks have changed since <config>\nwas initialized.\n\n:type config: astara_router.models.Configuration\n",
    "original_nl": "This function determines if the networks have changed since <config>\nwas initialized.\n\n:type config: astara_router.models.Configuration\n:param config: An astara_router.models.Configuration object containing\n               the current configuration of the system's networks.\n:rtype: bool"
  },
  {
    "code": "def download_file(self, url, data='', more_headers={\n    \n}):\n    (content, headers) = self.perform_request_next(url, data, more_headers, response_headers=['Content-Disposition'])\n    if (not content):\n        from api.exceptions import FailedDownloadingSubtitleBuffer\n        raise FailedDownloadingSubtitleBuffer(('Failed downloading: %s' % url))\n    file_name = ''\n    if ((not ('Content-Disposition' in headers)) or ('filename' not in headers['Content-Disposition'])):\n        logger.debug('Failed getting the file name for the download.')\n        splitted_url = url.rsplit('/', 1)\n        if (len(splitted_url) == 2):\n            file_name = splitted_url[1]\n    else:\n        file_name = utils.take_first(utils.get_regex_results(headers['Content-Disposition'], '(?<=filename\\\\=).*(?=$)'))\n        file_name = file_name.strip('\"\\'')\n    logger.debug(('Downloaded file name is: %s' % file_name))\n    return (file_name, content)",
    "nl": "Downloads the file from the given URL. Returns the name for the \ndownloaded file, and the file buffer itself.\n\nTries to extract the file name from the Content-Disposition header, \notherwise, the last portion of the URL is returned.",
    "original_nl": "Downloads the file from the given URL. Returns the name for the \ndownloaded file, and the file buffer itself.\n\nTries to extract the file name from the Content-Disposition header, \notherwise, the last portion of the URL is returned."
  },
  {
    "code": "def _build_graph(sess, tf_dtype):\n    x = tf.placeholder(tf_dtype, shape=[None, _tensor_size], name=_tensor_input_name)\n    _ = tf.reduce_max(x, axis=1, name=_tensor_output_name)",
    "nl": "Given a session (implicitly), adds nodes of computations\n\nIt takes a vector input, with `_tensor_size` columns and returns an float64 scalar.",
    "original_nl": "Given a session (implicitly), adds nodes of computations\n\nIt takes a vector input, with `_tensor_size` columns and returns an float64 scalar."
  },
  {
    "code": "def empty():\n    for f in range(6):\n        for row in range(3):\n            for col in range(3):\n                if ((row != 1) or (col != 1)):\n                    canvas.itemconfig(facelet_id[f][row][col], fill='grey')",
    "nl": "Removes the facelet colors except the center facelets colors.",
    "original_nl": "Removes the facelet colors except the center facelets colors."
  },
  {
    "code": "@staticmethod\ndef _is_a_match(frame_occurrence_part, official_frame_part):\n    if isinstance(official_frame_part['elem'], set):\n        return (frame_occurrence_part['elem'] in official_frame_part['elem'])\n    else:\n        return (frame_occurrence_part['elem'] == official_frame_part['elem'])",
    "nl": "Tell wether two elements can be considered as a match\n\nframe_occurrence_part is a seen element, while official_frame_elem can\ncontain a set of possible elements, such as prepositions",
    "original_nl": "Tell wether two elements can be considered as a match\n\nframe_occurrence_part is a seen element, while official_frame_elem can\ncontain a set of possible elements, such as prepositions"
  },
  {
    "code": "def callb(x):\n    callback(Wtrafoinv(x))",
    "nl": "Callback that displays the inverse wavelet transform of current iter.",
    "original_nl": "Callback that displays the inverse wavelet transform of current iter."
  },
  {
    "code": "@line_magic\ndef lprun(self, parameter_s=''):\n    opts_def = Struct(D=[''], T=[''], f=[], m=[], u=None)\n    parameter_s = parameter_s.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n    (opts, arg_str) = self.parse_options(parameter_s, 'rsf:m:D:T:u:', list_all=True)\n    opts.merge(opts_def)\n    global_ns = self.shell.user_global_ns\n    local_ns = self.shell.user_ns\n    funcs = []\n    for name in opts.f:\n        try:\n            funcs.append(eval(name, global_ns, local_ns))\n        except Exception as e:\n            raise UsageError(('Could not find function %r.\\n%s: %s' % (name, e.__class__.__name__, e)))\n    profile = LineProfiler(*funcs)\n    for modname in opts.m:\n        try:\n            mod = __import__(modname, fromlist=[''])\n            profile.add_module(mod)\n        except Exception as e:\n            raise UsageError(('Could not find module %r.\\n%s: %s' % (modname, e.__class__.__name__, e)))\n    if (opts.u is not None):\n        try:\n            output_unit = float(opts.u[0])\n        except Exception as e:\n            raise TypeError('Timer unit setting must be a float.')\n    else:\n        output_unit = None\n    if PY3:\n        import builtins\n    else:\n        import __builtin__ as builtins\n    if ('profile' in builtins.__dict__):\n        had_profile = True\n        old_profile = builtins.__dict__['profile']\n    else:\n        had_profile = False\n        old_profile = None\n    builtins.__dict__['profile'] = profile\n    try:\n        try:\n            profile.runctx(arg_str, global_ns, local_ns)\n            message = ''\n        except SystemExit:\n            message = '*** SystemExit exception caught in code being profiled.'\n        except KeyboardInterrupt:\n            message = '*** KeyboardInterrupt exception caught in code being profiled.'\n    finally:\n        if had_profile:\n            builtins.__dict__['profile'] = old_profile\n    stdout_trap = StringIO()\n    profile.print_stats(stdout_trap, output_unit=output_unit, stripzeros=('s' in opts))\n    output = stdout_trap.getvalue()\n    output = output.rstrip()\n    page(output)\n    print(message, end='')\n    dump_file = opts.D[0]\n    if dump_file:\n        profile.dump_stats(dump_file)\n        print(('\\n*** Profile stats pickled to file %r. %s' % (dump_file, message)))\n    text_file = opts.T[0]\n    if text_file:\n        pfile = open(text_file, 'w')\n        pfile.write(output)\n        pfile.close()\n        print(('\\n*** Profile printout saved to text file %r. %s' % (text_file, message)))\n    return_value = None\n    if ('r' in opts):\n        return_value = profile\n    return return_value",
    "nl": "Execute a statement under the line-by-line profiler from the\nline_profiler module.\n\nUsage:\n  %lprun -f func1 -f func2 <statement>\n\nThe given statement (which doesn't require quote marks) is run via the\nLineProfiler. Profiling is enabled for the functions specified by the -f\noptions. The statistics will be shown side-by-side with the code through the\npager once the statement has completed.\n\nOptions:\n\n-f <function>: LineProfiler only profiles functions and methods it is told\nto profile.  This option tells the profiler about these functions. Multiple\n-f options may be used. The argument may be any expression that gives\na Python function or method object. However, one must be careful to avoid\nspaces that may confuse the option parser.\n\n-m <module>: Get all the functions/methods in a module\n\nOne or more -f or -m options are required to get any useful results.\n\n-D <filename>: dump the raw statistics out to a pickle file on disk. The\nusual extension for this is \".lprof\". These statistics may be viewed later\nby running line_profiler.py as a script.\n\n-T <filename>: dump the text-formatted statistics with the code side-by-side\nout to a text file.\n\n-r: return the LineProfiler object after it has completed profiling.\n\n-s: strip out all entries from the print-out that have zeros.\n\n-u: specify time unit for the print-out in seconds.",
    "original_nl": "Execute a statement under the line-by-line profiler from the\nline_profiler module.\n\nUsage:\n  %lprun -f func1 -f func2 <statement>\n\nThe given statement (which doesn't require quote marks) is run via the\nLineProfiler. Profiling is enabled for the functions specified by the -f\noptions. The statistics will be shown side-by-side with the code through the\npager once the statement has completed.\n\nOptions:\n\n-f <function>: LineProfiler only profiles functions and methods it is told\nto profile.  This option tells the profiler about these functions. Multiple\n-f options may be used. The argument may be any expression that gives\na Python function or method object. However, one must be careful to avoid\nspaces that may confuse the option parser.\n\n-m <module>: Get all the functions/methods in a module\n\nOne or more -f or -m options are required to get any useful results.\n\n-D <filename>: dump the raw statistics out to a pickle file on disk. The\nusual extension for this is \".lprof\". These statistics may be viewed later\nby running line_profiler.py as a script.\n\n-T <filename>: dump the text-formatted statistics with the code side-by-side\nout to a text file.\n\n-r: return the LineProfiler object after it has completed profiling.\n\n-s: strip out all entries from the print-out that have zeros.\n\n-u: specify time unit for the print-out in seconds."
  },
  {
    "code": "def test_json(self):\n    self.assertEqual(self.prop.from_json(self.prop.to_json()), self.prop)",
    "nl": "Test json related methods.",
    "original_nl": "Test json related methods."
  },
  {
    "code": "@ops.RegisterGradient('MatrixInverse')\ndef _MatrixInverseGrad(op, grad):\n    ainv = op.outputs[0]\n    return (- math_ops.matmul(ainv, math_ops.matmul(grad, ainv, transpose_b=True), transpose_a=True))",
    "nl": "Gradient for MatrixInverse.",
    "original_nl": "Gradient for MatrixInverse."
  },
  {
    "code": "def __init__(self, contents, **kwargs):\n    super(BokehPlotBlock, self).__init__(**kwargs)\n    if (not isinstance(contents, BokehFigure)):\n        raise ValueError('Expected bokeh.plotting.figure.Figure type but got %s', type(contents))\n    self._contents = file_html(contents, INLINE, 'test')",
    "nl": "Writes out the content as raw text or HTML.\n\n",
    "original_nl": "Writes out the content as raw text or HTML.\n\n:param contents: Bokeh plotting figure.\n:param kwargs: Optional styling arguments. The `style` keyword argument has special\n               meaning in that it allows styling to be grouped as one argument.\n               It is also useful in case a styling parameter name clashes with a standard\n               block parameter."
  },
  {
    "code": "def register_definition_helper(self, func):\n    self._definition_helpers.append(func)",
    "nl": "Register a new definition helper. The helper **must** meet the following conditions:\n\n- Receive the `APISpec` instance as the first argument.\n- Receive the definition `name` as the second argument.\n- Include ``**kwargs`` in its signature.\n- Return a `dict` representation of the definition's Schema object.\n\nThe helper may define any named arguments after the `name` argument.\n``kwargs`` will include (among other things):\n- definition (dict): current state of the definition\n\nhttps://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#definitionsObject\n\n",
    "original_nl": "Register a new definition helper. The helper **must** meet the following conditions:\n\n- Receive the `APISpec` instance as the first argument.\n- Receive the definition `name` as the second argument.\n- Include ``**kwargs`` in its signature.\n- Return a `dict` representation of the definition's Schema object.\n\nThe helper may define any named arguments after the `name` argument.\n``kwargs`` will include (among other things):\n- definition (dict): current state of the definition\n\nhttps://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#definitionsObject\n\n:param callable func: The definition helper function."
  },
  {
    "code": "def test_creating_tools_edition_panel(exopy_qtbot, edition_view, dialog_sleep):\n    edition_view.show()\n    wait_for_window_displayed(exopy_qtbot, edition_view)\n    exopy_qtbot.wait(dialog_sleep)\n    ed = edition_view.widget.dock_widget().widgets()[0]\n    btn = ed.widgets()[4]\n    btn.clicked = True\n\n    def assert_created():\n        assert (len(edition_view.area.dock_items()) == 2)\n    exopy_qtbot.wait_until(assert_created)",
    "nl": "Test creating the tool edition panel using the button.",
    "original_nl": "Test creating the tool edition panel using the button."
  },
  {
    "code": "def feed_words(self, text):\n    for word in re.findall('(\\\\w+)', text, re.UNICODE):\n        word = word.lower()\n        count = self.words.get(word, 0)\n        count += 1\n        self.words[word] = count",
    "nl": "Add new words using the provided text.\n\nEach word in this text will be added to the list of words for\na future auto-completion.",
    "original_nl": "Add new words using the provided text.\n\nEach word in this text will be added to the list of words for\na future auto-completion."
  },
  {
    "code": "def test_Trace_record():\n    t = Trace(variables=['step', 'r'])\n    t.record(step=1, r=[1.0, 2.0])\n    t.record(step=2, r=[15.0, 2.0])\n    t.record(step=3, r=[4.0, 2.0])\n    assert (len(t['step']) == 3)\n    assert (len(t['r']) == 3)\n    t.record(step=4)\n    assert (len(t['r']) == 3)\n    assert (len(t['step']) == 4)\n    assert_raises(KeyError, t.record, step=1, g=[1.0, 2.0, 3.0])",
    "nl": "Test record function of Trace",
    "original_nl": "Test record function of Trace"
  },
  {
    "code": "def close(self):\n    logger.info('Closing open sockets ...')\n    for scheduler in self.scheduler_pool:\n        scheduler.close()\n    logger.info('Worker was closed successfully ...')",
    "nl": "Unregister open sockets from poller objects and close them.",
    "original_nl": "Unregister open sockets from poller objects and close them."
  },
  {
    "code": "def get_brick_lookup_options(self):\n    lookup = self.kwargs[(self.brick_lookup_url_kwarg or self.lookup_url_kwarg or self.lookup_field)]\n    return dict(part_name=lookup)",
    "nl": "Parses kwargs and returns corresponding querying keyword arguments.",
    "original_nl": "Parses kwargs and returns corresponding querying keyword arguments."
  },
  {
    "code": "def indexBaseTick(na, nb, pagesize, pna):\n    indlista = range(na)\n    indlistb = range(nb)\n    pnb = (pagesize - pna)\n    ma = (na - pna)\n    mb = (nb - pnb)\n    (npagea, npageb) = (0, 0)\n    if (ma > 0):\n        npagea = ((ma / pagesize) + sign((ma % pagesize)))\n    if (mb > 0):\n        npageb = ((mb / pagesize) + sign((mb % pagesize)))\n    ipages = range((- npageb), (npagea + 1))\n    yindex = {\n        \n    }\n    ybases = {\n        \n    }\n    yticks = {\n        \n    }\n    ia1 = min(pna, na)\n    ib0 = max(0, (nb - pnb))\n    inda = range(ia1)\n    indb = range(ib0, nb)\n    yindex[0] = [inda, indb]\n    for ipage in range(1, (npagea + 1)):\n        i1 = (pna + (ipage * pagesize))\n        i0 = (i1 - pagesize)\n        inda = range(i0, min(i1, na))\n        yindex[ipage] = [inda, []]\n    for ipage in range((- npageb), 0):\n        i1 = ((ib0 + (ipage * pagesize)) + pagesize)\n        i0 = max(0, (i1 - pagesize))\n        indb = range(i0, i1)\n        yindex[ipage] = [[], indb]\n    for ipage in range((- npageb), (npagea + 1)):\n        ybases[ipage] = [[], []]\n        yticks[ipage] = [[], []]\n    for ipage in range(0, (npagea + 1)):\n        ybases[ipage][0] = [((- 1) - ind) for ind in yindex[ipage][0]]\n        yticks[ipage][0] = [(1 + ind) for ind in yindex[ipage][0]]\n    for ipage in range((- npageb), 1):\n        ybases[ipage][1] = [(nb - ind) for ind in yindex[ipage][1]]\n        yticks[ipage][1] = [(ind - nb) for ind in yindex[ipage][1]]\n    return (ipages, yindex, ybases, yticks)",
    "nl": "Indexing for page navigation with two lists of length na and nb.\n\n Example:\n         list b (nb=5)    list a (na=11)\n         [ 0, 1, 2, 3, 4] [0,  1, 2, 3, 4, 5, 6, 7, 8,  9, 10] <-- yindex\n         [ 5, 4, 3, 2, 1] [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11] <-- ybases\n         [-5,-4,-3,-2,-1] [1,  2, 3, 4, 5, 6, 7, 8, 9, 10, 11] <-- yticks\n--page -1] [----page 0----] [---page 1---] [---page 2--- \n              [pnb=2] [pna=3 ]                   \n\n         yindex for na and nb:\n         {-1: [[], [0, 1, 2]],\n          0: [[0, 1, 2], [3, 4]],\n          1: [[3, 4, 5, 6, 7], []],\n          2: [[8, 9, 10, 11, 12], []]}\n\n         yybase:\n         {-1: [[], [5, 4, 3]],\n          0: [[-1, -2, -3], [2, 1]],\n          1: [[-4, -5, -6, -7, -8], []],\n          2: [[-9, -10, -11, -12, -13], []]}\n\n         yticks:\n         {-1: [[], [-5, -4, -3]],\n          0: [[1, 2, 3], [-2, -1]],\n          1: [[4, 5, 6, 7, 8], []],\n          2: [[9, 10, 11, 12, 13], []]}",
    "original_nl": "Indexing for page navigation with two lists of length na and nb.\n\n Example:\n         list b (nb=5)    list a (na=11)\n         [ 0, 1, 2, 3, 4] [0,  1, 2, 3, 4, 5, 6, 7, 8,  9, 10] <-- yindex\n         [ 5, 4, 3, 2, 1] [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11] <-- ybases\n         [-5,-4,-3,-2,-1] [1,  2, 3, 4, 5, 6, 7, 8, 9, 10, 11] <-- yticks\n--page -1] [----page 0----] [---page 1---] [---page 2--- \n              [pnb=2] [pna=3 ]                   \n\n         yindex for na and nb:\n         {-1: [[], [0, 1, 2]],\n          0: [[0, 1, 2], [3, 4]],\n          1: [[3, 4, 5, 6, 7], []],\n          2: [[8, 9, 10, 11, 12], []]}\n\n         yybase:\n         {-1: [[], [5, 4, 3]],\n          0: [[-1, -2, -3], [2, 1]],\n          1: [[-4, -5, -6, -7, -8], []],\n          2: [[-9, -10, -11, -12, -13], []]}\n\n         yticks:\n         {-1: [[], [-5, -4, -3]],\n          0: [[1, 2, 3], [-2, -1]],\n          1: [[4, 5, 6, 7, 8], []],\n          2: [[9, 10, 11, 12, 13], []]}"
  },
  {
    "code": "def find_seamicro15k_servers(ip, username, password, power_control):\n    api_versions = select_seamicro15k_api_version(power_control)\n    for version in api_versions:\n        servers = get_seamicro15k_servers(version, ip, username, password)\n        if (servers is not None):\n            return servers\n    raise SeaMicroError('Failure to retrieve servers.')",
    "nl": "Returns the list of servers, using the latest supported api version.",
    "original_nl": "Returns the list of servers, using the latest supported api version."
  },
  {
    "code": "def _get_strategy_function(self, locator):\n    locator_function = STRATEGIES.get(locator.lower().replace(' ', ''), None)\n    return (getattr(self, locator_function) if locator_function else None)",
    "nl": "Get the function for the provided locator",
    "original_nl": "Get the function for the provided locator"
  },
  {
    "code": "def _sfn(l, mask, myrad, bcast_var):\n    clf = bcast_var[2]\n    data = l[0][mask, :].T\n    skf = model_selection.StratifiedKFold(n_splits=bcast_var[1], shuffle=False)\n    accuracy = np.mean(model_selection.cross_val_score(clf, data, y=bcast_var[0], cv=skf, n_jobs=1))\n    return accuracy",
    "nl": "Score classifier on searchlight data using cross-validation.\n\nThe classifier is in `bcast_var[2]`. The labels are in `bast_var[0]`. The\nnumber of cross-validation folds is in `bast_var[1].",
    "original_nl": "Score classifier on searchlight data using cross-validation.\n\nThe classifier is in `bcast_var[2]`. The labels are in `bast_var[0]`. The\nnumber of cross-validation folds is in `bast_var[1]."
  },
  {
    "code": "def enumerate_fresh_vars_outside(fvs):\n    next_i = 0\n    while True:\n        next_i += 1\n        next_var = hslowlevel.HsType(('a' + str(next_i)))\n        if (next_var not in fvs):\n            (yield next_var)",
    "nl": "Given a set fvs of free variables (represented as strings) that\nare deemed already 'used', return a Python generater which yields\nan infinite succession of legal Haskell free variables which are\nnot 'used'.",
    "original_nl": "Given a set fvs of free variables (represented as strings) that\nare deemed already 'used', return a Python generater which yields\nan infinite succession of legal Haskell free variables which are\nnot 'used'."
  },
  {
    "code": "def error_message(self, error):\n    if (error == QProcess.FailedToStart):\n        self.message(_('Could not start {program}.\\nPlease check path and permissions.').format(program=self.command[0]), FAILURE)\n    elif (error == QProcess.ReadError):\n        self.message(_('Could not read from the process.'), FAILURE)\n    elif (self._process.state() == QProcess.NotRunning):\n        self.message(_('An unknown error occurred.'), FAILURE)",
    "nl": "Called when there is an error (by _error()).\n\nOutputs a message describing the given QProcess.Error.",
    "original_nl": "Called when there is an error (by _error()).\n\nOutputs a message describing the given QProcess.Error."
  },
  {
    "code": "def keys(self, sort=True, limit=100):\n    return [key[0] for key in self.key_details(sort=sort, limit=limit)]",
    "nl": "Return a list of keys in use",
    "original_nl": "Return a list of keys in use"
  },
  {
    "code": "@property\ndef AUTHORIZATION_URL(self):\n    url_root = self.setting('PUBLIC_URL_ROOT')\n    if (not url_root):\n        url_root = self.setting('URL_ROOT')\n    return '{}/authorize/'.format(url_root)",
    "nl": "URL of the auth provider's authorization endpoint.",
    "original_nl": "URL of the auth provider's authorization endpoint."
  },
  {
    "code": "def reaction_formula(reaction, compound_formula):\n\n    def multiply_formula(compound_list):\n        for (compound, count) in compound_list:\n            (yield (count * compound_formula[compound.name]))\n    for (compound, _) in reaction.compounds:\n        if (compound.name not in compound_formula):\n            return None\n    else:\n        left_form = reduce(operator.or_, multiply_formula(reaction.left), Formula())\n        right_form = reduce(operator.or_, multiply_formula(reaction.right), Formula())\n    return (left_form, right_form)",
    "nl": "Calculate formula compositions for both sides of the specified reaction.\n\nIf the compounds in the reaction all have formula, then calculate and",
    "original_nl": "Calculate formula compositions for both sides of the specified reaction.\n\nIf the compounds in the reaction all have formula, then calculate and\nreturn the chemical compositions for both sides, otherwise return `None`.\n\nArgs:\n    reaction: :class:`psamm.reaction.Reaction`.\n    compound_formula: a map from compound id to formula."
  },
  {
    "code": "def get_rms_map_qual(var):\n    return _safe_single_attr(var.INFO.get('MQ'))",
    "nl": "Return the RMS mapping quality,\nor None if it isn't present in the VCF.",
    "original_nl": "Return the RMS mapping quality,\nor None if it isn't present in the VCF."
  },
  {
    "code": "def make_venv(self, venv_dir):\n    lgr.debug('Creating virtualenv in {0}'.format(venv_dir))\n    return sh.virtualenv(venv_dir)",
    "nl": "creates a virtualenv\n\n",
    "original_nl": "creates a virtualenv\n\n:param string venv_dir: venv path to create"
  },
  {
    "code": "def create_workbook_with_sheet(name):\n    book = xlwt.Workbook()\n    valid_name = re.sub('[^\\\\.0-9a-zA-Z]+', '', os.path.basename(name))\n    sheet1 = book.add_sheet(valid_name)\n    return (book, sheet1)",
    "nl": "Removes non-alpha-numerical values in name.",
    "original_nl": "Removes non-alpha-numerical values in name."
  },
  {
    "code": "def finish(self):\n    return self.find_and_kill_process('tor -f /etc/tor/torrc-raspap', self.interface)",
    "nl": "Shutdown TOR\n",
    "original_nl": "Shutdown TOR\n:return:"
  },
  {
    "code": "def expectScreen(self, filename, maxrms=0):\n    log.debug('expectScreen %s', filename)\n    return self._expectFramebuffer(filename, 0, 0, maxrms)",
    "nl": "Wait until the display matches a target image\n\nfilename: an image file to read and compare against\nmaxrms: the maximum root mean square between histograms of the\n        screen and target image",
    "original_nl": "Wait until the display matches a target image\n\nfilename: an image file to read and compare against\nmaxrms: the maximum root mean square between histograms of the\n        screen and target image"
  },
  {
    "code": "def test_link(self):\n    expected = 'www.example.com'\n    self.assertEqual(expected, md.gen_link('www.example.com'))",
    "nl": "Simple link generation where only the URL is provided.",
    "original_nl": "Simple link generation where only the URL is provided."
  },
  {
    "code": "def grab_files(directory):\n    if (not os.path.isdir(directory)):\n        return\n        (yield)\n    else:\n        for name in os.listdir(directory):\n            full_path = os.path.join(directory, name)\n            if os.path.isdir(full_path):\n                for entry in grab_files(full_path):\n                    (yield entry)\n            elif os.path.isfile(full_path):\n                (yield full_path)",
    "nl": "Generator that returns all files in a directory.",
    "original_nl": "Generator that returns all files in a directory."
  },
  {
    "code": "def unregister(self, collector):\n    if self.is_registered:\n        driver_id = self.id\n        try:\n            del collector.contributions[driver_id]\n        except KeyError:\n            pass\n        self.is_registered = False",
    "nl": "Remove contributed infos from the collector.",
    "original_nl": "Remove contributed infos from the collector."
  },
  {
    "code": "def handle_msg(self, msg_text, _chan, nick):\n    lol_regexp = '[lI1]+[o0u]+[lI1]+z?'\n    lulz = len(re.findall(lol_regexp, msg_text, flags=re.IGNORECASE))\n    if ((lulz > 0) and (not msg_text.startswith('!'))):\n        current_ts = TimeSlice()\n        if (current_ts != self.lol_rate[0]):\n            self.lol_rate.insert(0, current_ts)\n        if (len(self.lol_rate) > self.bot.lolRateDepth):\n            self.lol_rate.pop()\n        self.lol_rate[0].lol(nick, lulz)",
    "nl": "If msg_text matches the lol regexp,\nincrement the lolness for the current timeslice.",
    "original_nl": "If msg_text matches the lol regexp,\nincrement the lolness for the current timeslice."
  },
  {
    "code": "def check_network_connection(server='www.baidu.com'):\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Checking network connection to server '%s'...\", server)\n    try:\n        host = socket.gethostbyname(server)\n        socket.create_connection((host, 80), 2)\n    except Exception:\n        logger.debug('Network connection not working')\n        return False\n    else:\n        logger.debug('Network connection working')\n        return True",
    "nl": "@Brief:\n    Checks if pi can connect a network server.\n\n@Arguments:\n    server -- (optional) the server to connect with (Default:\n              \"www.baidu.com\")\n\n@Returns:\n    True or False",
    "original_nl": "@Brief:\n    Checks if pi can connect a network server.\n\n@Arguments:\n    server -- (optional) the server to connect with (Default:\n              \"www.baidu.com\")\n\n@Returns:\n    True or False"
  },
  {
    "code": "def __getattr__(self, item):\n    return getattr(self.desc, item)",
    "nl": "Redirects unknown attribute lookups to the wrapped descriptor\n",
    "original_nl": "Redirects unknown attribute lookups to the wrapped descriptor\n:param item: attribute being looked up"
  },
  {
    "code": "def _get_item_from_module(module_name, item_name):\n    try:\n        module = __import__(module_name, fromlist=[item_name])\n        klass = getattr(module, item_name)\n    except ImportError as error:\n        message = 'Module \"{modulename}\" could not be loaded: {e}'\n        raise Exception(message.format(modulename=module_name, e=error))\n    except AttributeError as error:\n        message = 'No item \"{itemname}\" in module \"{modulename}\": {e}'\n        raise Exception(message.format(modulename=module_name, itemname=item_name, e=error))\n    return klass",
    "nl": "Load classes/modules/functions/... from given config",
    "original_nl": "Load classes/modules/functions/... from given config"
  },
  {
    "code": "@QtCore.pyqtSlot(QgsVectorLayer)\ndef _vlayer_modified(self, vlayer):\n    logger.debug('Vector layer {v} was modified'.format(v=vlayer.id()))\n    idx = self.combox_vector.findData(vlayer.id())\n    if (idx == self.combox_vector.currentIndex()):\n        self._vlayer_changed(idx)",
    "nl": "Push an update when a vector layer has been modified\n\nNote that this only takes action if `vlayer` modified is one currently\nselected.\n\nArgs:\n  vlayer (QgsVectorLayer): vector layer modified",
    "original_nl": "Push an update when a vector layer has been modified\n\nNote that this only takes action if `vlayer` modified is one currently\nselected.\n\nArgs:\n  vlayer (QgsVectorLayer): vector layer modified"
  },
  {
    "code": "def world_to_pixel(self, x, y):\n    sx = self.scale\n    sy = (- sx)\n    (ogx, ogy) = self.__offset\n    return (((x + ogx) / sx), ((y + ogy) / sy))",
    "nl": "Returns the pixel of the world coordinate (x,y).\n\nEg.\n>>> view = WorldToViewTransform([0,0,100,100],500,500)\n>>> view.world_to_pixel(0,0)\n(0.0, 500.0)\n>>> view.world_to_pixel(100,100)\n(500.0, -0.0)",
    "original_nl": "Returns the pixel of the world coordinate (x,y).\n\nEg.\n>>> view = WorldToViewTransform([0,0,100,100],500,500)\n>>> view.world_to_pixel(0,0)\n(0.0, 500.0)\n>>> view.world_to_pixel(100,100)\n(500.0, -0.0)"
  },
  {
    "code": "@classmethod\ndef lookup(cls, value):\n    try:\n        return cls(value)\n    except ValueError:\n        return cls[value]",
    "nl": "Look up an enumeration by name or value.",
    "original_nl": "Look up an enumeration by name or value."
  },
  {
    "code": "@pytest.mark.parametrize('kernel', KERNELS)\ndef test_centered_makekernel(self, kernel):\n    shape = kernel.array.shape\n    x = np.zeros(shape)\n    xslice = [slice((sh // 2), ((sh // 2) + 1)) for sh in shape]\n    x[xslice] = 1.0\n    c2 = convolve_fft(x, kernel, boundary='fill')\n    c1 = convolve(x, kernel, boundary='fill')\n    assert_almost_equal(c1, c2, decimal=12)",
    "nl": "Test smoothing of an image with a single positive pixel",
    "original_nl": "Test smoothing of an image with a single positive pixel"
  },
  {
    "code": "def test_push_pop1(self):\n    q = self.queue()\n    q.push('a')\n    q.push('b')\n    q.push('c')\n    self.assertEqual(q.pop(), 'a')\n    self.assertEqual(q.pop(), 'b')\n    self.assertEqual(q.pop(), 'c')\n    self.assertEqual(q.pop(), None)",
    "nl": "Basic push/pop test",
    "original_nl": "Basic push/pop test"
  },
  {
    "code": "def converter_obj(expected, optional=False):\n\n    def converter(value):\n        if (optional and (value is None)):\n            return None\n        if isinstance(value, expected):\n            return value\n        elif isinstance(value, dict):\n            return expected(**value)\n        else:\n            raise TypeError(('%r is not of type %s or dict' % (value, expected)))\n    return converter",
    "nl": "Convert the given value to an object of type `expected`.",
    "original_nl": "Convert the given value to an object of type `expected`."
  },
  {
    "code": "def is_protected_type(obj):\n    import Decimal\n    import datetime\n    return isinstance(obj, (integer_types + (type(None), float, Decimal, datetime.datetime, datetime.date, datetime.time)))",
    "nl": "Determine if the object instance is of a protected type.\n\nObjects of protected types are preserved as-is when passed to\nforce_text(strings_only=True).",
    "original_nl": "Determine if the object instance is of a protected type.\n\nObjects of protected types are preserved as-is when passed to\nforce_text(strings_only=True)."
  },
  {
    "code": "def setPathSearch(self, path_search):\n    self.settingsStore('path_search', qvariant_converter.convertBool(path_search))",
    "nl": "Stores the path search value in the QSettings.",
    "original_nl": "Stores the path search value in the QSettings."
  },
  {
    "code": "def set_logger(name, level='INFO', fmt=None, datefmt=None, propagate=1, remove_handlers=False):\n    logger = logging.getLogger(name)\n    logger.setLevel(getattr(logging, level))\n    logger.propagate = propagate\n    if remove_handlers:\n        logger.handlers = []\n        return\n    handler = None\n    for h in logger.handlers:\n        if isinstance(h, logging.StreamHandler):\n            handler = h\n            break\n    if (not handler):\n        handler = logging.StreamHandler()\n        logger.addHandler(handler)\n    formatter_kwgs = {\n        \n    }\n    for i in ('fmt', 'datefmt'):\n        if (locals()[i] is not None):\n            formatter_kwgs[i] = locals()[i]\n    handler.setFormatter(BaseFormatter(**formatter_kwgs))",
    "nl": "This function will clear the previous handlers and set only one handler,\nwhich will only be StreamHandler for the logger.\n\nThis function is designed to be able to called multiple times in a context.\n\nNote that if a logger has no handlers, it will be added a handler automatically when it is used.",
    "original_nl": "This function will clear the previous handlers and set only one handler,\nwhich will only be StreamHandler for the logger.\n\nThis function is designed to be able to called multiple times in a context.\n\nNote that if a logger has no handlers, it will be added a handler automatically when it is used."
  },
  {
    "code": "@mock.patch('treadmill.cli.admin.cloud.vpc.VPC')\ndef test_list_all_vpc(self, vpc_mock):\n    result = self.runner.invoke(self.configure_cli, ['--domain=treadmill.org', 'list', 'vpc'], obj={\n        \n    })\n    self.assertEqual(result.exit_code, 0)\n    vpc_mock.all.assert_called_once()",
    "nl": "Test cloud list all vpc",
    "original_nl": "Test cloud list all vpc"
  },
  {
    "code": "def search(self, query, size=1000, recent=False, days=0):\n    es = pyelasticsearch.ElasticSearch(self._url)\n    args = {\n        'size': size,\n    }\n    if (recent or days):\n        datefmt = self._indexfmt\n        now = datetime.datetime.utcnow()\n        indexes = []\n        latest_index = now.strftime(datefmt)\n        if self._is_valid_index(es, latest_index):\n            indexes.append(latest_index)\n        if recent:\n            lasthr = (now - datetime.timedelta(hours=1))\n            lasthr_index = lasthr.strftime(datefmt)\n            if (lasthr_index != latest_index):\n                if self._is_valid_index(es, lasthr_index):\n                    indexes.append(lasthr.strftime(datefmt))\n        for day in range(1, days):\n            lastday = (now - datetime.timedelta(days=day))\n            index_name = lastday.strftime(datefmt)\n            if self._is_valid_index(es, index_name):\n                indexes.append(index_name)\n        args['index'] = indexes\n    results = es.search(query, **args)\n    return ResultSet(results)",
    "nl": "Search an elasticsearch server.\n\n`query` parameter is the complicated query structure that\npyelasticsearch uses. More details in their documentation.\n\n`size` is the max number of results to return from the search\nengine. We default it to 1000 to ensure we don't loose things.\nFor certain classes of queries (like faceted ones), this can actually\nbe set very low, as it won't impact the facet counts.\n\n`recent` search only most recent indexe(s), assuming this is basically\na real time query that you only care about the last hour of time.\nUsing recent dramatically reduces the load on the ES cluster.\n\n`days` search only the last number of days.\n\nThe returned result is a ResultSet query.",
    "original_nl": "Search an elasticsearch server.\n\n`query` parameter is the complicated query structure that\npyelasticsearch uses. More details in their documentation.\n\n`size` is the max number of results to return from the search\nengine. We default it to 1000 to ensure we don't loose things.\nFor certain classes of queries (like faceted ones), this can actually\nbe set very low, as it won't impact the facet counts.\n\n`recent` search only most recent indexe(s), assuming this is basically\na real time query that you only care about the last hour of time.\nUsing recent dramatically reduces the load on the ES cluster.\n\n`days` search only the last number of days.\n\nThe returned result is a ResultSet query."
  },
  {
    "code": "@doc_public\ndef sine(self, f, duration):\n    scale = maxv(numbits=self.bits)\n    ret = []\n    T = (1.0 / self.rate)\n    N = ((self.rate * duration) // 1000)\n    for n in xrange(N):\n        t = (n * T)\n        vsin = math.sin((((t * f) * 2) * math.pi))\n        A = (scale * (self.amplitude / 100))\n        s = (A * vsin)\n        if ((scale == 65535) or (scale == 255)):\n            s = ((s + scale) / 2)\n        ret.append(int(s))\n    return ret",
    "nl": "Return a sine wave\n\n",
    "original_nl": "Return a sine wave\n\n@param f: frequency\n@type f: integer        \n\n@param duration: duration in milliseconds\n@type duration: integer \n\n@return: values sample \n@rtype: list of integer"
  },
  {
    "code": "def get(self, session_id):\n    return self._items.get(session_id, None)",
    "nl": "Return session object or None if it is not available\n\n`session_id`\n    Session identifier",
    "original_nl": "Return session object or None if it is not available\n\n`session_id`\n    Session identifier"
  },
  {
    "code": "def Update(self, data):\n    self._sha256_context.update(data)",
    "nl": "Updates the current state of the hasher with a new block of data.\n\nRepeated calls to update are equivalent to one single call with the\nconcatenation of the arguments.\n\nArgs:\n  data(bytes): block of data with which to update the context of the hasher.",
    "original_nl": "Updates the current state of the hasher with a new block of data.\n\nRepeated calls to update are equivalent to one single call with the\nconcatenation of the arguments.\n\nArgs:\n  data(bytes): block of data with which to update the context of the hasher."
  },
  {
    "code": "def _find_in_path(path, file):\n    for p in path.split(os.pathsep):\n        candidate = os.path.join(p, file)\n        if os.path.exists(os.path.join(p, file)):\n            return candidate\n    return False",
    "nl": "Find a file in a given path string.",
    "original_nl": "Find a file in a given path string."
  },
  {
    "code": "@templatefilter('hex')\ndef hexfilter(text):\n    return node.hex(text)",
    "nl": "Any text. Convert a binary Mercurial node identifier into\nits long hexadecimal representation.",
    "original_nl": "Any text. Convert a binary Mercurial node identifier into\nits long hexadecimal representation."
  },
  {
    "code": "def test_internal():\n    tests_passed = (subprocess.call(['./droopescan', 'test']) == 0)\n    if (not tests_passed):\n        f.error('Unit tests failed... abort.')",
    "nl": "Runs unit tests.",
    "original_nl": "Runs unit tests."
  },
  {
    "code": "def blackScholes(cp, s, k, t, r, d, v, full=False, comp=inf):\n    if (comp != inf):\n        r = toExp(r, comp)\n        d = toExp(d, comp)\n    f = (s * exp((t * (d - r))))\n    results = black(cp, f, k, t, r, v, full, comp=inf)\n    if (not full):\n        return results\n    else:\n        for (key, item) in results.items():\n            if (key in ['delta', 'gamma']):\n                item /= (f / s)\n        results['fwd'] = f\n        return results",
    "nl": "blackScholes, risk-free-rate and dividend-yield make up the forward rate\ncp   = \"c\" for a call, anything else assumes a put\nf    = Forward Price of the underlying asset\nk    = Strike Price\nt    = Time until maturity (in years)\nr    = Interest rate\nd    = Dividend yield\nv    = Implied volatility\ncomp = How many times interest is compounded a year (default = inf, i.e. continous rates)\nfull = If True, function returns a dictionary with price and all sensitivities\n       Otherwise only the calculated/calibrated parameter will be returned",
    "original_nl": "blackScholes, risk-free-rate and dividend-yield make up the forward rate\ncp   = \"c\" for a call, anything else assumes a put\nf    = Forward Price of the underlying asset\nk    = Strike Price\nt    = Time until maturity (in years)\nr    = Interest rate\nd    = Dividend yield\nv    = Implied volatility\ncomp = How many times interest is compounded a year (default = inf, i.e. continous rates)\nfull = If True, function returns a dictionary with price and all sensitivities\n       Otherwise only the calculated/calibrated parameter will be returned"
  },
  {
    "code": "def create_session(self, item=None):\n    return ItemCreateSessionRequestBuilder(self.append_to_request_url('upload.createSession'), self._client, item=item)",
    "nl": "Executes the createSession method\n\nArgs:\n    item (:class:`ChunkedUploadSessionDescriptor<onedrivesdk.model.chunked_upload_session_descriptor.ChunkedUploadSessionDescriptor>`):\n        Defaults to None, The item to use in the method request\n",
    "original_nl": "Executes the createSession method\n\nArgs:\n    item (:class:`ChunkedUploadSessionDescriptor<onedrivesdk.model.chunked_upload_session_descriptor.ChunkedUploadSessionDescriptor>`):\n        Defaults to None, The item to use in the method request\n\nReturns:\n    :class:`ItemCreateSessionRequestBuilder<onedrivesdk.request.item_create_session.ItemCreateSessionRequestBuilder>`:\n        A ItemCreateSessionRequestBuilder for the method"
  },
  {
    "code": "def getBrailleGenerator(self):\n    return None",
    "nl": "Returns the braille generator for this script.",
    "original_nl": "Returns the braille generator for this script."
  },
  {
    "code": "def getRequiredError(self):\n    raise NotImplementedError",
    "nl": "Gets the error message that is to be displayed if a required\nfield is empty.\n\n",
    "original_nl": "Gets the error message that is to be displayed if a required\nfield is empty.\n\n@return: Error message."
  },
  {
    "code": "def state_events(self, t, y, sw):\n    if sw[0]:\n        smax_event = self.dS_dt\n    else:\n        smax_event = (- 1.0)\n    t_cutoff_event = (t - self.t_cutoff)\n    return np.array([(smax_event > 0), (t_cutoff_event < 0)])",
    "nl": "Check whether an 'event' has occurred. We want to see if the\nsupersaturation is decreasing or not.",
    "original_nl": "Check whether an 'event' has occurred. We want to see if the\nsupersaturation is decreasing or not."
  },
  {
    "code": "def transform(self, input_dir):\n    if (not self._swdb):\n        self._initSwdb(input_dir)\n    else:\n        logger.error(_('Error: database is already initialized'))",
    "nl": "Interface for database transformation",
    "original_nl": "Interface for database transformation"
  },
  {
    "code": "@manager.command\n@manager.option('-l', '--language', dest='language', nargs='*')\n@manager.option('-c', '--country', dest='country', nargs='*')\n@manager.option('-f', '--foreign_id', dest='foreign_id')\ndef crawldir(path, language=None, country=None, foreign_id=None):\n    path = decode_path(os.path.abspath(os.path.normpath(path)))\n    if ((path is None) or (not os.path.exists(path))):\n        log.error('Invalid path: %r', path)\n        return\n    path_name = os.path.basename(path)\n    if (foreign_id is None):\n        foreign_id = ('directory:%s' % slugify(path))\n    role = Role.load_cli_user()\n    collection = Collection.by_foreign_id(foreign_id)\n    if (collection is None):\n        collection = Collection.create({\n            'foreign_id': foreign_id,\n            'label': path_name,\n            'casefile': False,\n        }, role=role)\n    if (language is not None):\n        collection.languages = [language]\n    if (country is not None):\n        collection.countries = [country]\n    db.session.commit()\n    update_collection(collection)\n    log.info('Crawling %r to %r...', path, collection.foreign_id)\n    document = Document.by_keys(collection=collection, foreign_id=path)\n    document.file_name = path_name\n    ingest_document(document, path, role_id=role.id)",
    "nl": "Crawl the given directory.",
    "original_nl": "Crawl the given directory."
  },
  {
    "code": "def timedelta_div(t1, t2):\n    if isinstance(t2, timedelta):\n        return (t1.total_seconds() / t2.total_seconds())\n    else:\n        return timedelta(seconds=(t1.total_seconds() / t2))",
    "nl": "divides a timedelta by a timedelta or a number.\nshould be a method of timedelta...",
    "original_nl": "divides a timedelta by a timedelta or a number.\nshould be a method of timedelta..."
  },
  {
    "code": "def resolve_ipv6_address(hostname):\n    for addrinfo in socket.getaddrinfo(hostname, 0):\n        (family, socktype, proto, name, sockaddr) = addrinfo\n        if (family == socket.AF_INET6):\n            sockaddr6 = sockaddr\n            (address, port, flow, scope) = sockaddr6\n            return address\n    raise LookupError(hostname)",
    "nl": "Resolve a hostname to an IP version 6 address",
    "original_nl": "Resolve a hostname to an IP version 6 address"
  },
  {
    "code": "def handle_endtag(self, tag_name):\n    self.__handle_data_if_exists()\n    if (self.__get_cur_tag()['name'] == tag_name):\n        self.__close_tag(self.__tag_stack.pop())\n    else:\n        for tag_id in xrange((len(self.__tag_stack) - 1), (- 1), (- 1)):\n            if (self.__tag_stack[tag_id]['name'] == tag_name):\n                for tag in reversed(self.__tag_stack[(tag_id + 1):]):\n                    self.__close_tag(tag, forced=True)\n                    self.__tag_stack.pop()\n                self.__close_tag(self.__tag_stack.pop())\n                break\n        else:\n            LOG.debug('Dropping excess end tag \"%s\"...', tag_name)",
    "nl": "Handles end of a tag.",
    "original_nl": "Handles end of a tag."
  },
  {
    "code": "def update(self):\n    self.reLayout()\n    if self._scaleChanged:\n        self.scaleChanged.emit(self._scale)\n        self._scaleChanged = False\n    self.changed.emit()",
    "nl": "Performs the layout (positions the Pages and adjusts our size).",
    "original_nl": "Performs the layout (positions the Pages and adjusts our size)."
  },
  {
    "code": "def validate_modules():\n    try:\n        jwt.rsa_load\n    except AttributeError:\n        raise ImproperlyConfigured('PyJWT-Mozilla not imported. This is because there is another JWT module installed. The JWT module imported is at: {0}. This can usually be fixed by running: `pip uninstall PyJWT` and `pip uninstall PyJWT-mozilla` and `pip install --force --no-deps PyJWT-mozilla`'.format(jwt.__file__))",
    "nl": "Validate that the modules that have been set up correctly.",
    "original_nl": "Validate that the modules that have been set up correctly."
  },
  {
    "code": "def _InnerModelClassesSupported(self):\n    return False",
    "nl": "Gets whether or not inner model classes are supported.",
    "original_nl": "Gets whether or not inner model classes are supported."
  },
  {
    "code": "def setup_course(self, default_store=None):\n    if (not default_store):\n        default_store = self.store.default_modulestore.get_modulestore_type()\n    with self.store.default_store(default_store):\n        self.course = CourseFactory.create(**self.course_options())\n        chapter = ItemFactory.create(parent=self.course, category='chapter')\n        self.html_block = ItemFactory.create(parent=chapter, category='html', data='<p>Test HTML Content<p>')",
    "nl": "Helper method to create the course.",
    "original_nl": "Helper method to create the course."
  },
  {
    "code": "def levenshtein_distance(s1, s2):\n    if (len(s1) < len(s2)):\n        return levenshtein_distance(s2, s1)\n    if (len(s2) == 0):\n        return len(s1)\n    previous_row = range((len(s2) + 1))\n    for (i, c1) in enumerate(s1):\n        current_row = [(i + 1)]\n        for (j, c2) in enumerate(s2):\n            insertions = (previous_row[(j + 1)] + 1)\n            deletions = (current_row[j] + 1)\n            substitutions = (previous_row[j] + (c1 != c2))\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n    return previous_row[(- 1)]",
    "nl": "Computes the string edit distance based on the Levenshtein Distance.\n\nAll credit for implementation goes to:\nhttps://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\nMuch appreciated.\n\n.. note::\n    If ``s1`` and ``s2`` must both be strings or both be list. Cannot mix\n    types.\n",
    "original_nl": "Computes the string edit distance based on the Levenshtein Distance.\n\nAll credit for implementation goes to:\nhttps://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\nMuch appreciated.\n\n.. note::\n    If ``s1`` and ``s2`` must both be strings or both be list. Cannot mix\n    types.\n\nParameters\n----------\ns1 : str or list\n    String or list.\n\ns1 : str or list\n    Other string or list.\n\nReturns\n--------\nInteger equal to the number of edits to get from ``s1`` to ``s2``."
  },
  {
    "code": "def test_matrix_representation_product_to_lin_space():\n    n = 3\n    A = np.random.rand(n, n)\n    Aop = odl.MatrixOperator(A)\n    B = np.random.rand(n, n)\n    Bop = odl.MatrixOperator(B)\n    ABop = ProductSpaceOperator([[Aop, Bop]])\n    matrix_repr = matrix_representation(ABop)\n    assert (matrix_repr.shape == (1, n, 2, n))\n    assert (np.linalg.norm((A - matrix_repr[0, :, 0, :])) == pytest.approx(0))\n    assert (np.linalg.norm((B - matrix_repr[0, :, 1, :])) == pytest.approx(0))",
    "nl": "Verify that the matrix repr works for product spaces.\n\nHere, since the domain shape ``(2, 3)`` and the range has shape ``(1, 3)``,\nthe shape of the matrix representation will be ``(2, 3, 1, 3)``.",
    "original_nl": "Verify that the matrix repr works for product spaces.\n\nHere, since the domain shape ``(2, 3)`` and the range has shape ``(1, 3)``,\nthe shape of the matrix representation will be ``(2, 3, 1, 3)``."
  },
  {
    "code": "def has_module_perms(self, user_obj, app_label):\n    for perm in self.get_all_permissions(user_obj):\n        if (perm[:perm.index('.')] == app_label):\n            return True\n    return False",
    "nl": "Returns True if user_obj has any permissions in the given app_label.",
    "original_nl": "Returns True if user_obj has any permissions in the given app_label."
  },
  {
    "code": "def on_exception(wait_gen, exception, max_tries=None, max_time=None, jitter=full_jitter, giveup=(lambda e: False), on_success=None, on_backoff=None, on_giveup=None, **wait_gen_kwargs):\n\n    def decorate(target):\n        retry = None\n        if (sys.version_info[:2] >= (3, 4)):\n            import asyncio\n            if asyncio.iscoroutinefunction(target):\n                import backoff._async\n                retry = backoff._async.retry_exception\n            else:\n                try:\n                    asyncio.get_event_loop()\n                except RuntimeError:\n                    pass\n                else:\n                    if (asyncio.Task.current_task() is not None):\n                        raise TypeError('backoff.on_exception applied to a regular function inside coroutine, this will lead to event loop hiccups. Use backoff.on_exception on coroutines in asynchronous code.')\n        if (retry is None):\n            retry = _sync.retry_exception\n        return retry(target, wait_gen, exception, max_tries, max_time, jitter, giveup, on_success, on_backoff, on_giveup, wait_gen_kwargs)\n    return decorate",
    "nl": "Returns decorator for backoff and retry triggered by exception.\n\nArgs:\n    wait_gen: A generator yielding successive wait times in\n        seconds.\n    exception: An exception type (or tuple of types) which triggers\n        backoff.\n    max_tries: The maximum number of attempts to make before giving\n        up. Once exhausted, the exception will be allowed to escape.\n        The default value of None means their is no limit to the\n        number of tries. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    max_time: The maximum total amount of time to try for before\n        giving up. Once expired, the exception will be allowed to\n        escape. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    jitter: A function of the value yielded by wait_gen returning\n        the actual time to wait. This distributes wait times\n        stochastically in order to avoid timing collisions across\n        concurrent clients. Wait times are jittered by default\n        using the full_jitter function. Jittering may be disabled\n        altogether by passing jitter=None.\n    giveup: Function accepting an exception instance and\n        returning whether or not to give up. Optional. The default\n        is to always continue.\n    on_success: Callable (or iterable of callables) with a unary\n        signature to be called in the event of success. The\n        parameter is a dict containing details about the invocation.\n    on_backoff: Callable (or iterable of callables) with a unary\n        signature to be called in the event of a backoff. The\n        parameter is a dict containing details about the invocation.\n    on_giveup: Callable (or iterable of callables) with a unary\n        signature to be called in the event that max_tries\n        is exceeded.  The parameter is a dict containing details\n        about the invocation.\n    **wait_gen_kwargs: Any additional keyword args specified will be\n        passed to wait_gen when it is initialized.  Any callable\n        args will first be evaluated and their return values passed.\n        This is useful for runtime configuration.",
    "original_nl": "Returns decorator for backoff and retry triggered by exception.\n\nArgs:\n    wait_gen: A generator yielding successive wait times in\n        seconds.\n    exception: An exception type (or tuple of types) which triggers\n        backoff.\n    max_tries: The maximum number of attempts to make before giving\n        up. Once exhausted, the exception will be allowed to escape.\n        The default value of None means their is no limit to the\n        number of tries. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    max_time: The maximum total amount of time to try for before\n        giving up. Once expired, the exception will be allowed to\n        escape. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    jitter: A function of the value yielded by wait_gen returning\n        the actual time to wait. This distributes wait times\n        stochastically in order to avoid timing collisions across\n        concurrent clients. Wait times are jittered by default\n        using the full_jitter function. Jittering may be disabled\n        altogether by passing jitter=None.\n    giveup: Function accepting an exception instance and\n        returning whether or not to give up. Optional. The default\n        is to always continue.\n    on_success: Callable (or iterable of callables) with a unary\n        signature to be called in the event of success. The\n        parameter is a dict containing details about the invocation.\n    on_backoff: Callable (or iterable of callables) with a unary\n        signature to be called in the event of a backoff. The\n        parameter is a dict containing details about the invocation.\n    on_giveup: Callable (or iterable of callables) with a unary\n        signature to be called in the event that max_tries\n        is exceeded.  The parameter is a dict containing details\n        about the invocation.\n    **wait_gen_kwargs: Any additional keyword args specified will be\n        passed to wait_gen when it is initialized.  Any callable\n        args will first be evaluated and their return values passed.\n        This is useful for runtime configuration."
  },
  {
    "code": "def __get_plugin_element(self, plugin):\n    if (plugin not in self.__elements):\n        element = None\n        try:\n            element = plugin.setup_element()\n        except Exception:\n            util.print_exc()\n        if (not element):\n            util.print_w((_(\"GStreamer plugin '%(name)s' could not be initialized\") % {\n                'name': plugin.PLUGIN_ID,\n            }))\n            return\n        plugin.update_element(element)\n        self.__elements[plugin] = element\n    return self.__elements[plugin]",
    "nl": "Setup element and cache it, so we can pass the linked/active\none to the plugin for live updates",
    "original_nl": "Setup element and cache it, so we can pass the linked/active\none to the plugin for live updates"
  },
  {
    "code": "def ecdh(self, identity, pubkey):\n    raise NotImplementedError()",
    "nl": "Get shared session key using Elliptic Curve Diffie-Hellman.",
    "original_nl": "Get shared session key using Elliptic Curve Diffie-Hellman."
  },
  {
    "code": "def _mount_coord_to_skycoord(self, mount_coords):\n    if isinstance(mount_coords, dict):\n        ra = mount_coords['ra']\n        dec = mount_coords['dec']\n    else:\n        (ra, dec) = mount_coords.split(' ')\n    ra = (float(ra) * u.hourangle).to(u.degree)\n    dec = (float(dec) * u.deg)\n    coords = SkyCoord(ra, dec)\n    return coords",
    "nl": "Converts between iOptron RA/Dec format and a SkyCoord\n\nArgs:\n    mount_coords (str): Coordinates as returned by mount\n",
    "original_nl": "Converts between iOptron RA/Dec format and a SkyCoord\n\nArgs:\n    mount_coords (str): Coordinates as returned by mount\n\nReturns:\n    astropy.SkyCoord:   Mount coordinates as astropy SkyCoord with\n        EarthLocation included."
  },
  {
    "code": "def test_get_fields_for_model_many_to_one(self):\n    Base = declarative_base()\n\n    class Parent(Base):\n        __tablename__ = 'parent'\n        id = Column(Integer, primary_key=True)\n        child_id = Column(Integer, ForeignKey('child.id'))\n        child = relationship('Child', backref='parents')\n\n    class Child(Base):\n        __tablename__ = 'child'\n        id = Column(Integer, primary_key=True)\n    resp = _get_fields_for_model(Parent)\n    self.assertAllIn(resp, ('id', 'child.id', 'child_id'))\n    resp = _get_fields_for_model(Child)\n    self.assertAllIn(resp, ('id', 'parents.id'))",
    "nl": "Tests getting the fields for a many_to_one",
    "original_nl": "Tests getting the fields for a many_to_one"
  },
  {
    "code": "def now():\n    return datetime.datetime.now(tz=pytz.utc).isoformat()",
    "nl": "Returns a datetime now object at utc. For convenience.",
    "original_nl": "Returns a datetime now object at utc. For convenience."
  },
  {
    "code": "@property\ndef clean_prefix(self):\n    user = self.context.bot.user\n    return self.context.prefix.replace(user.mention, ('@' + user.name))",
    "nl": "The cleaned up invoke prefix. i.e. mentions are ``@name`` instead of ``<@id>``.",
    "original_nl": "The cleaned up invoke prefix. i.e. mentions are ``@name`` instead of ``<@id>``."
  },
  {
    "code": "def op(scalars_layout, collections=None):\n    assert isinstance(scalars_layout, layout_pb2.Layout)\n    return tf.summary.tensor_summary(name=metadata.CONFIG_SUMMARY_TAG, tensor=tf.constant(scalars_layout.SerializeToString(), dtype=tf.string), collections=collections, summary_metadata=_create_summary_metadata())",
    "nl": "Creates a summary that contains a layout.\n\nWhen users navigate to the custom scalars dashboard, they will see a layout\nbased on the proto provided to this function.\n\nArgs:\n  scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n      layout.\n  collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n",
    "original_nl": "Creates a summary that contains a layout.\n\nWhen users navigate to the custom scalars dashboard, they will see a layout\nbased on the proto provided to this function.\n\nArgs:\n  scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n      layout.\n  collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n\nReturns:\n  A tensor summary op that writes the layout to disk."
  },
  {
    "code": "def test100():\n    return reader_creator(paddle.dataset.common.download(CIFAR100_URL, 'cifar', CIFAR100_MD5), 'test')",
    "nl": "CIFAR-100 test set creator.\n\nIt returns a reader creator, each sample in the reader is image pixels in\n[0, 1] and label in [0, 9].\n\n",
    "original_nl": "CIFAR-100 test set creator.\n\nIt returns a reader creator, each sample in the reader is image pixels in\n[0, 1] and label in [0, 9].\n\n:return: Test reader creator.\n:rtype: callable"
  },
  {
    "code": "def get_url(self, absolute=True):\n    return absolutify(urlparams(reverse('phonebook:register'), code=self.code))",
    "nl": "A url that can be used to redeem this invite.",
    "original_nl": "A url that can be used to redeem this invite."
  },
  {
    "code": "@staticmethod\ndef ping_from_remotehost(session_object, ip_type, dest_address, prompt, count):\n    ping_cmd = 'ping'\n    if (ip_type == 'ipv6'):\n        ping_cmd = 'ping6'\n    command = (ping_cmd + ' -c {} {}'.format(count, dest_address))\n    (_, output) = session_object.send_command('.*', prompt, command)\n    if ((' 0% packet loss' in output) or ('alive' in output)):\n        pNote('ping successfully completed')\n        status = True\n    else:\n        pNote('ping command failed')\n        status = False\n    return status",
    "nl": "ping  to dest_system from remote host\n\n:Arguments:\n    1. session_object(string)  = expect session object\n    2. command(string) = command to be executed\n    3. ip_type = ip/ipv6/dns\n    4. dest_address(string) = ip or dns name\n    3. prompt(string) = prompt\n\n",
    "original_nl": "ping  to dest_system from remote host\n\n:Arguments:\n    1. session_object(string)  = expect session object\n    2. command(string) = command to be executed\n    3. ip_type = ip/ipv6/dns\n    4. dest_address(string) = ip or dns name\n    3. prompt(string) = prompt\n\n:Returns:\n    1. bool (True/False)"
  },
  {
    "code": "def test_serialize(api_rf, media_resource_category_factory, serialized_time):\n    category = media_resource_category_factory()\n    api_rf.user = category.km_user.user\n    request = api_rf.get(category.get_absolute_url())\n    serializer = serializers.MediaResourceCategorySerializer(category, context={\n        'request': request,\n    })\n    expected = {\n        'id': category.id,\n        'url': request.build_absolute_uri(),\n        'created_at': serialized_time(category.created_at),\n        'updated_at': serialized_time(category.updated_at),\n        'km_user_id': category.km_user.id,\n        'name': category.name,\n        'permissions': {\n            'read': category.has_object_read_permission(request),\n            'write': category.has_object_write_permission(request),\n        },\n    }\n    assert (serializer.data == expected)",
    "nl": "Test serializing a media resource category.",
    "original_nl": "Test serializing a media resource category."
  },
  {
    "code": "def set_point_coordinate(self, name, x, y, z):\n    try:\n        pt = self.session.query(Point).filter_by(name=name).first()\n        if (pt is None):\n            raise Exception(\"Point doesn't exists.\")\n        scale = self.scale()\n        pt.x = (x * scale['L'])\n        pt.y = (y * scale['L'])\n        pt.z = (z * scale['L'])\n        self.session.add(pt)\n        return True\n    except Exception as e:\n        logger.info(str(e))\n        self.session.rollback()\n        return False",
    "nl": "Set point coordinate.\nif a point in same location exists, the name of the point will be returned.\n        ",
    "original_nl": "Set point coordinate.\nif a point in same location exists, the name of the point will be returned.\n        \nparam:\n    x,y,z: float-like, coordinates in current unit.\n    [name]: str, name, optional.\nreturn:\n    str, the new point's name."
  },
  {
    "code": "def command_set_brightness(self, value):\n    if ((value >= 0) and (value <= 9)):\n        self.write((12, 1, (value + 2)))",
    "nl": "Set Brightness (value between 0 and 9)",
    "original_nl": "Set Brightness (value between 0 and 9)"
  },
  {
    "code": "def get_default_backend():\n    global default_backend\n    if (not default_backend):\n        default_backend = DictCache()\n    return default_backend",
    "nl": "Returns the currently configured `default_backend`.\n\nIf not set, the `default_backend` is a `supycache.DictCache` instance. Use\n`supycache.set_default_backend` to change this. A `backend` is any\n(caching) object that has at least the `.get()`, `.set()` and `.delete()`\nmethods.",
    "original_nl": "Returns the currently configured `default_backend`.\n\nIf not set, the `default_backend` is a `supycache.DictCache` instance. Use\n`supycache.set_default_backend` to change this. A `backend` is any\n(caching) object that has at least the `.get()`, `.set()` and `.delete()`\nmethods."
  },
  {
    "code": "def apply(self, action, monotone=False):\n    new_preds = set(self.predicates)\n    new_preds |= set(action.add_effects)\n    if (not monotone):\n        new_preds -= set(action.del_effects)\n    new_functions = dict()\n    new_functions.update(self.functions)\n    for (function, value) in action.num_effects:\n        new_functions[function] += value\n    return State(new_preds, new_functions, (self.cost + 1), (self, action))",
    "nl": "Apply the action to this state to produce a new state.\nIf monotone, ignore the delete list (for A* heuristic)",
    "original_nl": "Apply the action to this state to produce a new state.\nIf monotone, ignore the delete list (for A* heuristic)"
  },
  {
    "code": "def run(self, package, body, operation='replace'):\n    package_path = self._zip_package(package)\n    try:\n        package = self._import_package(package_path)\n        self._update_package(package, body, operation)\n        self._delete_package(package)\n    finally:\n        os.remove(package_path)",
    "nl": "Import Murano package, modify it and then delete it.\n\nMeasure the Murano import, update and delete package\ncommands performance.\nIt imports Murano package from \"package\" (if it is not a zip archive\nthen zip archive will be prepared), modifies it (using data from\n\"body\") and deletes.\n\n",
    "original_nl": "Import Murano package, modify it and then delete it.\n\nMeasure the Murano import, update and delete package\ncommands performance.\nIt imports Murano package from \"package\" (if it is not a zip archive\nthen zip archive will be prepared), modifies it (using data from\n\"body\") and deletes.\n\n:param package: path to zip archive that represents Murano\n                application package or absolute path to folder with\n                package components\n:param body: dict object that defines what package property will be\n             updated, e.g {\"tags\": [\"tag\"]} or {\"enabled\": \"true\"}\n:param operation: string object that defines the way of how package\n                  property will be updated, allowed operations are\n                  \"add\", \"replace\" or \"delete\".\n                  Default value is \"replace\"."
  },
  {
    "code": "@classmethod\ndef from_response(cls, response_data):\n    identity = (lambda x: x)\n    return Mail(**_transform_dict(response_data, {\n        'guid': ('mail_id', identity),\n        'subject': ('mail_subject', identity),\n        'sender': ('mail_from', identity),\n        'datetime': ('mail_timestamp', (lambda x: datetime.utcfromtimestamp(int(x)).replace(tzinfo=utc))),\n        'read': ('mail_read', int),\n        'excerpt': ('mail_excerpt', identity),\n        'body': ('mail_body', identity),\n    }))",
    "nl": "Factory method to create a Mail instance from a Guerrillamail response\ndict.",
    "original_nl": "Factory method to create a Mail instance from a Guerrillamail response\ndict."
  },
  {
    "code": "def connect(self):\n    host = mpd_config['host'].get(unicode)\n    port = mpd_config['port'].get(int)\n    if (host[0] in ['/', '~']):\n        host = os.path.expanduser(host)\n    self._log.info('connecting to {0}:{1}', host, port)\n    try:\n        self.client.connect(host, port)\n    except socket.error as e:\n        raise ui.UserError('could not connect to MPD: {0}'.format(e))\n    password = mpd_config['password'].get(unicode)\n    if password:\n        try:\n            self.client.password(password)\n        except mpd.CommandError as e:\n            raise ui.UserError('could not authenticate to MPD: {0}'.format(e))",
    "nl": "Connect to the MPD.",
    "original_nl": "Connect to the MPD."
  },
  {
    "code": "def GetNormalizedOutputAndLeakyTests(output):\n    output = ToUnixLineEnding(output)\n    output = RemoveReportHeaderAndFooter(output)\n    output = NormalizeErrorMarker(output)\n    output = RemoveLocations(output)\n    output = RemoveMemoryAddresses(output)\n    return (RemoveTestNamesOfLeakedMocks(output), GetLeakyTests(output))",
    "nl": "Normalizes the output of gmock_output_test_.\n\nArgs:\n  output: The test output.\n",
    "original_nl": "Normalizes the output of gmock_output_test_.\n\nArgs:\n  output: The test output.\n\nReturns:\n  A tuple (the normalized test output, the list of test names that have\n  leaked mocks)."
  },
  {
    "code": "def test_nonlinear_variational_solver_custom_comm():\n    if (MPI.rank(mpi_comm_world()) == 0):\n        mesh = UnitIntervalMesh(mpi_comm_self(), 2)\n        V = FunctionSpace(mesh, 'CG', 1)\n        f = Constant(1)\n        u = Function(V)\n        v = TestFunction(V)\n        F = ((inner(u, v) * dx) - (inner(f, v) * dx))\n        solve((F == 0), u)\n        solve((F == 0), u, solver_parameters={\n            'nonlinear_solver': 'newton',\n        })\n        if has_petsc():\n            solve((F == 0), u, solver_parameters={\n                'nonlinear_solver': 'snes',\n            })",
    "nl": "Check that nonlinear variational solver works on subset of comm_world",
    "original_nl": "Check that nonlinear variational solver works on subset of comm_world"
  },
  {
    "code": "@asyncio.coroutine\ndef update_async(self, tag):\n    entity = (yield from self.request().update_async(tag))\n    return entity",
    "nl": "Updates the specified Tag in async\n\nArgs:\n    tag (:class:`Tag<onedrivesdk.model.tag.Tag>`):\n        The Tag to update.\n",
    "original_nl": "Updates the specified Tag in async\n\nArgs:\n    tag (:class:`Tag<onedrivesdk.model.tag.Tag>`):\n        The Tag to update.\n\nReturns:\n    :class:`Tag<onedrivesdk.model.tag.Tag>`:\n        The updated Tag."
  },
  {
    "code": "def predict_by_pct(self, recur_pct, del_pct, total):\n    recur_ct = (recur_pct * total)\n    del_ct = (del_pct * total)\n    if (self.kind == 'vogelstein'):\n        if ((recur_pct >= self.onco_threshold) and (recur_ct >= self.onco_min)):\n            if (del_pct <= 0.05):\n                return self.onco_label\n            elif (del_ct >= self.tsg_min):\n                return self.tsg_label\n            else:\n                return self.other_label\n        elif ((del_pct >= self.tsg_threshold) and (del_ct >= self.tsg_min)):\n            return self.tsg_label\n        else:\n            return self.other_label\n    elif (self.kind == 'min'):\n        if (total < self.min_count):\n            return self.other_label\n        elif (recur_pct >= self.onco_threshold):\n            if (recur_pct >= del_pct):\n                return self.onco_label\n            else:\n                return self.tsg_label\n        elif (del_pct >= self.tsg_threshold):\n            return self.tsg_label\n        else:\n            return self.other_label",
    "nl": "The actual 20/20 rule logic to classify genes.",
    "original_nl": "The actual 20/20 rule logic to classify genes."
  },
  {
    "code": "def get_remote_group_names(self):\n    return frozenset(self.filter(local=False).values_list('name', flat=True))",
    "nl": "Return names of groups related to external authentication.",
    "original_nl": "Return names of groups related to external authentication."
  },
  {
    "code": "def authorize_user_context(context, user_id):\n    if is_user_context(context):\n        if (not context.user_id):\n            raise exception.Forbidden()\n        elif (context.user_id != user_id):\n            raise exception.Forbidden()",
    "nl": "Ensures a request has permission to access the given user.",
    "original_nl": "Ensures a request has permission to access the given user."
  },
  {
    "code": "@parse_command([('name', 1), ('password', '?')], launch_invalid=False)\ndef admin_command_channel_join(self, msg, args):\n    self.join(args.name[0], args.password)",
    "nl": "Joins a channel.\n\nSyntax: channel_join CHANNEL_NAME [CHANNEL_PASSWORD]",
    "original_nl": "Joins a channel.\n\nSyntax: channel_join CHANNEL_NAME [CHANNEL_PASSWORD]"
  },
  {
    "code": "def PushEvents(self, events):\n    for event in events:\n        self.PushEvent(event)",
    "nl": "Pushes events onto the heap.\n\nArgs:\n  events list[EventObject]: events.",
    "original_nl": "Pushes events onto the heap.\n\nArgs:\n  events list[EventObject]: events."
  },
  {
    "code": "def handle_response(self, lvap):\n    lvaps = RUNTIME.tenants[self.tenant_id].lvaps\n    if (lvap.addr not in lvaps):\n        return\n    self.handle_callback(lvap)",
    "nl": "Handle an LVAL_LEAVE message.\nArgs:\n    lvap, an LVAP object",
    "original_nl": "Handle an LVAL_LEAVE message.\nArgs:\n    lvap, an LVAP object\nReturns:\n    None"
  },
  {
    "code": "def get_port_id(self, tg_id, port_id):\n    port_name = self.tgs[tg_id].ports[(port_id - 1)]\n    return (self.ports.index(Port(tg_id, port_name)) + 1)",
    "nl": "Return port's sequence number in list of ports.\n\nArgs:\n    tg_id(int):  TG instance ID\n    port_id(int):  TG instance port's sequence number\n\nRaises:\n    ValueError:  in case expected port is not in list of ports\n",
    "original_nl": "Return port's sequence number in list of ports.\n\nArgs:\n    tg_id(int):  TG instance ID\n    port_id(int):  TG instance port's sequence number\n\nRaises:\n    ValueError:  in case expected port is not in list of ports\n\nReturns:\n    int:  Port sequence number in list of ports starting from 1"
  },
  {
    "code": "def clear(self):\n    if (not os.path.exists(self._dir)):\n        return\n    for fname in self._list_cache_files():\n        self._delete(fname)",
    "nl": "Remove all the cache files.",
    "original_nl": "Remove all the cache files."
  },
  {
    "code": "def setup(hass, config):\n    from toonapilib.toonapilibexceptions import InvalidConsumerSecret, InvalidConsumerKey, InvalidCredentials\n    try:\n        hass.data[TOON_HANDLE] = ToonDataStore(config['toon'][CONF_USERNAME], config['toon'][CONF_PASSWORD], config['toon'][CONF_KEY], config['toon'][CONF_SECRET], config['toon'][CONF_GAS], config['toon'][CONF_SOLAR], config['toon'][CONF_TENANT], config['toon'][CONF_NAME])\n    except InvalidCredentials:\n        _LOGGER.warning('The credentials in your config are invalid')\n        return False\n    except InvalidConsumerKey:\n        _LOGGER.warning('Your customer key is invalid')\n        return False\n    except InvalidConsumerSecret:\n        _LOGGER.warning('Your customer secret is invalid')\n        return False\n    for platform in ('climate', 'sensor', 'switch'):\n        load_platform(hass, platform, DOMAIN, {\n            \n        }, config)\n    return True",
    "nl": "Setup toon.",
    "original_nl": "Setup toon."
  },
  {
    "code": "def test_handle_update(self):\n    test_stack = self.create_stack(template=alarm_template)\n    update_mock = self.patchobject(self.fa.alarm, 'update')\n    test_stack.create()\n    rsrc = test_stack['cps_alarm']\n    self.assertEqual((rsrc.CREATE, rsrc.COMPLETE), rsrc.state)\n    after_props = copy.deepcopy(rsrc.properties.data)\n    update_props = {\n        'enabled': False,\n        'repeat_actions': False,\n        'insufficient_data_actions': [],\n        'ok_actions': ['signal_handler'],\n    }\n    after_props.update(update_props)\n    snippet = rsrc_defn.ResourceDefinition(rsrc.name, rsrc.type(), after_props)\n    scheduler.TaskRunner(rsrc.update, snippet)()\n    self.assertEqual((rsrc.UPDATE, rsrc.COMPLETE), rsrc.state)\n    self.assertEqual(1, update_mock.call_count)",
    "nl": "Test update the composite alarm.",
    "original_nl": "Test update the composite alarm."
  },
  {
    "code": "def _plotGaussianKDE(axes, plot_config, data, label):\n    style = plot_config.histogramStyle()\n    if (data.dtype == 'object'):\n        try:\n            data = pd.to_numeric(data, errors='coerce')\n        except AttributeError:\n            data = data.convert_objects(convert_numeric=True)\n    if (data.dtype == 'object'):\n        pass\n    else:\n        sample_range = (data.max() - data.min())\n        indexes = numpy.linspace((data.min() - (0.5 * sample_range)), (data.max() + (0.5 * sample_range)), 1000)\n        gkde = gaussian_kde(data.values)\n        evaluated_gkde = gkde.evaluate(indexes)\n        lines = axes.plot(indexes, evaluated_gkde, linewidth=style.width, color=style.color, alpha=style.alpha)\n        if (len(lines) > 0):\n            plot_config.addLegendItem(label, lines[0])",
    "nl": "@type axes: matplotlib.axes.Axes\n@type plot_config: PlotConfig\n@type data: DataFrame\n@type label: Str",
    "original_nl": "@type axes: matplotlib.axes.Axes\n@type plot_config: PlotConfig\n@type data: DataFrame\n@type label: Str"
  },
  {
    "code": "def _do_edit_config(self, target, config, default_operation, test_option, error_option):\n    try:\n        log.debug('edit-config', target=target, config=config)\n        response = self._session.edit_config(target=target, config=config)\n        log.debug('response', response=response)\n    except RPCError as e:\n        log.exception('do_edit_config', e=e)\n        raise\n    return response",
    "nl": "Lock the configuration system",
    "original_nl": "Lock the configuration system"
  },
  {
    "code": "def collapse(self):\n    if self.cardinality:\n        r = None\n        for si in self._si_set:\n            r = (r._union(si) if (r is not None) else si)\n        return r\n    else:\n        return StridedInterval.empty(self._bits)",
    "nl": "Collapse into a StridedInterval instance.\n\n",
    "original_nl": "Collapse into a StridedInterval instance.\n\n:return: A new StridedInterval instance."
  },
  {
    "code": "@classmethod\ndef create(cls, name, template):\n    try:\n        fw_template = IPSTemplatePolicy(template).href\n    except ElementNotFound:\n        raise LoadPolicyFailed('Cannot find specified firewall template: {}'.format(template))\n    json = {\n        'name': name,\n        'template': fw_template,\n    }\n    try:\n        return ElementCreator(cls, json)\n    except CreateElementFailed as err:\n        raise CreatePolicyFailed(err)",
    "nl": "Create an IPS Policy\n\n",
    "original_nl": "Create an IPS Policy\n\n:param str name: Name of policy\n:param str template: name of template\n:raises CreatePolicyFailed: policy failed to create\n:return: IPSPolicy"
  },
  {
    "code": "def cleanup(self, *args, **kwargs):\n    super(OpenMPI, self).cleanup(*args, **kwargs)\n    tmpdir = os.environ.get('TMPDIR')\n    if (tmpdir != self.orig_tmpdir):\n        try:\n            shutil.rmtree(tmpdir)\n        except OSError as err:\n            print_warning('Failed to clean up temporary directory %s: %s', tmpdir, err)\n        env.setvar('TMPDIR', self.orig_tmpdir)\n        self.log.info('$TMPDIR restored to %s', self.orig_tmpdir)",
    "nl": "Clean up after using OpenMPI in toolchain.",
    "original_nl": "Clean up after using OpenMPI in toolchain."
  },
  {
    "code": "def test_auto_coord(self):\n    (x, y) = (3.14, 2.72)\n    b = Block('Spam', 'eggs', (x, y), '1 cm')\n    self.assertEqual(b.get_tikz_coordinate('g'), '\\\\coordinate (eggs--coord) at (3.14, 2.72);')",
    "nl": "Test auto TikZ coordinate specification for a block.",
    "original_nl": "Test auto TikZ coordinate specification for a block."
  },
  {
    "code": "@task\ndef docs(ctx, clean=False, browse=False, watch=False):\n    if clean:\n        clean_docs(ctx)\n    ctx.run(('sphinx-build %s %s' % (docs_dir, build_dir)), echo=True)\n    if browse:\n        browse_docs(ctx)\n    if watch:\n        watch_docs(ctx)",
    "nl": "Build the docs.",
    "original_nl": "Build the docs."
  },
  {
    "code": "def _plot(self):\n    for serie in self.series:\n        self.funnel(serie)",
    "nl": "Plot the funnel",
    "original_nl": "Plot the funnel"
  },
  {
    "code": "def contains(self, item):\n    lock = RLock()\n    isPresent = False\n    lock.acquire()\n    isPresent = (item in self.all_items)\n    lock.release()\n    return isPresent",
    "nl": "Attempt to implement thread-safety for contains method using Re-entrant locks.\n## I'm still not sure whether this implementation is correct.",
    "original_nl": "Attempt to implement thread-safety for contains method using Re-entrant locks.\n## I'm still not sure whether this implementation is correct."
  },
  {
    "code": "def forward(self, inputs):\n    out = np.array(inputs)\n    for key in self.W.keys():\n        W = self.W[key]\n        b = self.b[key]\n        act = self.activation[key]\n        z = (np.dot(out, W) + b)\n        out = act(z)\n    return out",
    "nl": "Forward propagate the inputs through the neural network.\nINPUTS\n    inputs - array_like\n        An input to the neural network. Must match the specfied input\n        dimension or errors will be thrown.\nOUTPUTS\n    output - array_like\n        The output of the neural network. Its dimension is determined\n        by the number of units in the final specified layer.",
    "original_nl": "Forward propagate the inputs through the neural network.\nINPUTS\n    inputs - array_like\n        An input to the neural network. Must match the specfied input\n        dimension or errors will be thrown.\nOUTPUTS\n    output - array_like\n        The output of the neural network. Its dimension is determined\n        by the number of units in the final specified layer."
  },
  {
    "code": "def test_vamp_bg(self):\n    vamp_bg_test(nz=1000, ny=500, ns=10, verbose=False)",
    "nl": "Run VAMP with a BG prior",
    "original_nl": "Run VAMP with a BG prior"
  },
  {
    "code": "def c_moves_s(client):\n    return 'south'",
    "nl": "move through south exit if available",
    "original_nl": "move through south exit if available"
  },
  {
    "code": "def forever(self, key, value):\n    self.put(key, value, 0)",
    "nl": "Store an item in the cache indefinitely.\n\n",
    "original_nl": "Store an item in the cache indefinitely.\n\n:param key: The cache key\n:type key: str\n\n:param value: The value\n:type value: mixed"
  },
  {
    "code": "def call_monitor_plugin(self, callback, *args, **kwargs):\n    try:\n        reservation_flags = callback(*args, **kwargs)\n    except Exception as e:\n        LOG.exception('Caught an exception while executing a callback. %s', str(e))\n    if reservation_flags:\n        self._update_flags(reservation_flags)",
    "nl": "Call a callback and update lease/reservation flags.",
    "original_nl": "Call a callback and update lease/reservation flags."
  },
  {
    "code": "def hasLayers(self):\n    return (len(self.getTimeLayerList()) > 0)",
    "nl": "returns true if the manager has at least one layer registered",
    "original_nl": "returns true if the manager has at least one layer registered"
  },
  {
    "code": "def test_add(self):\n    self.fsdb.add(self.createTestFile())",
    "nl": "test insertion through path",
    "original_nl": "test insertion through path"
  },
  {
    "code": "@property\ndef if_none_match(self):\n    for option in self.options:\n        if (option.number == defines.OptionRegistry.IF_NONE_MATCH.number):\n            return True\n    return False",
    "nl": "Get the if-none-match option of a request.\n\n",
    "original_nl": "Get the if-none-match option of a request.\n\n:return: True, if if-none-match is present\n:rtype : bool"
  },
  {
    "code": "def to_scipy_sparse(m, **options):\n    dtype = options.get('dtype', 'complex')\n    if isinstance(m, (Matrix, Expr)):\n        return sympy_to_scipy_sparse(m, dtype=dtype)\n    elif isinstance(m, numpy_ndarray):\n        if (not sparse):\n            raise ImportError\n        return sparse.csr_matrix(m)\n    elif isinstance(m, scipy_sparse_matrix):\n        return m\n    raise TypeError(('Expected sympy/numpy/scipy.sparse matrix, got: %r' % m))",
    "nl": "Convert a sympy/numpy matrix to a scipy.sparse matrix.",
    "original_nl": "Convert a sympy/numpy matrix to a scipy.sparse matrix."
  },
  {
    "code": "@property\ndef output(self):\n    return self.args[2]",
    "nl": "The output of the test runner (if applicable).",
    "original_nl": "The output of the test runner (if applicable)."
  },
  {
    "code": "def showCountDialog(countType, counts):\n    isNoStats = (not counts)\n    noStatsMsg = \"Usage stats aren't available yet, press any key...\"\n    if isNoStats:\n        (popup, width, height) = cli.popups.init(3, (len(noStatsMsg) + 4))\n    else:\n        (popup, width, height) = cli.popups.init((4 + max(1, len(counts))), 80)\n    if (not popup):\n        return\n    try:\n        control = cli.controller.getController()\n        popup.win.box()\n        if (countType == CountType.CLIENT_LOCALE):\n            title = 'Client Locales'\n        elif (countType == CountType.EXIT_PORT):\n            title = 'Exiting Port Usage'\n        else:\n            title = ''\n            log.warn(('Unrecognized count type: %s' % countType))\n        popup.addstr(0, 0, title, curses.A_STANDOUT)\n        if isNoStats:\n            popup.addstr(1, 2, noStatsMsg, (curses.A_BOLD | uiTools.getColor('cyan')))\n        else:\n            sortedCounts = sorted(counts.iteritems(), key=operator.itemgetter(1))\n            sortedCounts.reverse()\n            (keyWidth, valWidth, valueTotal) = (3, 1, 0)\n            for (k, v) in sortedCounts:\n                keyWidth = max(keyWidth, len(k))\n                valWidth = max(valWidth, len(str(v)))\n                valueTotal += v\n            if (countType == CountType.EXIT_PORT):\n                keyWidth += EXIT_USAGE_WIDTH\n            labelFormat = ('%%-%is %%%ii (%%%%%%-2i)' % (keyWidth, valWidth))\n            for i in range((height - 4)):\n                (k, v) = sortedCounts[i]\n                if (countType == CountType.EXIT_PORT):\n                    usage = connections.getPortUsage(k)\n                    if usage:\n                        keyFormat = ('%%-%is   %%s' % (keyWidth - EXIT_USAGE_WIDTH))\n                        k = (keyFormat % (k, usage[:(EXIT_USAGE_WIDTH - 3)]))\n                label = (labelFormat % (k, v, ((v * 100) / valueTotal)))\n                popup.addstr((i + 1), 2, label, (curses.A_BOLD | uiTools.getColor('green')))\n                labelWidth = len(label)\n                fillWidth = ((v * ((width - 4) - labelWidth)) / valueTotal)\n                for j in range(fillWidth):\n                    popup.addstr((i + 1), ((3 + labelWidth) + j), ' ', (curses.A_STANDOUT | uiTools.getColor('red')))\n            popup.addstr((height - 2), 2, 'Press any key...')\n        popup.win.refresh()\n        curses.cbreak()\n        control.getScreen().getch()\n    finally:\n        cli.popups.finalize()",
    "nl": "Provides a dialog with bar graphs and percentages for the given set of\ncounts. Pressing any key closes the dialog.\n\nArguments:\n  countType - type of counts being presented\n  counts    - mapping of labels to counts",
    "original_nl": "Provides a dialog with bar graphs and percentages for the given set of\ncounts. Pressing any key closes the dialog.\n\nArguments:\n  countType - type of counts being presented\n  counts    - mapping of labels to counts"
  },
  {
    "code": "@property\ndef kml(self):\n    return ('<MultiGeometry>%s</MultiGeometry>' % ''.join((g.kml for g in self)))",
    "nl": "Returns the KML for this Geometry Collection.",
    "original_nl": "Returns the KML for this Geometry Collection."
  },
  {
    "code": "@cmdfilter\ndef acls(self, msg, cmd, args, dry_run):\n    self.log.debug(('Check %s for ACLs.' % cmd))\n    f = self._bot.all_commands[cmd]\n    cmd_str = '{plugin}:{command}'.format(plugin=f.__self__.name, command=cmd)\n    usr = get_acl_usr(msg)\n    acl = self.bot_config.ACCESS_CONTROLS_DEFAULT.copy()\n    for (pattern, acls) in self.bot_config.ACCESS_CONTROLS.items():\n        if (':' not in pattern):\n            pattern = '*:{command}'.format(command=pattern)\n        if ciglob(cmd_str, (pattern,)):\n            acl.update(acls)\n            break\n    self.log.info(('Matching ACL %s against username %s for command %s' % (acl, usr, cmd_str)))\n    if (('allowusers' in acl) and (not glob(usr, acl['allowusers']))):\n        return self.access_denied(msg, \"You're not allowed to access this command from this user\", dry_run)\n    if (('denyusers' in acl) and glob(usr, acl['denyusers'])):\n        return self.access_denied(msg, \"You're not allowed to access this command from this user\", dry_run)\n    if msg.is_group:\n        if (not isinstance(msg.frm, RoomOccupant)):\n            raise Exception(('msg.frm is not a RoomOccupant. Class of frm: %s' % msg.frm.__class__))\n        room = str(msg.frm.room)\n        if (('allowmuc' in acl) and (acl['allowmuc'] is False)):\n            return self.access_denied(msg, \"You're not allowed to access this command from a chatroom\", dry_run)\n        if (('allowrooms' in acl) and (not glob(room, acl['allowrooms']))):\n            return self.access_denied(msg, \"You're not allowed to access this command from this room\", dry_run)\n        if (('denyrooms' in acl) and glob(room, acl['denyrooms'])):\n            return self.access_denied(msg, \"You're not allowed to access this command from this room\", dry_run)\n    elif (('allowprivate' in acl) and (acl['allowprivate'] is False)):\n        return self.access_denied(msg, \"You're not allowed to access this command via private message to me\", dry_run)\n    self.log.info(('Check if %s is admin only command.' % cmd))\n    if f._err_command_admin_only:\n        if (not glob(get_acl_usr(msg), self.bot_config.BOT_ADMINS)):\n            return self.access_denied(msg, 'This command requires bot-admin privileges', dry_run)\n        if (msg.is_group and (not acl.get('allowmuc', False))):\n            return self.access_denied(msg, 'This command may only be issued through a direct message', dry_run)\n    return (msg, cmd, args)",
    "nl": "Check command against ACL rules as defined in the bot configuration.\n\n",
    "original_nl": "Check command against ACL rules as defined in the bot configuration.\n\n:param msg: The original chat message.\n:param cmd: The command name itself.\n:param args: Arguments passed to the command.\n:param dry_run: True when this is a dry-run."
  },
  {
    "code": "@classmethod\ndef create(cls, name, form, update_if_exists=False):\n    survey = cls.get(name, throw_if_not_found=False)\n    if (not survey):\n        survey = SurveyForm(name=name, form=form)\n    elif update_if_exists:\n        survey.form = form\n    else:\n        raise SurveyFormNameAlreadyExists()\n    survey.save()\n    return survey",
    "nl": "Helper class method to create a new Survey Form.\n\nupdate_if_exists=True means that if a form already exists with that name, then update it.\nOtherwise throw an SurveyFormAlreadyExists exception",
    "original_nl": "Helper class method to create a new Survey Form.\n\nupdate_if_exists=True means that if a form already exists with that name, then update it.\nOtherwise throw an SurveyFormAlreadyExists exception"
  },
  {
    "code": "def run_tests(test_names):\n    import xmlrunner\n    suite = unittest.TestSuite((unittest.defaultTestLoader.loadTestsFromModule(module) for module in import_tests(test_names)))\n    if test_names:\n        suite = filter_suite(suite, test_names)\n        if (suite.countTestCases() == 0):\n            raise Exception('No matching tests found')\n    runner = xmlrunner.XMLTestRunner(output='test.results', outsuffix='')\n    results = runner.run(suite)\n    return (len(results.errors) + len(results.failures))",
    "nl": "Runs tests using unittest, returns the number of failures.",
    "original_nl": "Runs tests using unittest, returns the number of failures."
  },
  {
    "code": "@salt.setter\ndef salt(self, value):\n    self._salt = value",
    "nl": "Updates the salt (integer) used when encrypting messages.",
    "original_nl": "Updates the salt (integer) used when encrypting messages."
  },
  {
    "code": "@staticmethod\ndef _extract_address(bitcoin_url):\n    (address_text, _) = bitcoin_url.split('?')\n    address = address_text.split(':')[1]\n    return address",
    "nl": "Extract address from bitcoin url\n",
    "original_nl": "Extract address from bitcoin url\n:param bitcoin_url: bitcoin url\n:return: Bitcoin address"
  },
  {
    "code": "def get_wrapper(self, module_class):\n    for wrapper in self.module_wrappers:\n        if (wrapper.name == get_ident_string(module_class)):\n            return wrapper\n    return None",
    "nl": "Checks if a module is loaded. Returns ModuleWrapper or None on\nfailure.",
    "original_nl": "Checks if a module is loaded. Returns ModuleWrapper or None on\nfailure."
  },
  {
    "code": "def quotereplacechar(char, sub, string):\n    quote = False\n    fixed_string = ''\n    for i in string:\n        if (i in [\"'\", '\"']):\n            if (not quote):\n                quote = i\n                fixed_string += i\n            elif (quote == i):\n                quote = False\n                fixed_string += i\n            else:\n                fixed_string += i\n        elif quote:\n            fixed_string += i\n        elif (i == char):\n            fixed_string += sub\n        else:\n            fixed_string += i\n    return fixed_string",
    "nl": "Quote respecting replace.",
    "original_nl": "Quote respecting replace."
  },
  {
    "code": "def read_arguments():\n    parser = ArgumentParser(description=('Convert RTI Connext logs in ' + 'human-readable format.'))\n    parser.add_argument('-i', '--input', help='log file path, by default stdin')\n    parser.add_argument('-v', action='count', help=\"verbosity level - increased by multiple 'v'\")\n    parser.add_argument('--output', '-o', help='write the output into the specified file')\n    parser.add_argument('--overwrite-output', '-oo', help='write the output into a new/truncated file')\n    parser.add_argument('--write-original', help='write the original log output into a file')\n    parser.add_argument('--show-ip', action='store_true', help='show the IP address instead of an assigned name')\n    parser.add_argument('--obfuscate', action='store_true', help='hide sensitive information like IP addresses')\n    parser.add_argument('--salt', '-s', help='salt for obfuscation - from random if not set')\n    parser.add_argument('--show-timestamp', '-t', action='store_true', help='show timestamp log field')\n    parser.add_argument('--show-lines', action='store_true', help='print the original and parsed log lines')\n    parser.add_argument('--only', help='show only log messages that match the regex')\n    parser.add_argument('--colors', '-c', action='store_true', help='apply colors to log messages (e.g.: warnings)')\n    parser.add_argument('--highlight', help='show in bold regex matched logs, requires -c')\n    parser.add_argument('--local-host', help='set the local address')\n    parser.add_argument('--no-network', action='store_true', help='do not show the network related logs')\n    parser.add_argument('--no-inline', action='store_true', help='do not show warnigns and errors in network logs')\n    parser.add_argument('--no-stats', action='store_true', help='do not show the network and packet statistics')\n    parser.add_argument('--no-progress', action='store_true', help='do not show the interative info at the bottom')\n    parser.add_argument('--debug', action='store_true', help='debug mode - export unmatched logs')\n    parser.add_argument('--version', action='version', help='show the program version', version=('%(prog)s ' + __version__))\n    return parser.parse_args()",
    "nl": "Parse the command-line arguments.",
    "original_nl": "Parse the command-line arguments."
  },
  {
    "code": "def testAddArguments(self):\n    argument_parser = argparse.ArgumentParser(prog='cli_helper.py', description='Test argument parser.', add_help=False, formatter_class=cli_test_lib.SortedArgumentsHelpFormatter)\n    zeromq.ZeroMQArgumentsHelper.AddArguments(argument_parser)\n    output = self._RunArgparseFormatHelp(argument_parser)\n    self.assertEqual(output, self._EXPECTED_OUTPUT)",
    "nl": "Tests the AddArguments function.",
    "original_nl": "Tests the AddArguments function."
  },
  {
    "code": "def retrieve_alias(self, email, **kwargs):\n    uri = self.MakeMultidomainAliasProvisioningUri(email=email)\n    return self.GetEntry(uri, desired_class=gdata.apps.multidomain.data.AliasEntry, **kwargs)",
    "nl": "Retrieves a single alias in the domain.\n\nArgs:\n  email: string The email address of the alias to be retrieved\n  kwargs: The other parameters to pass to gdata.client.GDClient.GetEntry()\n",
    "original_nl": "Retrieves a single alias in the domain.\n\nArgs:\n  email: string The email address of the alias to be retrieved\n  kwargs: The other parameters to pass to gdata.client.GDClient.GetEntry()\n\nReturns:\n  A gdata.apps.multidomain.data.AliasEntry representing the alias"
  },
  {
    "code": "@property\ndef seekable(self):\n    return self.props.seekable",
    "nl": "If the current song can be seeked, in case it's not clear defaults\nto True. See the \"seekable\" GObject property for notifications.",
    "original_nl": "If the current song can be seeked, in case it's not clear defaults\nto True. See the \"seekable\" GObject property for notifications."
  },
  {
    "code": "def all(self):\n    for (checksum, path) in self.hash_db.items():\n        (yield (checksum, path))",
    "nl": "Generator to get all entries from self.hash_db\n\n",
    "original_nl": "Generator to get all entries from self.hash_db\n\n:returns tuple(string)"
  },
  {
    "code": "def attach(self, widget):\n    if self.is_attached:\n        return\n    if (not self.events):\n        return\n    if self.debug:\n        print('Attach:', self)\n    self.doAttach(widget.real_widget)\n    self.widget_ref = weakref.ref(widget)\n    self.is_attached = True",
    "nl": "Start receiving events.\nNo need to call this manually.",
    "original_nl": "Start receiving events.\nNo need to call this manually."
  },
  {
    "code": "def get_package_versions(lines):\n    versions = {\n        \n    }\n    for line in lines:\n        line = line.strip()\n        if ((len(line) == 0) or line.startswith('#') or line.startswith('-r ')):\n            continue\n        if line.startswith('https://'):\n            continue\n        (name, version_plus) = line.split('==', 1)\n        versions[name.lower()] = version_plus.split(' ', 1)[0]\n    return versions",
    "nl": "Return a dictionary of package versions.",
    "original_nl": "Return a dictionary of package versions."
  },
  {
    "code": "@patch('__builtin__.open', mock_open())\ndef test_failure_on_diffquality_jshint(self):\n    self._mock_paver_sh.side_effect = CustomShMock().fail_on_jshint\n    _mock_pep8_violations = MagicMock(return_value=(0, []))\n    with patch('pavelib.quality._get_pep8_violations', _mock_pep8_violations):\n        with self.assertRaises(SystemExit):\n            pavelib.quality.run_quality('')\n            self.assertRaises(BuildFailure)\n    self.assertEqual(_mock_pep8_violations.call_count, 1)\n    self.assertEqual(self._mock_paver_sh.call_count, 2)",
    "nl": "If diff-quality fails on jshint, the paver task should also fail",
    "original_nl": "If diff-quality fails on jshint, the paver task should also fail"
  },
  {
    "code": "def getEstimatorParamMaps(self):\n    return self.getOrDefault(self.estimatorParamMaps)",
    "nl": "Gets the value of estimatorParamMaps or its default value.",
    "original_nl": "Gets the value of estimatorParamMaps or its default value."
  },
  {
    "code": "def parse_url(url):\n    parsed = urlparse.urlparse(url)\n    queries = urlparse.parse_qs(parsed.query)\n    path = parsed.path.split('/')\n    return (queries['id'][0] if ('id' in queries) else path[3])",
    "nl": "Return the 3rd part of the url or get id param if it exists.\n\nArgs:\n    url: google drive url\n",
    "original_nl": "Return the 3rd part of the url or get id param if it exists.\n\nArgs:\n    url: google drive url\n\nReturns:\n    url id"
  },
  {
    "code": "@mock.patch('olympia.devhub.utils.chain')\ndef test_run_once_file_upload(self, chain):\n    task = mock.Mock()\n    chain.return_value = task\n    task.delay.return_value = mock.Mock(task_id='42')\n    assert isinstance(tasks.validate(self.file_upload, listed=True), mock.Mock)\n    assert (task.delay.call_count == 1)\n    assert isinstance(tasks.validate(self.file_upload, listed=True), AsyncResult)\n    assert (task.delay.call_count == 1)",
    "nl": "Tests that only a single validation task is run for a given file\nupload.",
    "original_nl": "Tests that only a single validation task is run for a given file\nupload."
  },
  {
    "code": "def eval_instance(self, best_path, gold):\n    total_labels = len(best_path)\n    correct_labels = np.sum(np.equal(best_path, gold))\n    gold_chunks = utils.iobes_to_spans(gold, self.r_l_map)\n    gold_count = len(gold_chunks)\n    guess_chunks = utils.iobes_to_spans(best_path, self.r_l_map)\n    guess_count = len(guess_chunks)\n    overlap_chunks = (gold_chunks & guess_chunks)\n    overlap_count = len(overlap_chunks)\n    return (correct_labels, total_labels, gold_count, guess_count, overlap_count)",
    "nl": "update statics for one instance\n\nargs: \n    best_path (seq_len): predicted\n    gold (seq_len): ground-truth",
    "original_nl": "update statics for one instance\n\nargs: \n    best_path (seq_len): predicted\n    gold (seq_len): ground-truth"
  },
  {
    "code": "def load_bitmap(filename, width=(- 1), height=(- 1)):\n    i = load_icon(filename, width, height)\n    b = wx.EmptyBitmap(i.GetWidth(), i.GetHeight())\n    b.CopyFromIcon(i)\n    return b",
    "nl": "load icon as a wx.Bitmap",
    "original_nl": "load icon as a wx.Bitmap"
  },
  {
    "code": "def setup_masquerade(request, course_key, staff_access=False, reset_masquerade_data=False):\n    if ((request.user is None) or (not settings.FEATURES.get('ENABLE_MASQUERADE', False)) or (not staff_access)):\n        return (None, request.user)\n    if reset_masquerade_data:\n        request.session.pop(MASQUERADE_DATA_KEY, None)\n    masquerade_settings = request.session.setdefault(MASQUERADE_SETTINGS_KEY, {\n        \n    })\n    request.user.masquerade_settings = masquerade_settings\n    course_masquerade = masquerade_settings.get(course_key, None)\n    masquerade_user = None\n    if (course_masquerade and course_masquerade.user_name):\n        try:\n            masquerade_user = CourseEnrollment.objects.users_enrolled_in(course_key).get(username=course_masquerade.user_name)\n        except User.DoesNotExist:\n            course_masquerade = None\n            del masquerade_settings[course_key]\n            request.session.modified = True\n        else:\n            masquerade_user.masquerade_settings = request.user.masquerade_settings\n            masquerade_user.real_user = request.user\n    return (course_masquerade, (masquerade_user or request.user))",
    "nl": "Sets up masquerading for the current user within the current request. The request's user is\nupdated to have a 'masquerade_settings' attribute with the dict of all masqueraded settings if\ncalled from within a request context. The function then returns a pair (CourseMasquerade, User)\nwith the masquerade settings for the specified course key or None if there isn't one, and the\nuser we are masquerading as or request.user if masquerading as a specific user is not active.\n\nIf the reset_masquerade_data flag is set, the field data stored in the session will be cleared.",
    "original_nl": "Sets up masquerading for the current user within the current request. The request's user is\nupdated to have a 'masquerade_settings' attribute with the dict of all masqueraded settings if\ncalled from within a request context. The function then returns a pair (CourseMasquerade, User)\nwith the masquerade settings for the specified course key or None if there isn't one, and the\nuser we are masquerading as or request.user if masquerading as a specific user is not active.\n\nIf the reset_masquerade_data flag is set, the field data stored in the session will be cleared."
  },
  {
    "code": "@staticmethod\ndef from_tsvs(tsvs, bucketized_float_cols=[], string_cols=[], raw_float_cols=[]):\n    d = _DataStore()\n    d.load_tsv(tsvs, bucketized_float_cols=bucketized_float_cols, string_cols=string_cols, raw_float_cols=raw_float_cols)\n    return DataStore(d)",
    "nl": "Loads data from tsvs.\nInputs:\n  tsvs: Blocks of tsvs, among which only the first contains header.\n  bucketized_float_cols: Float columns that will be bucketized. All features will be bucketized.\n  string_cols: String cols.\n  raw_float_cols: Float columns that are loaded raw. Target columns are usually not bucketized.",
    "original_nl": "Loads data from tsvs.\nInputs:\n  tsvs: Blocks of tsvs, among which only the first contains header.\n  bucketized_float_cols: Float columns that will be bucketized. All features will be bucketized.\n  string_cols: String cols.\n  raw_float_cols: Float columns that are loaded raw. Target columns are usually not bucketized."
  },
  {
    "code": "def unzip_file(self, filename, pattern='*'):\n    tar = tarfile.open(name=filename)\n    dir = tempfile.mkdtemp(prefix='tmp_import_custom')\n    tar.extractall(path=dir)\n    return (dir, (glob.glob(('%s/%s' % (dir, pattern))) + glob.glob(('%s/*/%s' % (dir, pattern)))))",
    "nl": "extract *.tar.gz files\n",
    "original_nl": "extract *.tar.gz files\n\nreturns list of extracted file names"
  },
  {
    "code": "def validate(self, attrs):\n    verification_deadline = attrs.get('verification_deadline', None)\n    if verification_deadline:\n        upgrade_deadline = None\n        for mode in attrs['modes']:\n            expires = mode.get('expiration_datetime')\n            if expires:\n                upgrade_deadline = min(expires, (upgrade_deadline or datetime.max.replace(tzinfo=pytz.utc)))\n        if ((upgrade_deadline is not None) and (verification_deadline < upgrade_deadline)):\n            raise serializers.ValidationError('Verification deadline must be after the course mode upgrade deadlines.')\n    return attrs",
    "nl": "Ensure the verification deadline occurs AFTER the course mode enrollment deadlines.",
    "original_nl": "Ensure the verification deadline occurs AFTER the course mode enrollment deadlines."
  },
  {
    "code": "def handle_lib_name(lib_name, basedir):\n    if isinstance(lib_name, dict):\n        lib_names = to_tuple(select_platform_value(lib_name))\n    else:\n        lib_names = to_tuple(lib_name)\n    for try_name in lib_names:\n        if os.path.isabs(try_name):\n            path = try_name\n        else:\n            path = os.path.join(basedir, try_name)\n        if os.path.exists(path):\n            return path\n    for try_name in lib_names:\n        path = find_library(try_name)\n        if path:\n            return path\n    raise ValueError(\"Cannot find library '{}'\".format(lib_names))",
    "nl": "Find the path to the library\n\n`lib_name` can be specified directly as a string (or sequence of strings), or within a\n\"platform\" dict. If multiple libs are specified, the first one found will be returned.",
    "original_nl": "Find the path to the library\n\n`lib_name` can be specified directly as a string (or sequence of strings), or within a\n\"platform\" dict. If multiple libs are specified, the first one found will be returned."
  },
  {
    "code": "def print_variant(variant_line=None, variant_dict=None, header_line=None, priority=None, outfile=None, mode='vcf', silent=False):\n    if variant_dict:\n        if (not header_line):\n            raise IOError('Print line needs a header_line when printing variant dict.')\n        print_line = [variant_dict.get(entry, '.') for entry in header_line]\n    else:\n        print_line = variant_line.rstrip().split('\\t')\n    if (mode == 'modified'):\n        print_line = print_line[1:]\n    elif priority:\n        print_line = ([priority] + print_line)\n    print_string = '\\t'.join(print_line)\n    if (not isinstance(print_string, str)):\n        print_string = print_string.encode('utf-8')\n    if outfile:\n        outfile.write((print_string + '\\n'))\n    elif (not silent):\n        print(print_string)\n    return",
    "nl": "Print a variant line.\n\nIf a result file is provided the variante will be appended to the file, \notherwise they are printed to stdout.\n\nThere are two modes, 'vcf' or 'modified'.\nIf 'vcf' we expect plain vcf variants and print them as they came in.\nIf 'modified' the first column has been used for sorting so we skip \nthat one.\n\nArgs:\n    variant_line (str): A vcf formatted variant line\n    variant_dict (dict): A variant dictionary\n    header_line (list): A list with haeder columns\n    priority (str): the priority for this variant\n    outfile (FileHandle): An opened file_handle\n    mode (str): 'vcf' or 'modified'\n    silent (bool): Bool. If nothing should be printed.",
    "original_nl": "Print a variant line.\n\nIf a result file is provided the variante will be appended to the file, \notherwise they are printed to stdout.\n\nThere are two modes, 'vcf' or 'modified'.\nIf 'vcf' we expect plain vcf variants and print them as they came in.\nIf 'modified' the first column has been used for sorting so we skip \nthat one.\n\nArgs:\n    variant_line (str): A vcf formatted variant line\n    variant_dict (dict): A variant dictionary\n    header_line (list): A list with haeder columns\n    priority (str): the priority for this variant\n    outfile (FileHandle): An opened file_handle\n    mode (str): 'vcf' or 'modified'\n    silent (bool): Bool. If nothing should be printed."
  },
  {
    "code": "@staticmethod\ndef get_elastic_mappings(es_major):\n    if (es_major != '2'):\n        mapping = '\\n            {\\n                \"properties\": {\\n                    \"title_analyzed\": {\\n                        \"type\": \"text\"\\n                    }\\n               }\\n            } '\n    else:\n        mapping = '\\n                {\\n                    \"properties\": {\\n                        \"title_analyzed\": {\\n                            \"type\": \"string\",\\n                             \"index\": \"analyzed\"\\n                        }\\n                   }\\n                } '\n    return {\n        'items': mapping,\n    }",
    "nl": "Get Elasticsearch mapping.\n\n",
    "original_nl": "Get Elasticsearch mapping.\n\n:param es_major: major version of Elasticsearch, as string\n:returns:        dictionary with a key, 'items', with the mapping"
  },
  {
    "code": "def clear_cookies(self):\n    lib.ph_context_clear_cookies()",
    "nl": "Clear all cookies.",
    "original_nl": "Clear all cookies."
  },
  {
    "code": "def test_DoubleQuoteString(self):\n    manhole.lastColorizedLine('\"1\"')",
    "nl": "Colorize an integer in double quotes.",
    "original_nl": "Colorize an integer in double quotes."
  },
  {
    "code": "def test_perform1(self):\n    self.task.write_in_database('val', 'World')\n    self.task.message = 'Hello {Test_val}'\n    self.root.prepare()\n    self.task.perform()\n    assert (self.task.get_from_database('Test_message') == 'Hello World')",
    "nl": "Test checking that the message value gets written to the database",
    "original_nl": "Test checking that the message value gets written to the database"
  },
  {
    "code": "def buildFlattener():\n    with IsolatedSession() as issn:\n        mat_input = tf.placeholder(tf.float32, [None, None])\n        mat_output = tf.identity(tf.reshape(mat_input, shape=[(- 1)]), name='output')\n        gfn = issn.asGraphFunction([mat_input], [mat_output])\n    return gfn",
    "nl": "Build a flattening layer to remove the extra leading tensor dimension.\ne.g. a tensor of shape [1, W, H, C] will have a shape [W, H, C] after applying this.",
    "original_nl": "Build a flattening layer to remove the extra leading tensor dimension.\ne.g. a tensor of shape [1, W, H, C] will have a shape [W, H, C] after applying this."
  },
  {
    "code": "def parse_term(s):\n    (size, s) = s.split(b':', 1)\n    size = int(size)\n    return (s[:size], s[size:])",
    "nl": "Parse single s-expr term from bytes.",
    "original_nl": "Parse single s-expr term from bytes."
  },
  {
    "code": "def verify_tooltips_displayed(self):\n    for (index, tab) in enumerate(self.q(css='#sequence-list > li')):\n        ActionChains(self.browser).move_to_element(tab).perform()\n        self.wait_for_element_visibility('#tab_{index} > .sequence-tooltip'.format(index=index), 'Tab {index} should appear'.format(index=index))",
    "nl": "Verify that all sequence navigation bar tooltips are being displayed upon mouse hover.\n\nIf a tooltip does not appear, raise a BrokenPromise.",
    "original_nl": "Verify that all sequence navigation bar tooltips are being displayed upon mouse hover.\n\nIf a tooltip does not appear, raise a BrokenPromise."
  },
  {
    "code": "def test_error_log_message(self, skip_if_no_fixture, caplog, simple_log, request):\n    simple_log.error('test message for error')\n    if ('.' in request.module.__name__):\n        mod_name = request.module.__name__.split('.', 1)[1]\n    else:\n        mod_name = request.module.__name__\n    for records in caplog.records():\n        assert (records.message == 'test message for error')\n        assert (records.levelname == 'ERROR')\n        assert (records.module == mod_name)\n        assert (records.classname == 'TestLogger.')\n        assert (records.funcName == 'test_error_log_message')\n        thread_name = threading.current_thread().name\n        assert (records.threadName == thread_name)",
    "nl": "Verify that log message for level ERROR contains correct values.",
    "original_nl": "Verify that log message for level ERROR contains correct values."
  },
  {
    "code": "@property\ndef hasz(self):\n    return self._z",
    "nl": "Returns whether this coordinate sequence is 3D.  This property value is\ninherited from the parent Geometry.",
    "original_nl": "Returns whether this coordinate sequence is 3D.  This property value is\ninherited from the parent Geometry."
  },
  {
    "code": "def _GetProvides(sources):\n    provides = set()\n    for source in sources:\n        provides.update(source.provides)\n    return provides",
    "nl": "Get all namespaces provided by a collection of sources.",
    "original_nl": "Get all namespaces provided by a collection of sources."
  },
  {
    "code": "def get_command_info(self):\n    return self.command_info",
    "nl": "Returns the ChildInfo object for the current command or level",
    "original_nl": "Returns the ChildInfo object for the current command or level"
  },
  {
    "code": "def all_parents(self):\n    result = {self}\n    for parent in self.parents:\n        result.update(parent.all_parents())\n    return result",
    "nl": "Returns all transitive parent nodes.",
    "original_nl": "Returns all transitive parent nodes."
  },
  {
    "code": "def __init__(self, pin):\n    Illuminator__.__init__(self, pin)",
    "nl": "create an instance of a LED connected to the \npin provided.\nPin should be configured as Pin('X1', Pin.OUT_PP)",
    "original_nl": "create an instance of a LED connected to the \npin provided.\nPin should be configured as Pin('X1', Pin.OUT_PP)"
  },
  {
    "code": "def _get_num_pages(self):\n    if (self._num_pages is None):\n        hits = ((self.count - 1) - self.orphans)\n        if (hits < 1):\n            hits = 0\n        if ((hits == 0) and (not self.allow_empty_first_page)):\n            self._num_pages = 0\n        else:\n            self._num_pages = ((hits // self.per_page) + 1)\n    return self._num_pages",
    "nl": "Returns the total number of pages.",
    "original_nl": "Returns the total number of pages."
  },
  {
    "code": "def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\n    placemarks = []\n    klass = get_model(label, model)\n    if (not klass):\n        raise Http404(('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model)))\n    if field_name:\n        try:\n            info = klass._meta.get_field_by_name(field_name)\n            if (not isinstance(info[0], GeometryField)):\n                raise Exception\n        except:\n            raise Http404('Invalid geometry field.')\n    connection = connections[using]\n    if connection.ops.postgis:\n        placemarks = klass._default_manager.using(using).kml(field_name=field_name)\n    else:\n        placemarks = []\n        if connection.ops.oracle:\n            qs = klass._default_manager.using(using).transform(4326, field_name=field_name)\n        else:\n            qs = klass._default_manager.using(using).all()\n        for mod in qs:\n            setattr(mod, 'kml', getattr(mod, field_name).kml)\n            placemarks.append(mod)\n    if compress:\n        render = render_to_kmz\n    else:\n        render = render_to_kml\n    return render('gis/kml/placemarks.kml', {\n        'places': placemarks,\n    })",
    "nl": "This view generates KML for the given app label, model, and field name.\n\nThe model's default manager must be GeoManager, and the field name\nmust be that of a geographic field.",
    "original_nl": "This view generates KML for the given app label, model, and field name.\n\nThe model's default manager must be GeoManager, and the field name\nmust be that of a geographic field."
  },
  {
    "code": "def get_entity_class(resource):\n    reg = get_current_registry()\n    if (IInterface in provided_by(resource)):\n        ent_cls = reg.getUtility(resource, name='entity-class')\n    else:\n        ent_cls = reg.getAdapter(resource, IEntity, name='entity-class')\n    return ent_cls",
    "nl": "Returns the entity class registered for the given registered resource.\n\n",
    "original_nl": "Returns the entity class registered for the given registered resource.\n\n:param resource: registered resource\n:type collection: class implementing or instance providing a registered\n    resource interface.\n:return: entity class\n    (class implementing `everest.entities.interfaces.IEntity`)"
  },
  {
    "code": "def SpecToYAML(spec, level=0):\n    lines = []\n    for (k, v) in spec.iteritems():\n        if (k[0] is '_'):\n            continue\n        elif isinstance(v, Param):\n            com = ('# ' if (v.default is None) else '')\n            sv = v.decoratedString()\n            lines.append(('%s%s%s: %s' % (('    ' * level), com, k, sv)))\n        elif isinstance(v, dict):\n            lines.append(('%s%s:' % (('    ' * level), k)))\n            lines.append(SpecToYAML(v, (level + 1)))\n        else:\n            raise Exception(('Malformed config spec. ' + 'Should be dict tree with Param leaves'))\n    return '\\n'.join(lines)",
    "nl": "Convert a config spec to YAML",
    "original_nl": "Convert a config spec to YAML"
  },
  {
    "code": "def refresh_access_token(self, raw_token):\n    raise NotImplementedError('Defined in a sub-class')",
    "nl": "Refreshing an OAuth2 token using a refresh token.",
    "original_nl": "Refreshing an OAuth2 token using a refresh token."
  },
  {
    "code": "def to_glyphs_master_user_data(self, ufo, master):\n    target_user_data = master.userData\n    for (key, value) in ufo.lib.items():\n        if _user_data_has_no_special_meaning(key):\n            target_user_data[key] = value\n    if ufo.data.fileNames:\n        from glyphsLib.types import BinaryData\n        ufo_data = {\n            \n        }\n        for os_filename in ufo.data.fileNames:\n            filename = posixpath.join(*os_filename.split(os.path.sep))\n            ufo_data[filename] = BinaryData(ufo.data[os_filename])\n        master.userData[UFO_DATA_KEY] = ufo_data",
    "nl": "Set the GSFontMaster userData from the UFO master-specific lib data.",
    "original_nl": "Set the GSFontMaster userData from the UFO master-specific lib data."
  },
  {
    "code": "def analyze_apk(self, eandro_apk):\n    if (eandro_apk is not None):\n        res = AnalyzeUtil.analyze_apk(eandro_apk, self.androscripts, self.min_script_needs, reset_scripts=True)\n        if (res is not None):\n            (fastapk, scripts) = res\n            res[1] = deepcopy(scripts)\n            clilog.debug('analyzed %s', fastapk.short_description())\n            return res",
    "nl": "Analyze the `eandro_apk` and return the analysis results.\n",
    "original_nl": "Analyze the `eandro_apk` and return the analysis results.\n\nParameters\n----------\neandro_apk : EAndroApk\n    The apk to analyze.\n\nReturns\n-------\nlist<FastApk, AndroScript>\nNone\n    If error happened."
  },
  {
    "code": "def test_match_slack_github_username(self):\n    name = slack._match_slack_github_username(self.USERS, GENERIC_USERNAME)\n    self.assertEqual(name, GENERIC_USERNAME)\n    users = []\n    name = slack._match_slack_github_username(users, GENERIC_USERNAME)\n    self.assertIsNone(name)",
    "nl": "Test matching a slack and github username.",
    "original_nl": "Test matching a slack and github username."
  },
  {
    "code": "@wrappers.Request.application\ndef _serve_runs(self, request):\n    if self._db_connection_provider:\n        db = self._db_connection_provider()\n        cursor = db.execute('\\n        SELECT\\n          run_name,\\n          started_time IS NULL as started_time_nulls_last,\\n          started_time\\n        FROM Runs\\n        ORDER BY started_time_nulls_last, started_time, run_name\\n      ')\n        run_names = [row[0] for row in cursor]\n    else:\n        run_names = sorted(self._multiplexer.Runs())\n\n        def get_first_event_timestamp(run_name):\n            try:\n                return self._multiplexer.FirstEventTimestamp(run_name)\n            except ValueError:\n                tf.logging.warning('Unable to get first event timestamp for run %s', run_name)\n                return float('inf')\n        run_names.sort(key=get_first_event_timestamp)\n    return http_util.Respond(request, run_names, 'application/json')",
    "nl": "Serve a JSON array of run names, ordered by run started time.\n\nSort order is by started time (aka first event time) with empty times sorted\nlast, and then ties are broken by sorting on the run name.",
    "original_nl": "Serve a JSON array of run names, ordered by run started time.\n\nSort order is by started time (aka first event time) with empty times sorted\nlast, and then ties are broken by sorting on the run name."
  },
  {
    "code": "def admins(self):\n    return self.root_admins",
    "nl": "Returns the list of admins accounts\n@rtype: List\n",
    "original_nl": "Returns the list of admins accounts\n@rtype: List\n@return: list of admin accounts"
  },
  {
    "code": "def filename_to_uri(self, filename):\n    try:\n        return self._uri_cache[filename]\n    except KeyError:\n        value = self._relativeize(filename)\n        self._uri_cache[filename] = value\n        return value",
    "nl": "Convert the given filename to a uri relative to \nthis TemplateCollection.",
    "original_nl": "Convert the given filename to a uri relative to \nthis TemplateCollection."
  },
  {
    "code": "def write_text(self, text):\n    self.write('<p>')\n    self.write(text)\n    self.write('</p>')",
    "nl": "Writes a paragraph of text",
    "original_nl": "Writes a paragraph of text"
  },
  {
    "code": "def fixModelRefs(self, phrasedml_str):\n    model_ref = re.compile('^.*\\\\s*model\\\\s*\"([^\"]*)\"\\\\s*$')\n    out_str = ''\n    for line in phrasedml_str.splitlines():\n        match = model_ref.match(line)\n        if match:\n            filename = match.group(1)\n            if self.isInRootDir(filename):\n                line = line.replace(filename, self.formatResource(filename))\n        out_str += (line + '\\n')\n    return out_str",
    "nl": "Changes all references of type myModel.xml to myModel.",
    "original_nl": "Changes all references of type myModel.xml to myModel."
  },
  {
    "code": "def _adjust_width(self):\n    if (self.bar_width > self.max_iter):\n        self.bar_width = int(self.max_iter)",
    "nl": "Shrinks bar if number of iterations is less than the bar width",
    "original_nl": "Shrinks bar if number of iterations is less than the bar width"
  },
  {
    "code": "def _checkPasswordConfirm(self, editable=None, reset_status=None):\n    if reset_status:\n        return GUICheck.CHECK_OK\n    pw = self.pw.get_text()\n    confirm = self.confirm.get_text()\n    if (((not pw) and (not confirm)) and self._kickstarted):\n        result = GUICheck.CHECK_OK\n    elif (confirm and (pw != confirm)):\n        result = _(PASSWORD_CONFIRM_ERROR_GUI)\n    else:\n        result = GUICheck.CHECK_OK\n    if (result == GUICheck.CHECK_OK):\n        if (editable == self.confirm):\n            self._password_check.update_check_status(check_data=True)\n        else:\n            self._confirm_check.update_check_status(check_data=True)\n    return result",
    "nl": "Check whether the password matches the confirmation data.",
    "original_nl": "Check whether the password matches the confirmation data."
  },
  {
    "code": "def login_required(func):\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if (not g.current_user.is_active):\n            flash_notice('Bitte melde dich an.')\n            return redirect_to('authentication.login_form')\n        return func(*args, **kwargs)\n    return wrapper",
    "nl": "Ensure the current user has logged in.",
    "original_nl": "Ensure the current user has logged in."
  },
  {
    "code": "def test_reproject_celestial_3d():\n    header_in = fits.Header.fromtextfile(get_pkg_data_filename('../../tests/data/cube.hdr'))\n    array_in = np.ones((3, 200, 180))\n    wcs_in = WCS(header_in)\n    wcs_out = wcs_in.deepcopy()\n    wcs_out.wcs.ctype = ['GLON-SIN', 'GLAT-SIN', wcs_in.wcs.ctype[2]]\n    wcs_out.wcs.crval = [158.0501, (- 21.530282), wcs_in.wcs.crval[2]]\n    wcs_out.wcs.crpix = [50.0, 50.0, (wcs_in.wcs.crpix[2] + 0.4)]\n    (out_full, foot_full) = _reproject_full(array_in, wcs_in, wcs_out, (3, 160, 170))\n    (out_celestial, foot_celestial) = _reproject_celestial(array_in, wcs_in, wcs_out, (3, 160, 170))\n    np.testing.assert_allclose(out_full, out_celestial)\n    np.testing.assert_allclose(foot_full, foot_celestial)",
    "nl": "Test both full_reproject and slicewise reprojection. We use a case where the\nnon-celestial slices are the same and therefore where both algorithms can\nwork.",
    "original_nl": "Test both full_reproject and slicewise reprojection. We use a case where the\nnon-celestial slices are the same and therefore where both algorithms can\nwork."
  },
  {
    "code": "@dev.command()\n@click.pass_context\n@click.argument('ticket', default='')\ndef ticket(ctx, ticket):\n    issue_id = (make_issue_descriptor(ctx.obj.repo.active_branch.name).id if (not ticket) else ticket)\n    issue = verify_ticket_exists(ctx.obj.issue_tracker(), issue_id)\n    url = issue.browse_url\n    click.echo('Opening \"{}\"'.format(url))\n    webbrowser.open_new(url)",
    "nl": "Open the ticket for the current feature or the one supplied in the ticket argument.",
    "original_nl": "Open the ticket for the current feature or the one supplied in the ticket argument."
  },
  {
    "code": "def assert_is_bbox_dataset(dataset, n_fg_class, n_example=None):\n    assert (len(dataset) > 0), 'The length of dataset must be greater than zero.'\n    if n_example:\n        for _ in six.moves.range(n_example):\n            i = np.random.randint(0, len(dataset))\n            _check_example(dataset[i], n_fg_class)\n    else:\n        for i in six.moves.range(len(dataset)):\n            _check_example(dataset[i], n_fg_class)",
    "nl": "Checks if a dataset satisfies the bounding box dataset API.\n\nThis function checks if a given dataset satisfies the bounding box dataset\nAPI or not.\nIf the dataset does not satifiy the API, this function raises an\n:class:`AssertionError`.\n\nArgs:\n    dataset: A dataset to be checked.\n    n_fg_class (int): The number of foreground classes.\n    n_example (int): The number of examples to be checked.\n        If this argument is specified, this function picks\n        examples ramdomly and checks them. Otherwise,\n        this function checks all examples.",
    "original_nl": "Checks if a dataset satisfies the bounding box dataset API.\n\nThis function checks if a given dataset satisfies the bounding box dataset\nAPI or not.\nIf the dataset does not satifiy the API, this function raises an\n:class:`AssertionError`.\n\nArgs:\n    dataset: A dataset to be checked.\n    n_fg_class (int): The number of foreground classes.\n    n_example (int): The number of examples to be checked.\n        If this argument is specified, this function picks\n        examples ramdomly and checks them. Otherwise,\n        this function checks all examples."
  },
  {
    "code": "def addFromUpperLowerFile(self, fileName):\n    fileText = settings.getFileInAlterationsOrGivenDirectory(os.path.dirname(__file__), fileName)\n    fileLines = archive.getTextLines(fileText)\n    self.distanceFeedRate.addLinesSetAbsoluteDistanceMode(fileLines)",
    "nl": "Add lines of text from the fileName or the lowercase fileName, if there is no file by the original fileName in the directory.",
    "original_nl": "Add lines of text from the fileName or the lowercase fileName, if there is no file by the original fileName in the directory."
  },
  {
    "code": "def main():\n    options = _parse_args()\n    archive = download_setuptools(version=options.version, download_base=options.download_base, downloader_factory=options.downloader_factory)\n    return _install(archive, _build_install_args(options))",
    "nl": "Install or upgrade setuptools and EasyInstall.",
    "original_nl": "Install or upgrade setuptools and EasyInstall."
  },
  {
    "code": "def _check_active(self):\n    time.sleep(5)\n    return True",
    "nl": "Dummy service activity check.",
    "original_nl": "Dummy service activity check."
  },
  {
    "code": "def get_num_of_req(self):\n    if (self.num_of_req > 0):\n        return self.num_of_req\n    self.num_of_req = 0\n    if self.c_reader:\n        self.num_of_req = c_cacheReader.get_num_of_req(self.c_reader)\n    else:\n        while (self.read_one_req() is not None):\n            self.num_of_req += 1\n    self.reset()\n    return self.num_of_req",
    "nl": "count the number of requests in the trace, fast for binary type trace,\nfor plain/csv type trace, this is slow\n",
    "original_nl": "count the number of requests in the trace, fast for binary type trace,\nfor plain/csv type trace, this is slow\n:return: the number of requests in the trace"
  },
  {
    "code": "def add_getwork(server, username, password):\n    __patch()\n    key = get_key(server, username, password)\n    if (key not in getworks):\n        getworks[key] = 0\n    getworks[key] += 1\n    username = shorten(username)\n    logging.info(('Getwork: %s:%s@%s' % (username, password, server)))",
    "nl": "Adds a getwork to the database",
    "original_nl": "Adds a getwork to the database"
  },
  {
    "code": "def remove(self, key):\n    self.__delitem__(key)",
    "nl": "Remove object being referenced to by ident from collection\n\n",
    "original_nl": "Remove object being referenced to by ident from collection\n\n:param key: ID or serial\n:type key: str"
  },
  {
    "code": "def normalised(self, max_time_back_seconds=None, resolution=60, status_type=None):\n    if (max_time_back_seconds is None):\n        start_time = self.EPOCH_TIME\n    else:\n        start_time = (self.START_TIME - max_time_back_seconds)\n    for tick in range(start_time, self.START_TIME, resolution):\n        status_obj = self.get_status(tick)\n        if (status_type is None):\n            (yield status_obj.highest_active_status_type())\n        else:\n            (yield status_obj._status[status_type])",
    "nl": "Turns a sparse time series into a dense one, with number of seconds per bucket specified by resolution.\nIf a status_type (status, webStatus, messengerStatus etc.) is given, returns a generator of the status level (online, offline, idle) for that status type.",
    "original_nl": "Turns a sparse time series into a dense one, with number of seconds per bucket specified by resolution.\nIf a status_type (status, webStatus, messengerStatus etc.) is given, returns a generator of the status level (online, offline, idle) for that status type."
  },
  {
    "code": "@cached_func\ndef get_replacement_mapping():\n    mapping = {\n        \n    }\n    for (cp, repl) in iteritems(generate_re_mapping(diacritic_for_letters(regenerate=False))):\n        mapping.setdefault(cp, []).extend(repl)\n    for (cp, repl) in iteritems(get_decomps_mapping(regenerate=False)):\n        mapping.setdefault(cp, []).extend(repl)\n    for (cp, repl) in iteritems(get_punctuation_mapping(regenerate=False)):\n        mapping.setdefault(cp, []).extend(repl)\n    return mapping",
    "nl": "Returns a dict mapping a sequence of characters to another sequence\nof characters.\n\nIf a key occurs in a text, it should also match any of the characters in\nin the value.",
    "original_nl": "Returns a dict mapping a sequence of characters to another sequence\nof characters.\n\nIf a key occurs in a text, it should also match any of the characters in\nin the value."
  },
  {
    "code": "def tag_resources(ResourceARNList=None, Tags=None):\n    pass",
    "nl": "Applies one or more tags to the specified resources. Note the following:\nSee also: AWS API Documentation\n\n\n:example: response = client.tag_resources(\n    ResourceARNList=[\n        'string',\n    ],\n    Tags={\n        'string': 'string'\n    }\n)\n\n\n:type ResourceARNList: list\n",
    "original_nl": "Applies one or more tags to the specified resources. Note the following:\nSee also: AWS API Documentation\n\n\n:example: response = client.tag_resources(\n    ResourceARNList=[\n        'string',\n    ],\n    Tags={\n        'string': 'string'\n    }\n)\n\n\n:type ResourceARNList: list\n:param ResourceARNList: [REQUIRED]\n        A list of ARNs. An ARN (Amazon Resource Name) uniquely identifies a resource. You can specify a minimum of 1 and a maximum of 20 ARNs (resources) to tag. An ARN can be set to a maximum of 1600 characters. For more information, see Amazon Resource Names (ARNs) and AWS Service Namespaces in the AWS General Reference .\n        (string) --\n        \n\n:type Tags: dict\n:param Tags: [REQUIRED]\n        The tags that you want to add to the specified resources. A tag consists of a key and a value that you define.\n        (string) --\n        (string) --\n        \n\n:rtype: dict\n:return: {\n    'FailedResourcesMap': {\n        'string': {\n            'StatusCode': 123,\n            'ErrorCode': 'InternalServiceException'|'InvalidParameterException',\n            'ErrorMessage': 'string'\n        }\n    }\n}\n\n\n:returns: \nResourceARNList (list) -- [REQUIRED]\nA list of ARNs. An ARN (Amazon Resource Name) uniquely identifies a resource. You can specify a minimum of 1 and a maximum of 20 ARNs (resources) to tag. An ARN can be set to a maximum of 1600 characters. For more information, see Amazon Resource Names (ARNs) and AWS Service Namespaces in the AWS General Reference .\n\n(string) --\n\n\nTags (dict) -- [REQUIRED]\nThe tags that you want to add to the specified resources. A tag consists of a key and a value that you define.\n\n(string) --\n(string) --"
  },
  {
    "code": "@property\ndef capabilities(self):\n    return [self.Capability(item) for item in self._editor.get_element_value(self._capabilities_element).split(',') if (item != 'Uploads')]",
    "nl": "Gets a list of the enabled operations (by type name) that are currently enabled for the feature service.",
    "original_nl": "Gets a list of the enabled operations (by type name) that are currently enabled for the feature service."
  },
  {
    "code": "def build_tables(target, source, env):\n    builder_attributes = {\n        'name': 'Tablefill',\n        'valid_extensions': ['.lyx', '.tex'],\n        'exec_opts': '-interaction nonstopmode -jobname',\n    }\n    builder = TableBuilder(target, source, env, **builder_attributes)\n    builder.execute_system_call()\n    return None",
    "nl": "Build a SCons target by filling a table\n\nThis function uses the tablefill function from gslab_fill to produced a \nfilled table from (i) an empty table in a LyX/Tex file and (ii) text files \ncontaining data to be used in filling the table. \n",
    "original_nl": "Build a SCons target by filling a table\n\nThis function uses the tablefill function from gslab_fill to produced a \nfilled table from (i) an empty table in a LyX/Tex file and (ii) text files \ncontaining data to be used in filling the table. \n\nParameters\n----------\ntarget: string or list \n    The target(s) of the SCons command.\nsource: string or list\n    The source(s) of the SCons command. The first source specified\n    should be the LyX/Tex file specifying the table format. The subsequent \n    sources should be the text files containing the data with which the\n    tables are to be filled. \nenv: SCons construction environment, see SCons user guide 7.2"
  },
  {
    "code": "def batch_size_from_nested_tensors(tensors):\n    for tensor in snt.nest.flatten(tensors):\n        if (tensor.get_shape().ndims > 0):\n            return tf.shape(tensor)[0]\n    return None",
    "nl": "Returns the batch dimension from the first non-scalar tensor given.",
    "original_nl": "Returns the batch dimension from the first non-scalar tensor given."
  },
  {
    "code": "def connection_lost(self, exc):\n    self._transport = None\n    super().connection_lost(exc)",
    "nl": "Called when the connection is lost or closed.",
    "original_nl": "Called when the connection is lost or closed."
  },
  {
    "code": "def get_bash_file(self, homefolder):\n    rc_file = os.path.abspath(('%s/.bashrc' % homefolder))\n    if os.path.exists(os.path.abspath(('%s/.bash_aliases' % homefolder))):\n        rc_file = os.path.abspath(('%s/.bash_aliases' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.bashrc' % homefolder))):\n        rc_file = os.path.abspath(('%s/.bashrc' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.bash_profile' % homefolder))):\n        rc_file = os.path.abspath(('%s/.bash_profile' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.profile' % homefolder))):\n        rc_file = os.path.abspath(('%s/.profile' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.login' % homefolder))):\n        rc_file = os.path.abspath(('%s/.login' % homefolder))\n    return rc_file",
    "nl": "Return the path to the user's bash rc file.",
    "original_nl": "Return the path to the user's bash rc file."
  },
  {
    "code": "def parse_out_axes(codes):\n    axesCodes = 'XYZAB'\n    parsedAxes = (set(axesCodes) & set(codes))\n    return list(sorted(parsedAxes))",
    "nl": "Given a list of codes, returns a list of all present axes\n\n",
    "original_nl": "Given a list of codes, returns a list of all present axes\n\n@param list codes: Codes parsed out of the gcode command\n@return list: List of axes in codes"
  },
  {
    "code": "def load(read_file, formatting):\n    if (formatting == 'json'):\n        return json.load(read_file)\n    if (formatting == 'pkl'):\n        return pickle.load(read_file)\n    raise ValueError('Unsupported format: {}'.format(formatting))",
    "nl": "Return a writable from read_file. Support json, and pkl formats.",
    "original_nl": "Return a writable from read_file. Support json, and pkl formats."
  },
  {
    "code": "def test_kml(self):\n    self._load_city_data()\n    h = City3D.objects.kml(precision=6).get(name='Houston')\n    ref_kml_regex = re.compile('^<Point><coordinates>-95.363\\\\d+,29.763\\\\d+,18</coordinates></Point>$')\n    self.assertTrue(ref_kml_regex.match(h.kml))",
    "nl": "Test GeoQuerySet.kml() with Z values.",
    "original_nl": "Test GeoQuerySet.kml() with Z values."
  },
  {
    "code": "def test_overflowBytesSentToWrappedProtocol(self):\n    factory = HAProxyWrappingFactory(Factory.forProtocol(StaticProtocol))\n    proto = factory.buildProtocol(address.IPv6Address('TCP', b'::1', 8080))\n    transport = StringTransportWithDisconnection()\n    proto.makeConnection(transport)\n    proto.dataReceived((self.IPV6HEADER + b'HTTP/1.1 / GET'))\n    self.assertEqual(proto.wrappedProtocol.data, b'HTTP/1.1 / GET')",
    "nl": "Test if non-header bytes are passed to the wrapped protocol.",
    "original_nl": "Test if non-header bytes are passed to the wrapped protocol."
  },
  {
    "code": "def test_limit(self):\n    N = 10\n    var = list(Pagination(limit=N).limit(self.variants_django))\n    self.assertEqual(len(var), N)\n    var = list(Pagination(limit=N).limit(self.variants))\n    self.assertEqual(len(var), N)",
    "nl": "Limiting to N results should return N elements.",
    "original_nl": "Limiting to N results should return N elements."
  },
  {
    "code": "def _trace_summary(self):\n    for (i, (val, args)) in enumerate(self.trace):\n        if (args is StopIteration):\n            info = 'Terminated'\n        else:\n            pprint = ','.join(((('{' + ','.join((('%s=%r' % (k, v)) for (k, v) in arg.items()))) + '}') for arg in args))\n            info = ('exploring arguments [%s]' % pprint)\n        if (i == 0):\n            print(('Step %d: Initially %s.' % (i, info)))\n        else:\n            print(('Step %d: %s after receiving input(s) %s.' % (i, info.capitalize(), val)))",
    "nl": "Summarizes the trace of values used to update the DynamicArgs\nand the arguments subsequently returned. May be used to\nimplement the summary method.",
    "original_nl": "Summarizes the trace of values used to update the DynamicArgs\nand the arguments subsequently returned. May be used to\nimplement the summary method."
  },
  {
    "code": "def on_request(self, *args):\n    return EndpointOutput(content=self.server.endpoint_sources)",
    "nl": "RPC task to get all available celery endpoints.\n",
    "original_nl": "RPC task to get all available celery endpoints.\n\nReturns:\n    ReturnMessage: A return message where the content is a list of\n    dictionaries containing information about the available endpoints."
  },
  {
    "code": "def test_serie_precedence_over_global_config():\n    chart = Line(stroke=False)\n    chart.add('1', s1, stroke=True)\n    chart.add('2', s2)\n    q = chart.render_pyquery()\n    assert (len(q('.serie-0 .line')) == 1)\n    assert (len(q('.serie-1 .line')) == 0)\n    assert (len(q('.serie-0 .dot')) == 5)\n    assert (len(q('.serie-1 .dot')) == 6)",
    "nl": "Test that per serie configuration overide global configuration",
    "original_nl": "Test that per serie configuration overide global configuration"
  },
  {
    "code": "def get_queue_limit(self, queue_name):\n    for queue_def in self._config['queue_definitions']:\n        if (queue_def['name'] == queue_name):\n            return int(queue_def['walltime_limit'])\n    return None",
    "nl": "get the maximum walltime for the queue specified",
    "original_nl": "get the maximum walltime for the queue specified"
  },
  {
    "code": "def extend_volume(self, volume, new_size):\n    return self._unify_volume(self._impl.extend_volume(volume, new_size=new_size))",
    "nl": "Extend the size of the specified volume.",
    "original_nl": "Extend the size of the specified volume."
  },
  {
    "code": "@property\ndef inverse(self):\n    return AsinhStretch(a=(1.0 / np.sinh((1.0 / self.a))))",
    "nl": "A stretch object that performs the inverse operation.",
    "original_nl": "A stretch object that performs the inverse operation."
  },
  {
    "code": "def action_visualize(args, fromshell=False, path=None, title='', theme='', verbose=False):\n    if (not args):\n        ontouri = ontospy_actions.action_listlocal(all_details=False)\n        if ontouri:\n            islocal = True\n        else:\n            raise SystemExit(1)\n    elif fromshell:\n        ontouri = args\n        islocal = True\n    else:\n        ontouri = args[0]\n        islocal = False\n    viztype = ask_visualization()\n    if (viztype == ''):\n        return None\n    USE_CACHE = False\n    if (islocal and USE_CACHE):\n        g = get_pickled_ontology(ontouri)\n        if (not g):\n            g = do_pickle_ontology(ontouri)\n    else:\n        printDebug('Loading graph...', dim=True)\n        if islocal:\n            g = Ontospy(os.path.join(ontospy_manager.get_home_location(), ontouri), verbose=verbose)\n        else:\n            g = Ontospy(ontouri, verbose=verbose)\n    if (not path):\n        from os.path import expanduser\n        home = expanduser('~')\n        onto_path = slugify(unicode(ontouri))\n        viz_path = slugify(unicode(VISUALIZATIONS_LIST[viztype]['Title']))\n        path = os.path.join(home, ((('ontospy-viz/' + onto_path) + '/') + viz_path))\n        if (not os.path.exists(path)):\n            os.makedirs(path)\n    printDebug('Building visualization...', dim=True)\n    url = build_visualization(ontouri, g, viztype, path, title, theme)\n    return url",
    "nl": "export model into another format eg html, d3 etc...\n<fromshell> : the local name is being passed from ontospy shell",
    "original_nl": "export model into another format eg html, d3 etc...\n<fromshell> : the local name is being passed from ontospy shell"
  },
  {
    "code": "def capitalize_first_letter(string):\n    if string:\n        string = (string[0].upper() + string[1:])\n    return string",
    "nl": "Same as capitalize() but leaves the other letter untouched.\n\nE.g.: 'abCd Efg' -> 'AbDd Efg'",
    "original_nl": "Same as capitalize() but leaves the other letter untouched.\n\nE.g.: 'abCd Efg' -> 'AbDd Efg'"
  },
  {
    "code": "def get_cons_enc(cons):\n    res = list()\n    for c in list(set(cons)):\n        (name, form) = c.strip(')').split(' ')[1:]\n        if (form == 'Bool'):\n            res.append(z3.Bool(name))\n        elif (form == 'Real'):\n            res.append(z3.Real(name))\n    return res",
    "nl": "Returns z3 instance for each string that declares a constant.",
    "original_nl": "Returns z3 instance for each string that declares a constant."
  },
  {
    "code": "@site_name.setter\ndef site_name(self, site_name):\n    if (site_name in SITE_LIST):\n        self.__site_name = site_name\n        self.__site_url = SITE_LIST[site_name]['url']\n        if ('api_version' and ('hashed_string' in SITE_LIST[site_name])):\n            self.api_version = SITE_LIST[site_name]['api_version']\n            self.hash_string = SITE_LIST[site_name]['hashed_string']\n    else:\n        raise PybooruError(\"The 'site_name' is not valid, specify a valid 'site_name'.\")",
    "nl": "Function that sets and checks the site name and set url.\n",
    "original_nl": "Function that sets and checks the site name and set url.\n\nParameters:\n    site_name (str): The site name in 'SITE_LIST', default sites.\n\nRaises:\n    PybooruError: When 'site_name' isn't valid."
  },
  {
    "code": "def find_cluster(fa_list, thr=0.5):\n    fa0 = fa_list[0]\n    fa0_group = [fa0]\n    fa_other = fa_list[1:]\n    for fa_o in fa_list[1:]:\n        tm_d = jchem.calc_tm_dist_int(fa0, fa_o)\n        if (tm_d > thr):\n            fa0_group.append(fa_o)\n            fa_other.remove(fa_o)\n    return (fa0_group, fa_other)",
    "nl": "find similar pattern with \nthe first element: fa0",
    "original_nl": "find similar pattern with \nthe first element: fa0"
  },
  {
    "code": "def read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()",
    "nl": "Return file content.",
    "original_nl": "Return file content."
  },
  {
    "code": "@mock.patch('know_me.profile.models.ProfileItem.has_object_read_permission')\ndef test_has_object_read_permission(mock_parent_permission, api_rf, list_entry_factory):\n    entry = list_entry_factory()\n    request = api_rf.get('/')\n    expected = mock_parent_permission.return_value\n    assert (entry.has_object_read_permission(request) == expected)\n    assert (mock_parent_permission.call_count == 1)\n    assert (mock_parent_permission.call_args[0] == (request,))",
    "nl": "List entries should delegate the read permission check to their\nparent profile item.",
    "original_nl": "List entries should delegate the read permission check to their\nparent profile item."
  },
  {
    "code": "def verification(url=None, path=None, lang='chi_sim', clean=False, engine='pytesseract'):\n    if (url is not None):\n        image = read_url_img(url, decode=True)\n    if (path is not None):\n        if os.path.exists(path):\n            try:\n                image = Image.open(path)\n            except:\n                traceback.print_exc()\n                return ''\n        else:\n            return ''\n    if clean:\n        img = np.array(image.convert('L'))\n        meanImg = img.mean()\n        img[(img > meanImg)] = 255\n        img[(img <= meanImg)] = 0\n        image = Image.fromarray(img)\n    if (engine == 'pytesseract'):\n        return tesseract(image, lang)\n    else:\n        return crnn(image)",
    "nl": "tesseract ocr chinses\\english verification\n@",
    "original_nl": "tesseract ocr chinses\\english verification\n@@param:url ,if url is not None,it will get image from url with function read_url_img\n@@oarm:path,if path is not None,it with get image from file\n@@param:lang,language choose in ['chi_sim','eng','chi_sim']\n@@param:clean,whether simple clean ths image\n@@ engine:pytesseract,crnn engine\n@@return :uft-8 string"
  },
  {
    "code": "def median1d(self, param, return_errors=False):\n    v = [self.mergers[m].median1d(param, return_errors=return_errors) for m in self.mergers]\n    if return_errors:\n        (value, merror, perror) = zip(*v)\n        return (numpy.array(value), numpy.array(merror), numpy.array(perror))\n    else:\n        return numpy.array(v)",
    "nl": "Return median 1d marginalized parameters\n",
    "original_nl": "Return median 1d marginalized parameters\n\nParameters\n----------\nname: str\n    The name of the parameter requested\nreturn_errors: Optional, {bool, False}\n    If true, return a second and third parameter that represents the\n    lower and upper 90% error on the parameter.\n\nReturns\n-------\nparam: nump.ndarray or tuple\n    The requested parameter"
  },
  {
    "code": "def test_031_role_match_regex(self):\n    q = ConstraintQuery(self.p, role='test31r.', role_regex=True)\n    constraint = sorted((c.tclass for c in q.results()))\n    self.assertListEqual(['test31a', 'test31b'], constraint)",
    "nl": "Constraint query with regex role match.",
    "original_nl": "Constraint query with regex role match."
  },
  {
    "code": "def generate_batch_pvdm(doc_ids, word_ids, batch_size, window_size):\n    data_index = 0\n    assert ((batch_size % window_size) == 0)\n    batch = np.ndarray(shape=(batch_size, (window_size + 1)), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = (window_size + 1)\n    buffer = collections.deque(maxlen=span)\n    buffer_doc = collections.deque(maxlen=span)\n    mask = ([1] * span)\n    mask[(- 1)] = 0\n    i = 0\n    doc_id = 0\n    while (data_index < len(word_ids)):\n        if ((len(set(buffer_doc)) == 1) and (len(buffer_doc) == span)):\n            doc_id = buffer_doc[(- 1)]\n            batch[i, :] = (list(compress(buffer, mask)) + [doc_id])\n            labels[(i, 0)] = buffer[(- 1)]\n            i += 1\n        buffer.append(word_ids[data_index])\n        buffer_doc.append(doc_ids[data_index])\n        data_index = (data_index + 1)\n        if (i == batch_size):\n            (yield (batch, labels, doc_id))\n            batch = np.ndarray(shape=(batch_size, (window_size + 1)), dtype=np.int32)\n            labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n            i = 0",
    "nl": "batch generator for PV-DM (Distribbuted Memory Model of Paragraph Vectors)\n",
    "original_nl": "batch generator for PV-DM (Distribbuted Memory Model of Paragraph Vectors)\n:param doc_ids: list of document indices\n:param word_ids: list of word indices\n:param batch_size: number of words in each mini-batch\n:param window_size: number of words before the target word\n:return: tuple of (batch, labels)"
  },
  {
    "code": "def get_room_id(self):\n    return self.lesson_details[self.lesson_id][2]",
    "nl": "Return the ID of the room Eric's will be expected to show up in at\nsome point during the current period, or else the name of the period\n(such as PLAYTIME).",
    "original_nl": "Return the ID of the room Eric's will be expected to show up in at\nsome point during the current period, or else the name of the period\n(such as PLAYTIME)."
  },
  {
    "code": "def convertColumns(self, columns):\n    new = []\n    for desc in columns:\n        if (not isinstance(desc, dict)):\n            desc = {\n                'column': desc,\n            }\n        if ('expression' in desc):\n            assert ('column' not in desc), ('You cannot provide both an expression and a column (for %s in index %s in %s)' % (desc, self.name, self.soClass))\n            assert ('length' not in desc), ('length does not apply to expressions (for %s in index %s in %s)' % (desc, self.name, self.soClass))\n            new.append(desc)\n            continue\n        columnName = desc['column']\n        if (not isinstance(columnName, str)):\n            columnName = columnName.name\n        colDict = self.soClass.sqlmeta.columns\n        if (columnName not in colDict):\n            for possible in colDict.values():\n                if (possible.origName == columnName):\n                    column = possible\n                    break\n            else:\n                raise ValueError(('The column by the name %r was not found in the class %r' % (columnName, self.soClass)))\n        else:\n            column = colDict[columnName]\n        desc['column'] = column\n        new.append(desc)\n    return new",
    "nl": "Converts all the columns to dictionary descriptors;\ndereferences string column names.",
    "original_nl": "Converts all the columns to dictionary descriptors;\ndereferences string column names."
  },
  {
    "code": "def Kill(self):\n    VimCommunicator.Kill(self)\n    if os.path.exists(self.args.servername):\n        os.remove(self.args.servername)",
    "nl": "Kills the Neovim process and removes the socket",
    "original_nl": "Kills the Neovim process and removes the socket"
  },
  {
    "code": "def render_widget(self, name, context, options):\n    dbsys = DashboardSystem(self.env)\n    params = ('layout', 'schema', 'show_captions', 'title')\n    (layout, schema, show_captions, title) = self.bind_params(name, options, *params)\n    lp = dbsys.resolve_layout(layout)\n    dbmod = DashboardModule(self.env)\n    layout_data = lp.expand_layout(layout, context, {\n        'schema': schema,\n        'embed': True,\n    })\n    widgets = dbmod.expand_widget_data(context, schema)\n    return (layout_data['template'], {\n        'title': title,\n        'data': dict(context=context, layout=schema, widgets=widgets, title='', default={\n            'height': (dbmod.default_widget_height or None),\n        }),\n    }, context)",
    "nl": "Count ocurrences of values assigned to given ticket field.",
    "original_nl": "Count ocurrences of values assigned to given ticket field."
  },
  {
    "code": "def test_get_raw_device_name_from_alias():\n    di = no_datastore_interface.NoDatastoreInterface()\n    assert (di.get_raw_device_name_from_alias(api_key, 'Alias') == '')",
    "nl": "This function checks if get_raw_device_name_from_alias returns nothing.",
    "original_nl": "This function checks if get_raw_device_name_from_alias returns nothing."
  },
  {
    "code": "def is_feature_enabled(course):\n    return EdxNotesTab.is_enabled(course)",
    "nl": "Returns True if Student Notes feature is enabled for the course, False otherwise.",
    "original_nl": "Returns True if Student Notes feature is enabled for the course, False otherwise."
  },
  {
    "code": "@property\ndef node_id(self):\n    return self._node_id",
    "nl": "The node ID of the device on the bus.",
    "original_nl": "The node ID of the device on the bus."
  },
  {
    "code": "@property\ndef loc_data(self):\n    return (json.loads(self.loc_payload) if self.loc_payload else None)",
    "nl": "The loc_data property is used to specify localization paramaters within the 'alert' aps key.\nhttps://developer.apple.com/library/ios/documentation/NetworkingInternet/Conceptual/RemoteNotificationsPG/Chapters/ApplePushService.html",
    "original_nl": "The loc_data property is used to specify localization paramaters within the 'alert' aps key.\nhttps://developer.apple.com/library/ios/documentation/NetworkingInternet/Conceptual/RemoteNotificationsPG/Chapters/ApplePushService.html"
  },
  {
    "code": "def show_password_dialog(self):\n    dlg = xbmcgui.Dialog()\n    dialog = dlg.input(heading=self.get_local_string(string_id=30004), type=xbmcgui.INPUT_ALPHANUM, option=xbmcgui.ALPHANUM_HIDE_INPUT)\n    return dialog",
    "nl": "Asks the user for its Netflix password\n\n",
    "original_nl": "Asks the user for its Netflix password\n\n:returns: str - Netflix password"
  },
  {
    "code": "def gametime_to_realtime(format=False, **kwargs):\n    rtime = 0\n    for (name, value) in kwargs.items():\n        if ((name not in UNITS) and name.endswith('s')):\n            name = name[:(- 1)]\n        if (name not in UNITS):\n            raise ValueError(\"the unit {} isn't defined as a valid game time unit\".format(name))\n        rtime += (value * UNITS[name])\n    rtime /= TIMEFACTOR\n    if format:\n        return time_to_tuple(rtime, 31536000, 2628000, 604800, 86400, 3600, 60)\n    return rtime",
    "nl": "This method helps to figure out the real-world time it will take until an\nin-game time has passed. E.g. if an event should take place a month later\nin-game, you will be able to find the number of real-world seconds this\ncorresponds to (hint: Interval events deal with real life seconds).\n\nKwargs:\n    format (bool): Formatting the output.\n    days, month etc (int): These are the names of time units that must\n        match the `settings.TIME_UNITS` dict keys.\n",
    "original_nl": "This method helps to figure out the real-world time it will take until an\nin-game time has passed. E.g. if an event should take place a month later\nin-game, you will be able to find the number of real-world seconds this\ncorresponds to (hint: Interval events deal with real life seconds).\n\nKwargs:\n    format (bool): Formatting the output.\n    days, month etc (int): These are the names of time units that must\n        match the `settings.TIME_UNITS` dict keys.\n\nReturns:\n    time (float or tuple): The realtime difference or the same\n        time split up into time units.\n\nExample:\n     gametime_to_realtime(days=2) -> number of seconds in real life from\n                    now after which 2 in-game days will have passed."
  },
  {
    "code": "def test_cleanedFailure(self):\n\n    def failing_func():\n        (1 / 0)\n    try:\n        failing_func()\n    except ZeroDivisionError:\n        failure = Failure()\n    failure.cleanFailure()\n    event = dict(log_format='Hi mom', who='me', log_failure=failure)\n    (records, output) = self.logEvent(event)\n    self.assertEqual(len(records), 1)\n    self.assertIn('Hi mom', output)\n    self.assertIn('in failing_func', output)\n    self.assertIn('ZeroDivisionError', output)",
    "nl": "A cleaned Failure object has a fake traceback object; make sure that\nlogging such a failure still results in the exception details being\nlogged.",
    "original_nl": "A cleaned Failure object has a fake traceback object; make sure that\nlogging such a failure still results in the exception details being\nlogged."
  },
  {
    "code": "def get_model():\n    (ch, row, col) = (3, 45, 160)\n    model = Sequential()\n    model.add(Lambda((lambda x: ((x / 255.0) - 0.5)), input_shape=(row, col, ch), output_shape=(row, col, ch)))\n    model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode='same'))\n    model.add(ELU())\n    model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode='same'))\n    model.add(ELU())\n    model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode='same'))\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(ELU())\n    model.add(Dense(512))\n    model.add(Dropout(0.5))\n    model.add(ELU())\n    model.add(Dense(1))\n    return model",
    "nl": "Get the model, this is a slight modification of comma.ai model",
    "original_nl": "Get the model, this is a slight modification of comma.ai model"
  },
  {
    "code": "def test_tcp4(self):\n    self.onePrefix('haproxy:tcp:8080', TCP4ServerEndpoint)",
    "nl": "Test if the parser generates a wrapped TCP4 endpoint.",
    "original_nl": "Test if the parser generates a wrapped TCP4 endpoint."
  },
  {
    "code": "def update_list(client, list_id, revision, title=None, public=None):\n    if (title is not None):\n        _check_title_length(title, client.api)\n    data = {\n        'revision': revision,\n        'title': title,\n        'public': public,\n    }\n    data = {key: value for (key, value) in data.iteritems() if (value is not None)}\n    endpoint = '/'.join([client.api.Endpoints.LISTS, str(list_id)])\n    response = client.authenticated_request(endpoint, 'PATCH', data=data)\n    return response.json()",
    "nl": "Updates the list with the given ID to have the given properties\n\nSee https://developer.wunderlist.com/documentation/endpoints/list for detailed parameter information",
    "original_nl": "Updates the list with the given ID to have the given properties\n\nSee https://developer.wunderlist.com/documentation/endpoints/list for detailed parameter information"
  },
  {
    "code": "def app_settings(request):\n    settings_dict = dict()\n    settings = dict()\n    for obj in Setting.objects.all():\n        settings[obj.name] = obj.value\n    settings_dict['settings'] = json.dumps(settings)\n    return settings_dict",
    "nl": "Global values to pass to templates",
    "original_nl": "Global values to pass to templates"
  },
  {
    "code": "@task\ndef install():\n    local('pip install -r requirements.txt')",
    "nl": "Install requirements packages",
    "original_nl": "Install requirements packages"
  },
  {
    "code": "def __init__(self, num_input_states, num_memory_states, num_output_states, random_genome_length=10000, seed_num_markov_gates=4, probabilistic=True, genome=None):\n    self.num_input_states = num_input_states\n    self.num_memory_states = num_memory_states\n    self.num_output_states = num_output_states\n    self.states = np.zeros(((num_input_states + num_memory_states) + num_output_states), dtype=np.bool)\n    self.markov_gates = []\n    self.markov_gate_input_ids = []\n    self.markov_gate_output_ids = []\n    if (genome is None):\n        self.genome = np.random.randint(0, 256, random_genome_length).astype(np.uint8)\n        for _ in range(seed_num_markov_gates):\n            start_index = np.random.randint(0, int((len(self.genome) * 0.8)))\n            self.genome[start_index] = 42\n            self.genome[(start_index + 1)] = 213\n    else:\n        self.genome = np.array(genome, dtype=np.uint8)\n    self._setup_markov_network(probabilistic)",
    "nl": "Sets up a Markov Network\n",
    "original_nl": "Sets up a Markov Network\n\nParameters\n----------\nnum_input_states: int\n    The number of input states in the Markov Network\nnum_memory_states: int\n    The number of internal memory states in the Markov Network\nnum_output_states: int\n    The number of output states in the Markov Network\nrandom_genome_length: int (default: 10000)\n    Length of the genome if it is being randomly generated\n    This parameter is ignored if \"genome\" is not None\nseed_num_markov_gates: int (default: 4)\n    The number of Markov Gates with which to seed the Markov Network\n    It is important to ensure that randomly-generated Markov Networks have at least a few Markov Gates to begin with\n    May sometimes result in fewer Markov Gates if the Markov Gates are randomly seeded in the same location\n    This parameter is ignored if \"genome\" is not None\nprobabilistic: bool (default: True)\n    Flag indicating whether the Markov Gates are probabilistic or deterministic\ngenome: array-like (default: None)\n    An array representation of the Markov Network to construct\n    All values in the array must be integers in the range [0, 255]\n    If None, then a random Markov Network will be generated\n\nReturns\n-------\nNone"
  },
  {
    "code": "def finalize_options(self):\n    _build_ext.finalize_options(self)\n    __builtins__.__NUMPY_SETUP__ = False\n    import numpy\n    self.include_dirs.append(numpy.get_include())\n    for extension in self.extensions:\n        extension.include_dirs = self.include_dirs\n    try:\n        from Cython.Build import cythonize\n        _needs_stub = [ext._needs_stub for ext in self.extensions]\n        self.extensions = cythonize(self.extensions)\n        for (ns, ext) in zip(_needs_stub, self.extensions):\n            ext._needs_stub = ns\n    except ImportError:\n        print('Cython not imported')\n        print('If C sources exist in the build directory, they will be used')\n        for ext in self.extensions:\n            ext.sources = [f.replace('.pyx', '.c') for f in ext.sources]\n    for ext in self.extensions:\n        for src in ext.sources:\n            if (not exists(src)):\n                raise Exception('Extension source code not found ({0})\\nCython required for install'.format(src))\n    return",
    "nl": "Unsets __NUMPY_SETUP__ so that the get_include function can be used",
    "original_nl": "Unsets __NUMPY_SETUP__ so that the get_include function can be used"
  },
  {
    "code": "def test_noZipFile(self):\n    with open('./test_data/test.txt', 'wb') as f:\n        f.write('test')\n    testcat.unzipFiles()\n    l = os.listdir('./test_out')\n    self.assertEqual(l, [])",
    "nl": "Test that an empty list is returned when there is no zip file in the input directory.",
    "original_nl": "Test that an empty list is returned when there is no zip file in the input directory."
  },
  {
    "code": "def check_get_datafile(self):\n    datafile = xml_Utils.getChildTextbyParentTag(self.filepath, 'Details', 'InputDataFile')\n    if ((datafile is None) or (datafile is False) or (str(datafile).strip() == '')):\n        if (self.filetype == 'tc'):\n            datafile = get_default_xml_datafile(self.filepath)\n        if (self.filetype == 'ts'):\n            datatype = self.check_get_datatype(False)\n            if str(datatype).lower().startswith('iterative'):\n                datafile = get_default_xml_datafile(self.filepath)\n            else:\n                datafile = False\n        elif (self.filetype == 'proj'):\n            datafile = False\n    elif (str(datafile).strip().upper() == 'DEFAULT'):\n        print_info('This testcase will be executed using the default InputDataFile')\n        datafile = get_default_xml_datafile(self.filepath)\n    elif (str(datafile).strip().upper() == 'NO_DATA'):\n        print_info('This test case will be run without any InputDataFile')\n        datafile = 'NO_DATA'\n    elif ((datafile is not None) and (datafile is not False)):\n        datafile_rel = str(datafile).strip()\n        datafile = file_Utils.getAbsPath(datafile_rel, os.path.dirname(self.filepath))\n    if ((str(datafile).strip().upper() != 'NO_DATA') and (datafile is not False)):\n        if (not file_Utils.fileExists(datafile)):\n            print_info('\\n')\n            print_error('!!! *** InputDataFile does not exist in provided path:{0} *** !!!'.format(datafile))\n    return datafile",
    "nl": "Check InputDatFile tag in the xml file and\nbased on the values return the datafile to be used for the testcase/testsuite\n    - If user provided a datafile, will use it.\n    - If user specified 'Default' will use the default datafile\n    - If user did not provide any value will use default datafile\n    - If user specified 'NODATA' will print a msg saying so.",
    "original_nl": "Check InputDatFile tag in the xml file and\nbased on the values return the datafile to be used for the testcase/testsuite\n    - If user provided a datafile, will use it.\n    - If user specified 'Default' will use the default datafile\n    - If user did not provide any value will use default datafile\n    - If user specified 'NODATA' will print a msg saying so."
  },
  {
    "code": "def test_mult_ds_area(self):\n    from satpy.composites import CompositeBase\n    ds1 = self._get_test_ds()\n    ds2 = self._get_test_ds()\n    comp = CompositeBase('test_comp')\n    ret_datasets = comp.check_areas((ds1, ds2))\n    self.assertIs(ret_datasets[0], ds1)\n    self.assertIs(ret_datasets[1], ds2)",
    "nl": "Test multiple datasets successfully pass.",
    "original_nl": "Test multiple datasets successfully pass."
  },
  {
    "code": "def adjust_recursive_directory_permissions(pre_existing_dir, new_directory_list, module, directory_args, changed):\n    if (len(new_directory_list) > 0):\n        working_dir = os.path.join(pre_existing_dir, new_directory_list.pop(0))\n        directory_args['path'] = working_dir\n        changed = module.set_fs_attributes_if_different(directory_args, changed)\n        changed = adjust_recursive_directory_permissions(working_dir, new_directory_list, module, directory_args, changed)\n    return changed",
    "nl": "Walk the new directories list and make sure that permissions are as we would expect",
    "original_nl": "Walk the new directories list and make sure that permissions are as we would expect"
  },
  {
    "code": "def num_unique_categories(venue_collection):\n    unique_categories = set()\n    venues = venue_collection.find()\n    for venue in venues:\n        unique_categories.add(venue_primary_category_extractor(venue)[1][0])\n    return unique_categories",
    "nl": "Counts total number of unique categories.\n",
    "original_nl": "Counts total number of unique categories.\n:param venue_collection: MongoDB collection that contains the venues\n:return: a set of containing unique categories"
  },
  {
    "code": "def biselect(table, *args, **kwargs):\n    kwargs['complement'] = False\n    t1 = select(table, *args, **kwargs)\n    kwargs['complement'] = True\n    t2 = select(table, *args, **kwargs)\n    return (t1, t2)",
    "nl": "Return two tables, the first containing selected rows, the second\ncontaining remaining rows. E.g.::\n\n    >>> import petl as etl\n    >>> table1 = [['foo', 'bar', 'baz'],\n    ...           ['a', 4, 9.3],\n    ...           ['a', 2, 88.2],\n    ...           ['b', 1, 23.3],\n    ...           ['c', 8, 42.0],\n    ...           ['d', 7, 100.9],\n    ...           ['c', 2]]\n    >>> table2, table3 = etl.biselect(table1, lambda rec: rec.foo == 'a')\n    >>> table2\n    +-----+-----+------+\n    | foo | bar | baz  |\n    +=====+=====+======+\n    | 'a' |   4 |  9.3 |\n    +-----+-----+------+\n    | 'a' |   2 | 88.2 |\n    +-----+-----+------+\n    >>> table3\n    +-----+-----+-------+\n    | foo | bar | baz   |\n    +=====+=====+=======+\n    | 'b' |   1 |  23.3 |\n    +-----+-----+-------+\n    | 'c' |   8 |  42.0 |\n    +-----+-----+-------+\n    | 'd' |   7 | 100.9 |\n    +-----+-----+-------+\n    | 'c' |   2 |       |\n    +-----+-----+-------+\n\n.. versionadded:: 1.1.0",
    "original_nl": "Return two tables, the first containing selected rows, the second\ncontaining remaining rows. E.g.::\n\n    >>> import petl as etl\n    >>> table1 = [['foo', 'bar', 'baz'],\n    ...           ['a', 4, 9.3],\n    ...           ['a', 2, 88.2],\n    ...           ['b', 1, 23.3],\n    ...           ['c', 8, 42.0],\n    ...           ['d', 7, 100.9],\n    ...           ['c', 2]]\n    >>> table2, table3 = etl.biselect(table1, lambda rec: rec.foo == 'a')\n    >>> table2\n    +-----+-----+------+\n    | foo | bar | baz  |\n    +=====+=====+======+\n    | 'a' |   4 |  9.3 |\n    +-----+-----+------+\n    | 'a' |   2 | 88.2 |\n    +-----+-----+------+\n    >>> table3\n    +-----+-----+-------+\n    | foo | bar | baz   |\n    +=====+=====+=======+\n    | 'b' |   1 |  23.3 |\n    +-----+-----+-------+\n    | 'c' |   8 |  42.0 |\n    +-----+-----+-------+\n    | 'd' |   7 | 100.9 |\n    +-----+-----+-------+\n    | 'c' |   2 |       |\n    +-----+-----+-------+\n\n.. versionadded:: 1.1.0"
  },
  {
    "code": "def get_path_to_toplevel_modules(filename):\n    curr_dir = os.path.dirname(os.path.abspath(filename))\n    pattern = '__init__.py'\n    try:\n        for i in range(10):\n            files = set(os.listdir(curr_dir))\n            if (pattern in files):\n                curr_dir = os.path.dirname(curr_dir)\n            else:\n                return curr_dir\n    except IOError:\n        pass\n    return None",
    "nl": "Return the path to top-level directory that contains Python modules.\n\nIt will look in parent directories for __init__.py files. The first parent\ndirectory without __init__.py is the top-level directory.\n",
    "original_nl": "Return the path to top-level directory that contains Python modules.\n\nIt will look in parent directories for __init__.py files. The first parent\ndirectory without __init__.py is the top-level directory.\n\nReturned directory might be used to extend the PYTHONPATH."
  },
  {
    "code": "@property\ndef accountable_date(self):\n    fecha_transaccion = self.data['TBK_FECHA_CONTABLE']\n    m = int(fecha_transaccion[:2])\n    d = int(fecha_transaccion[2:])\n    santiago = pytz.timezone('America/Santiago')\n    today = santiago.localize(datetime.datetime.today())\n    year = today.year\n    if ((self.paid_at.month == 12) and (m == 1)):\n        year += 1\n    santiago_dt = santiago.localize(datetime.datetime(year, m, d))\n    return santiago_dt",
    "nl": "Accountable date of transaction, localized as America/Santiago",
    "original_nl": "Accountable date of transaction, localized as America/Santiago"
  },
  {
    "code": "def updateControlItem(self, cgi):\n    s = self.settings\n    pointsX = list(s.xPos)\n    pointsY = list(s.yPos)\n    ind = cgi.index\n    (xpos, ypos) = self._getGraphCoords(cgi.widgetposn, (cgi.deltacrosspos[0] + cgi.posn[0]), (cgi.deltacrosspos[1] + cgi.posn[1]))\n    (xposd, yposd) = self._getGraphCoords(cgi.widgetposn, ((cgi.deltacrosspos[0] + cgi.posn[0]) + 1), ((cgi.deltacrosspos[1] + cgi.posn[1]) + 1))\n    if ((xpos is None) or (ypos is None)):\n        return\n    roundx = utils.round2delt(xpos, xposd)\n    roundy = utils.round2delt(ypos, yposd)\n    (pointsX[ind], pointsY[ind]) = (roundx, roundy)\n    operations = (document.OperationSettingSet(s.get('xPos'), pointsX), document.OperationSettingSet(s.get('yPos'), pointsY))\n    self.document.applyOperation(document.OperationMultiple(operations, descr=_('move label')))",
    "nl": "Update position of point given new name and vals.",
    "original_nl": "Update position of point given new name and vals."
  },
  {
    "code": "@classmethod\ndef sort_menus(c):\n    for name in c.items:\n        if (not c.sorted[name]):\n            c.items[name].sort(key=(lambda x: x.weight))\n            c.sorted[name] = True",
    "nl": "sort_menus goes through the items and sorts them based on\ntheir weight",
    "original_nl": "sort_menus goes through the items and sorts them based on\ntheir weight"
  },
  {
    "code": "def test_default_config(self):\n    cfg_manager = ConfigurationManager.ConfigurationManager()\n    vmexp = VMExperiment.VMExperiment(None, None, cfg_manager)\n    self.assertNotEqual(vmexp.url, None)\n    self.assertTrue(vmexp.url.__contains__('localhost'))\n    self.assertTrue(vmexp.should_store_image, True)\n    self.assertEqual(vmexp.vm_type, 'VirtualMachineDummy')\n    self.assertEqual(vmexp.user_manager_type, 'DummyUserManager')\n    vm = vmexp.vm\n    vmexp.load_user_manager()\n    um = vmexp.user_manager\n    self.assertIs(type(vm), VirtualMachineDummy.VirtualMachineDummy)\n    self.assertIs(type(um), user_manager.DummyUserManager.DummyUserManager)\n    self.assertFalse(vmexp.is_ready)\n    self.assertFalse(vmexp.is_error)",
    "nl": "Tests that the ctor works and sets proper defaults with a blank configuration manager.",
    "original_nl": "Tests that the ctor works and sets proper defaults with a blank configuration manager."
  },
  {
    "code": "@mock.patch('django.core.files.storage.Storage.save')\ndef test_check_fails(self, m_mock):\n    m_mock.side_effect = Exception('Boom')\n    informer = StorageInformer()\n    self.assertRaises(InformerException, informer.check_availability)",
    "nl": "Test if with 'broken scenario', all goes bad",
    "original_nl": "Test if with 'broken scenario', all goes bad"
  },
  {
    "code": "def create_def_exec_dir(self):\n    if (self.ws_execution is None):\n        execdir = self.create_ws_execution()\n    else:\n        execdir = self.ws_execution\n    return execdir",
    "nl": "Create the default result execution directory",
    "original_nl": "Create the default result execution directory"
  },
  {
    "code": "def _has_undefined_space():\n    return VLAN.objects.filter(space__isnull=True).exists()",
    "nl": "Returns True if the undefined space contains at least one VLAN.",
    "original_nl": "Returns True if the undefined space contains at least one VLAN."
  },
  {
    "code": "def start(self):\n    assert (not self._sock_service)\n    service = Gio.SocketService.new()\n    try:\n        service.add_inet_port(self._port, None)\n    except GLib.GError as e:\n        raise ServerError(e)\n    except OverflowError as e:\n        raise ServerError(('port: %s' % e))\n    self._id = service.connect('incoming', self._incoming_connection_cb)\n    service.start()\n    self._sock_service = service",
    "nl": "Start accepting connections.\n\nMay raise ServerError.",
    "original_nl": "Start accepting connections.\n\nMay raise ServerError."
  },
  {
    "code": "def get_meta(self, table_name, constraints, column_to_field_name):\n    unique_together = []\n    for (index, params) in constraints.items():\n        if params['unique']:\n            columns = params['columns']\n            if (len(columns) > 1):\n                tup = (('(' + ', '.join(((\"'%s'\" % column_to_field_name[c]) for c in columns))) + ')')\n                unique_together.append(tup)\n    meta = ['', '    class Meta:', '        managed = False', (\"        db_table = '%s'\" % table_name)]\n    if unique_together:\n        tup = (('(' + ', '.join(unique_together)) + ',)')\n        meta += [('        unique_together = %s' % tup)]\n    return meta",
    "nl": "Return a sequence comprising the lines of code necessary\nto construct the inner Meta class for the model corresponding\nto the given database table name.",
    "original_nl": "Return a sequence comprising the lines of code necessary\nto construct the inner Meta class for the model corresponding\nto the given database table name."
  },
  {
    "code": "def insideout(ds):\n    ds = list(ds)\n    result = dict([(k, []) for k in ds[0].keys()])\n    for d in ds:\n        for (k, v) in d.items():\n            result[k].append(v)\n    return result",
    "nl": "Transform a list of dictionaries to a dictionary of lists.",
    "original_nl": "Transform a list of dictionaries to a dictionary of lists."
  },
  {
    "code": "def _get_from_context(self, ctx):\n    return (ctx['active_model'], ctx['active_ids'])",
    "nl": "Gets the active_model and active_ids attributes from context\n\n",
    "original_nl": "Gets the active_model and active_ids attributes from context\n\n:param ctx (dict): context that will be processed\n:return: active_model, active_ids"
  },
  {
    "code": "def add_requested_columns(args, update_cursor, col_names, col_types=None):\n    if (args.anno_type in ['count', 'boolean']):\n        col_name = col_names[0]\n        col_type = 'integer'\n        try:\n            alter_qry = ((((('ALTER TABLE variants ADD COLUMN ' + col_name) + ' ') + col_type) + ' ') + 'DEFAULT NULL')\n            update_cursor.execute(sql.text(alter_qry))\n        except sql.exc.OperationalError:\n            sys.stderr.write((('WARNING: Column \"(' + col_name) + ')\" already exists in variants table. Overwriting values.\\n'))\n            update_cursor.execute((('UPDATE variants SET ' + col_name) + ' = NULL WHERE 1'))\n    elif (args.anno_type == 'extract'):\n        for (col_name, col_type) in zip(col_names, col_types):\n            try:\n                alter_qry = ((((('ALTER TABLE variants ADD COLUMN ' + col_name) + ' ') + col_type) + ' ') + 'DEFAULT NULL')\n                update_cursor.execute(alter_qry)\n            except sql.exc.OperationalError:\n                sys.stderr.write((('WARNING: Column \"(' + col_name) + ')\" already exists in variants table. Overwriting values.\\n'))\n    else:\n        raise ValueError(('Unknown annotation type: %s\\n' % args.anno_type))",
    "nl": "Attempt to add new, user-defined columns to the\nvariants table.  Warn if the column already exists.",
    "original_nl": "Attempt to add new, user-defined columns to the\nvariants table.  Warn if the column already exists."
  },
  {
    "code": "def __init__(self, column_names=None, title=None):\n    super(BaseTableView, self).__init__()\n    self._columns = (column_names or [])\n    self._number_of_columns = len(self._columns)\n    self._rows = []\n    self._title = title",
    "nl": "Initializes a table view.\n\nArgs:\n  column_names (Optional[list[str]]): column names.\n  title (Optional[str]): title.",
    "original_nl": "Initializes a table view.\n\nArgs:\n  column_names (Optional[list[str]]): column names.\n  title (Optional[str]): title."
  },
  {
    "code": "def balance(slack_user_id, synapse_user, params):\n    user = User.from_slack_id(slack_user_id)\n    savings_node = Node.by_id(user=synapse_user, id=user.savings_node_id)\n    if savings_node:\n        balance = format_currency(savings_node.balance)\n        return '```Savings: {0}```'.format(balance)\n    else:\n        return '*No savings node found for user.*'",
    "nl": "Return balance on SYNAPSE-US savings account.",
    "original_nl": "Return balance on SYNAPSE-US savings account."
  },
  {
    "code": "def get_start_index_of_section(self, start_content):\n    matches = list(self.bracket_expression.finditer(start_content))\n    if (len(matches) > 0):\n        last_match = matches[(- 1)]\n        return last_match.start()\n    else:\n        return 0",
    "nl": "Return the index of the section.\n\nIf no section is found the first index (0) is returned",
    "original_nl": "Return the index of the section.\n\nIf no section is found the first index (0) is returned"
  },
  {
    "code": "def write_to_cache(self, data, filename):\n    json_data = self.json_format_dict(data, True)\n    cache = open(filename, 'w')\n    cache.write(json_data)\n    cache.close()",
    "nl": "Writes data in JSON format to a file",
    "original_nl": "Writes data in JSON format to a file"
  },
  {
    "code": "def _create_template(self, num_instances, num_replace=0, template_version=('heat_template_version', '2015-04-30')):\n    return super(AutoScalingResourceGroup, self)._create_template(num_instances, num_replace, template_version=template_version)",
    "nl": "Create a template in the HOT format for the nested stack.",
    "original_nl": "Create a template in the HOT format for the nested stack."
  },
  {
    "code": "def vertical_table(data, headers, sep_title='{n}. row', sep_character='*', sep_length=27):\n    header_len = max([len(x) for x in headers])\n    padded_headers = [x.ljust(header_len) for x in headers]\n    formatted_rows = [_format_row(padded_headers, row) for row in data]\n    output = []\n    for (i, result) in enumerate(formatted_rows):\n        (yield (_get_separator(i, sep_title, sep_character, sep_length) + result))",
    "nl": "Format *data* and *headers* as an vertical table.\n\nThe values in *data* and *headers* must be strings.\n\n",
    "original_nl": "Format *data* and *headers* as an vertical table.\n\nThe values in *data* and *headers* must be strings.\n\n:param iterable data: An :term:`iterable` (e.g. list) of rows.\n:param iterable headers: The column headers.\n:param str sep_title: The title given to each row separator. Defaults to\n                      ``'{n}. row'``. Any instance of ``'{n}'`` is\n                      replaced by the record number.\n:param str sep_character: The character used to separate rows. Defaults to\n                          ``'*'``.\n:param int/tuple sep_length: The number of separator characters that should\n                             appear on each side of the *sep_title*. Use\n                             a tuple to specify the left and right values\n                             separately.\n:return: The formatted data.\n:rtype: str"
  },
  {
    "code": "def split_pow_tgh(self, text):\n    return [n for n in re.split('/(?=([^{}]*{[^{}]*})*[^{}]*$)', text) if (n is not None)][:2]",
    "nl": "Split a power/toughness string on the correct slash.\n\nCorrectly accounts for curly braces to denote fractions.\nE.g., '2/2' --> ['2', '2']\n'3{1/2}/3{1/2}' --> ['3{1/2}', '3{1/2}']",
    "original_nl": "Split a power/toughness string on the correct slash.\n\nCorrectly accounts for curly braces to denote fractions.\nE.g., '2/2' --> ['2', '2']\n'3{1/2}/3{1/2}' --> ['3{1/2}', '3{1/2}']"
  },
  {
    "code": "def test_array_coordinates_distances():\n    ICRS(ra=(np.array([1, 2]) * u.deg), dec=(np.array([3, 4]) * u.deg), distance=([0.1, 0.2] * u.kpc))\n    with pytest.raises(ValueError):\n        ICRS(ra=(np.array([1, 2, 3]) * u.deg), dec=(np.array([[3, 4], [5, 6]]) * u.deg), distance=(2.0 * u.kpc))\n    with pytest.raises(ValueError):\n        ICRS(ra=(np.array([1, 2]) * u.deg), dec=(np.array([3, 4]) * u.deg), distance=([0.1, 0.2, 3.0] * u.kpc))",
    "nl": "Test creating coordinates from arrays and distances.",
    "original_nl": "Test creating coordinates from arrays and distances."
  },
  {
    "code": "def post(self, request, bot_id, format=None):\n    return super(MessengerChatStateList, self).post(request, bot_id, format)",
    "nl": "Add a new chat state\n---\nserializer: MessengerChatStateSerializer\nresponseMessages:\n    - code: 401\n      message: Not authenticated\n    - code: 400\n      message: Not valid request",
    "original_nl": "Add a new chat state\n---\nserializer: MessengerChatStateSerializer\nresponseMessages:\n    - code: 401\n      message: Not authenticated\n    - code: 400\n      message: Not valid request"
  },
  {
    "code": "def _get_tags_used_in_content(app_label=None, model=None):\n    tag_parser = HTMLTagParser()\n    queryset = FieldSanitizer.objects.all()\n    if (app_label and model):\n        queryset = queryset.filter(content_type__app_label=app_label, content_type__model=model)\n    for fs in queryset:\n        model_class = fs.content_type.model_class()\n        for content in model_class.objects.values_list(fs.field_name, flat=True):\n            tag_parser.feed(content)\n    return sorted(list(tag_parser.tags))",
    "nl": "Use html5lib's parser to get a list of HTML tags used in content\nassociated with a FieldSanitizer.\n\nThis can be useful when determining what to include in a whitelist,\nand is used in the `list_html_elements` and\n`list_html_elements_for_model` management commands.",
    "original_nl": "Use html5lib's parser to get a list of HTML tags used in content\nassociated with a FieldSanitizer.\n\nThis can be useful when determining what to include in a whitelist,\nand is used in the `list_html_elements` and\n`list_html_elements_for_model` management commands."
  },
  {
    "code": "def show(tournament, match, attachment):\n    return api.fetch_and_parse('GET', ('tournaments/%s/matches/%s/attachments/%s' % (tournament, match, attachment)))",
    "nl": "Retrieve a single match attachment record.",
    "original_nl": "Retrieve a single match attachment record."
  },
  {
    "code": "def add_model_score_header(head):\n    add_metadata(head, 'info', 'ModelScore', annotation_number='1', entry_type='Integer', description='PHRED score for genotype models.')\n    return",
    "nl": "Add Model Score to vcf header",
    "original_nl": "Add Model Score to vcf header"
  },
  {
    "code": "def get_view_url(self):\n    return self.get_categorization_object().get_absolute_url()",
    "nl": "Return object view url. Used in `get_translated_url` templatetag from parler.",
    "original_nl": "Return object view url. Used in `get_translated_url` templatetag from parler."
  },
  {
    "code": "def write(file_or_filename, pymm_element):\n    if (not isinstance(pymm_element, element.BaseElement)):\n        raise ValueError('pymm.write requires file/filename, then pymm element')\n    et_elem = encode(pymm_element)\n    xmltree = ET.ElementTree(et_elem)\n    xmltree.write(file_or_filename)",
    "nl": "Writes mindmap/element to file. Element must be pymm element.\nWill write element and children hierarchy to file.\nWriting any element to file works, but in order to be opened\nin Freeplane, the Mindmap element should be passed.\n\n",
    "original_nl": "Writes mindmap/element to file. Element must be pymm element.\nWill write element and children hierarchy to file.\nWriting any element to file works, but in order to be opened\nin Freeplane, the Mindmap element should be passed.\n\n:param mm_element: Mindmap or other pymm element\n:param file_or_filename: string path to file or file instance\n    of mindmap (.mm)\n:return:"
  },
  {
    "code": "def render_sparkline(self, **kwargs):\n    spark_options = dict(width=200, height=50, show_dots=False, show_legend=False, show_x_labels=False, show_y_labels=False, spacing=0, margin=5, min_scale=1, max_scale=2, explicit_size=True, no_data_text='', js=(), classes=(_ellipsis, 'pygal-sparkline'))\n    spark_options.update(kwargs)\n    return self.render(**spark_options)",
    "nl": "Render a sparkline",
    "original_nl": "Render a sparkline"
  },
  {
    "code": "def __repr__(self) -> str:\n    return self.to_json()",
    "nl": "Get the debug representation of the robot model.\n\n",
    "original_nl": "Get the debug representation of the robot model.\n\n:return:"
  },
  {
    "code": "def _get_anchor_data(anchor_data, ufo, components, anchor_name):\n    anchors = []\n    for component in components:\n        for anchor in ufo[component.baseGlyph].anchors:\n            if (anchor.name == anchor_name):\n                anchors.append((anchor, component))\n                break\n    if (len(anchors) > 1):\n        for (i, (anchor, component)) in enumerate(anchors):\n            t = Transform(*component.transformation)\n            name = ('%s_%d' % (anchor.name, (i + 1)))\n            anchor_data[name] = t.transformPoint((anchor.x, anchor.y))\n    elif anchors:\n        (anchor, component) = anchors[0]\n        t = Transform(*component.transformation)\n        anchor_data[anchor.name] = t.transformPoint((anchor.x, anchor.y))",
    "nl": "Get data for an anchor from a list of components.",
    "original_nl": "Get data for an anchor from a list of components."
  },
  {
    "code": "def testParseOptions(self):\n    options = cli_test_lib.TestOptions()\n    options.filter = 'event.timestamp == 0'\n    test_tool = tools.CLITool()\n    event_filters.EventFiltersArgumentsHelper.ParseOptions(options, test_tool)\n    self.assertEqual(test_tool._event_filter_expression, options.filter)\n    self.assertIsNotNone(test_tool._event_filter)\n    with self.assertRaises(errors.BadConfigObject):\n        event_filters.EventFiltersArgumentsHelper.ParseOptions(options, None)",
    "nl": "Tests the ParseOptions function.",
    "original_nl": "Tests the ParseOptions function."
  },
  {
    "code": "def getCubicPathByBeginEnd(begin, controlPoints, elementNode, end):\n    return svg_reader.getCubicPoints(begin, controlPoints, end, lineation.getNumberOfBezierPoints(begin, elementNode, end))",
    "nl": "Get the cubic path by begin and end.",
    "original_nl": "Get the cubic path by begin and end."
  },
  {
    "code": "def SetNodesInfos(self, values):\n    ida_idaapi.pygc_set_nodes_infos(self, values)",
    "nl": "Set the properties for the given nodes.\n\nExample usage (set first three nodes's bg color to purple):\n  inst = ...\n  p = idaapi.node_info_t()\n  p.bg_color = 0x00ff00ff\n  inst.SetNodesInfos({0 : p, 1 : p, 2 : p})\n\n",
    "original_nl": "Set the properties for the given nodes.\n\nExample usage (set first three nodes's bg color to purple):\n  inst = ...\n  p = idaapi.node_info_t()\n  p.bg_color = 0x00ff00ff\n  inst.SetNodesInfos({0 : p, 1 : p, 2 : p})\n\n@param values: A dictionary of 'int -> node_info_t' objects."
  },
  {
    "code": "def readTempC(self):\n    v = self._read32()\n    if (v & 7):\n        return float('NaN')\n    if (v & 2147483648):\n        v >>= 18\n        v -= 16384\n    else:\n        v >>= 18\n    return (v * 0.25)",
    "nl": "Return the thermocouple temperature value in degrees celsius.",
    "original_nl": "Return the thermocouple temperature value in degrees celsius."
  },
  {
    "code": "def bearing_to(lat1, lon1, lat2, lon2):\n    (lat1, lon1, lat2, lon2) = map(math.radians, [lat1, lon1, lat2, lon2])\n    dLon = (lon2 - lon1)\n    y = (math.sin(dLon) * math.cos(lat2))\n    x = ((math.cos(lat1) * math.sin(lat2)) - ((math.sin(lat1) * math.cos(lat2)) * math.cos(dLon)))\n    return math.degrees(math.atan2(y, x))",
    "nl": "Computes bearing between the current point and the heading point.\n\nInput angles and the output bearing are in degrees. Bearing\nis 0.0 when we are facing north, +90.0 when facing east,\n-90.0 when facing west, +/-180.0 when facing south.\n\nArgs:\n    lat1: A float, latitude of the current point.\n    lon1: A float, longitude of the current point.\n    lat2: A float, latitude of the heading to point.\n    lon2: A float, latitude of the heading to point.\n",
    "original_nl": "Computes bearing between the current point and the heading point.\n\nInput angles and the output bearing are in degrees. Bearing\nis 0.0 when we are facing north, +90.0 when facing east,\n-90.0 when facing west, +/-180.0 when facing south.\n\nArgs:\n    lat1: A float, latitude of the current point.\n    lon1: A float, longitude of the current point.\n    lat2: A float, latitude of the heading to point.\n    lon2: A float, latitude of the heading to point.\n\nReturns:\n    A float, the heading (north = 0.0)."
  },
  {
    "code": "def tearDown(self):\n    self.session_patcher.stop()\n    self.mock_session_cls = None\n    self.mock_session = None\n    self.client = None",
    "nl": "Stop the patcher",
    "original_nl": "Stop the patcher"
  },
  {
    "code": "def reproject(*args, **kwargs):\n    if (len(args) == 1):\n        C = np.asanyarray(args[0])\n        cshape = C.shape\n        numCols = C.shape[(- 1)]\n        C = C.reshape((- 1), numCols)\n        if ((numCols < 2) or (numCols > 3)):\n            raise TypeError(('Input Array column mismatch to %s' % 'reproject'))\n    else:\n        if (len(args) == 2):\n            (X, Y) = (np.asanyarray(arg) for arg in args)\n            numCols = 2\n        elif (len(args) == 3):\n            (X, Y, Z) = (np.asanyarray(arg) for arg in args)\n            zshape = Z.shape\n            numCols = 3\n        else:\n            raise TypeError(('Illegal arguments to %s' % 'reproject'))\n        xshape = X.shape\n        yshape = Y.shape\n        if (xshape != yshape):\n            raise TypeError(('Incompatible X, Y inputs to %s' % 'reproject'))\n        if ('Z' in locals()):\n            if (xshape != zshape):\n                raise TypeError(('Incompatible Z input to %s' % 'reproject'))\n            C = np.concatenate([X.ravel()[:, None], Y.ravel()[:, None], Z.ravel()[:, None]], axis=1)\n        else:\n            C = np.concatenate([X.ravel()[:, None], Y.ravel()[:, None]], axis=1)\n    projection_source = kwargs.get('projection_source', get_default_projection())\n    projection_target = kwargs.get('projection_target', get_default_projection())\n    ct = osr.CoordinateTransformation(projection_source, projection_target)\n    trans = np.array(ct.TransformPoints(C))\n    if (len(args) == 1):\n        trans = trans[:, 0:numCols].reshape(cshape)\n        return trans\n    else:\n        X = trans[:, 0].reshape(xshape)\n        Y = trans[:, 1].reshape(yshape)\n        if (len(args) == 2):\n            return (X, Y)\n        if (len(args) == 3):\n            Z = trans[:, 2].reshape(zshape)\n            return (X, Y, Z)",
    "nl": "Transform coordinates from a source projection to a target projection.\n\nCall signatures::\n\n    reproject(C, **kwargs)\n    reproject(X, Y, **kwargs)\n    reproject(X, Y, Z, **kwargs)\n\n*C* is the np array of source coordinates.\n*X*, *Y* and *Z* specify arrays of x, y, and z coordinate values\n",
    "original_nl": "Transform coordinates from a source projection to a target projection.\n\nCall signatures::\n\n    reproject(C, **kwargs)\n    reproject(X, Y, **kwargs)\n    reproject(X, Y, Z, **kwargs)\n\n*C* is the np array of source coordinates.\n*X*, *Y* and *Z* specify arrays of x, y, and z coordinate values\n\nParameters\n----------\nC : multidimensional :class:`numpy:numpy.ndarray`\n    Array of shape (...,2) or (...,3) with coordinates (x,y) or (x,y,z)\n    respectively\nX : :class:`numpy:numpy.ndarray`\n    Array of x coordinates\nY : :class:`numpy:numpy.ndarray`\n    Array of y coordinates\nZ : :class:`numpy:numpy.ndarray`\n    Array of z coordinates\n\nKeyword Arguments\n-----------------\nprojection_source : osr object\n    defaults to EPSG(4326)\nprojection_target : osr object\n    defaults to EPSG(4326)\n\nReturns\n-------\ntrans : :class:`numpy:numpy.ndarray`\n    Array of reprojected coordinates x,y (...,2) or x,y,z (...,3)\n    depending on input array.\nX, Y : :class:`numpy:numpy.ndarray`\n    Arrays of reprojected x,y coordinates, shape depending on input array\nX, Y, Z: :class:`numpy:numpy.ndarray`\n    Arrays of reprojected x,y,z coordinates, shape depending on input array\n\nExamples\n--------\n\nSee :ref:`/notebooks/georeferencing/wradlib_georef_example.ipynb`."
  },
  {
    "code": "def get_brands_with_person_counts() -> Iterator[Tuple[(Brand, int)]]:\n    brands = Brand.query.all()\n    person_counts_by_brand_id = get_person_count_by_brand_id()\n    for brand in brands:\n        person_count = person_counts_by_brand_id[brand.id]\n        (yield (brand, person_count))",
    "nl": "Yield (brand, person count) pairs.",
    "original_nl": "Yield (brand, person count) pairs."
  },
  {
    "code": "def switch_cursor(cursor_type, parent_window):\n    watch = Gdk.Cursor(cursor_type)\n    window = parent_window.get_root_window()\n    window.set_cursor(watch)",
    "nl": "Functions switches the cursor to cursor type",
    "original_nl": "Functions switches the cursor to cursor type"
  },
  {
    "code": "def assemble_sequence_from_recipe(self, recipe, template_name, order):\n    gcodes = []\n    template = self.machine_profile.values[template_name]\n    for routine in order:\n        if (recipe[routine] is not None):\n            gcodes.extend(template[routine][recipe[routine]])\n    return gcodes",
    "nl": "Given a recipe, template_name and ordering creates the correct\nsequence.\n\n",
    "original_nl": "Given a recipe, template_name and ordering creates the correct\nsequence.\n\n@param recipe: The recipe used to create the sequence\n@param template_name: The name of the template we want to use (start/end)\n@param order: The correct ordering of routines\n\n@return list gcodes: Sequence of gcodes derived from the recipe."
  },
  {
    "code": "def parse_requirements(requirements, ignore=('setuptools',)):\n    with open(requirements) as f:\n        packages = set()\n        for line in f:\n            line = line.strip()\n            if line.startswith(('#', '-r', '--')):\n                continue\n            if ('#egg=' in line):\n                line = line.split('#egg=')[1]\n            pkg = line.strip()\n            if (pkg not in ignore):\n                packages.add(pkg)\n        return packages",
    "nl": "Read dependencies from requirements file (with version numbers if any)\n\nNotes:\n    - this implementation does not support requirements files with extra\n      requirements\n    - this implementation has been taken from TailorDev/Watson's setup file",
    "original_nl": "Read dependencies from requirements file (with version numbers if any)\n\nNotes:\n    - this implementation does not support requirements files with extra\n      requirements\n    - this implementation has been taken from TailorDev/Watson's setup file"
  },
  {
    "code": "def init():\n    if (CUSTOM_ICONS not in _preview_collections):\n        pcoll = previews.new()\n        _preview_collections[CUSTOM_ICONS] = pcoll\n    else:\n        pcoll = _preview_collections[CUSTOM_ICONS]\n        print('INFO\\t - Icon collection is already in python memory, re-using it!')\n    tools_paths = _path.get_addon_installation_paths()\n    if (len(tools_paths) > 0):\n        for icon_type in _ICON_consts.Types.as_list():\n            icon_path = os.path.join(tools_paths[0], (((('ui' + os.sep) + 'icons') + os.sep) + icon_type))\n            if os.path.isfile(icon_path):\n                if (icon_type not in pcoll):\n                    pcoll.load(icon_type, icon_path, 'IMAGE', force_reload=True)\n            else:\n                lprint('W Icon %r is missing. Please try to install addon again!', (icon_type,))",
    "nl": "Initialization function for getting hold of preview collection variable with already created custom icon objects.",
    "original_nl": "Initialization function for getting hold of preview collection variable with already created custom icon objects."
  },
  {
    "code": "def flatten(*args):\n    for arg in args:\n        if (isinstance(arg, collections.Iterable) and (not isinstance(arg, (str, bytes)))):\n            (yield from flatten(*arg))\n        else:\n            (yield arg)",
    "nl": "Generator that recursively flattens embedded lists, tuples, etc.",
    "original_nl": "Generator that recursively flattens embedded lists, tuples, etc."
  },
  {
    "code": "def subs(d, **kwargs):\n    if d:\n        return top_down(do_one(*map(rl.subs, *zip(*d.items()))), **kwargs)\n    else:\n        return (lambda x: x)",
    "nl": "Full simultaneous exact substitution\n\nExamples\n========\n\n>>> from sympy.strategies.tools import subs\n>>> from sympy import Basic\n>>> mapping = {1: 4, 4: 1, Basic(5): Basic(6, 7)}\n>>> expr = Basic(1, Basic(2, 3), Basic(4, Basic(5)))\n>>> subs(mapping)(expr)\nBasic(4, Basic(2, 3), Basic(1, Basic(6, 7)))",
    "original_nl": "Full simultaneous exact substitution\n\nExamples\n========\n\n>>> from sympy.strategies.tools import subs\n>>> from sympy import Basic\n>>> mapping = {1: 4, 4: 1, Basic(5): Basic(6, 7)}\n>>> expr = Basic(1, Basic(2, 3), Basic(4, Basic(5)))\n>>> subs(mapping)(expr)\nBasic(4, Basic(2, 3), Basic(1, Basic(6, 7)))"
  },
  {
    "code": "@classmethod\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument('--tagging-file', '--tagging_file', dest='tagging_file', type=str, help='Specify a file to read tagging criteria from.', action='store')",
    "nl": "Adds command line arguments the helper supports to an argument group.\n\nThis function takes an argument parser or an argument group object and adds\nto it all the command line arguments this helper supports.\n\nArgs:\n  argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n      argparse group.",
    "original_nl": "Adds command line arguments the helper supports to an argument group.\n\nThis function takes an argument parser or an argument group object and adds\nto it all the command line arguments this helper supports.\n\nArgs:\n  argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n      argparse group."
  },
  {
    "code": "def cumulative_to_events(results):\n    if (hasattr(results, 'shape') and (np is not None)):\n        arr = np.zeros(len(results), dtype=float)\n        arr[1:] = np.diff(results)\n        return arr\n    try:\n        current = results[0][1]\n        res = []\n        for (t, v) in results:\n            res.append((t, (v - current)))\n            current = v\n        return res\n    except (TypeError, IndexError):\n        return results",
    "nl": "Transform cumulative counter values into the increasing events.",
    "original_nl": "Transform cumulative counter values into the increasing events."
  },
  {
    "code": "@property\ndef agent_state(self) -> Dict[(str, float)]:\n    return self.microbiota.abundances",
    "nl": "Returns the current state of the agent\n",
    "original_nl": "Returns the current state of the agent\n\nReturns\n-------\nDict[str, float]\n    A dictionary of microbe names and their corresponding biomass concentrations"
  },
  {
    "code": "@staticmethod\ndef get_elastic_mappings(es_major):\n    if (es_major != '2'):\n        mapping = '\\n             {\\n                \"dynamic\":true,\\n                    \"properties\": {\\n                        \"data\": {\\n                            \"properties\": {\\n                                \"description\": {\\n                                    \"type\": \"text\",\\n                                    \"index\": true\\n                                }\\n                            }\\n                        }\\n                    }\\n            }\\n            '\n    else:\n        mapping = '\\n             {\\n                \"dynamic\":true,\\n                    \"properties\": {\\n                        \"data\": {\\n                            \"properties\": {\\n                                \"description\": {\\n                                    \"type\": \"string\",\\n                                    \"index\": \"analyzed\"\\n                                }\\n                            }\\n                        }\\n                    }\\n            }\\n            '\n    return {\n        'items': mapping,\n    }",
    "nl": "Get Elasticsearch mapping.\n\nNon dynamic discovery of type for:\n    * data.answers.answered_by\n    * data.author\n\n",
    "original_nl": "Get Elasticsearch mapping.\n\nNon dynamic discovery of type for:\n    * data.answers.answered_by\n    * data.author\n\n:param es_major: major version of Elasticsearch, as string\n:returns:        dictionary with a key, 'items', with the mapping"
  },
  {
    "code": "def calcslope(dem, spacing, inc):\n    (azslope, rngslope) = np.gradient(dem)\n    azslope = np.arctan((azslope / spacing[0]))\n    rngslope = np.arctan((rngslope / ((spacing[1] / np.sin(inc)) + (rngslope / np.tan(inc)))))\n    return (rngslope, azslope)",
    "nl": "Calculate terrain slope angles.\n\nGiven an input DEM in radar (azimuth, slant range) coordinates, the DEM\npixel spacing (in azimuth and slant range), and the incidence angle,\ncalculate and return the azimuth and ground range slope angles (in\nradians).\n\nArguments:\n    dem: an array containing the DEM heights.\n    spacing: a tuple containing the (azimuth, slant range) pixel spacing\n        of the DEM, in meters.\n    inc: the incidence angle, in radians.\n  ",
    "original_nl": "Calculate terrain slope angles.\n\nGiven an input DEM in radar (azimuth, slant range) coordinates, the DEM\npixel spacing (in azimuth and slant range), and the incidence angle,\ncalculate and return the azimuth and ground range slope angles (in\nradians).\n\nArguments:\n    dem: an array containing the DEM heights.\n    spacing: a tuple containing the (azimuth, slant range) pixel spacing\n        of the DEM, in meters.\n    inc: the incidence angle, in radians.\n  \nReturns:\n    rngslope: the terrain slope angle in the ground range direction\n    azslope: the slope angle in the azimuth direction"
  },
  {
    "code": "def rebuild(self, paths=None):\n    if paths:\n        logger.info('Just adding new paths')\n    else:\n        logger.info('Rebuilding database')\n        self.truncate()\n        paths = self.paths\n    unsplitable_paths = set()\n    if (self.unsplitable_mode or self.exact_mode):\n        logger.info('Special modes enabled, doing a preliminary scan')\n        for root_path in paths:\n            logger.info(('Preliminary scanning %s' % root_path))\n            for (root, dirs, files) in os.walk(root_path):\n                if is_unsplitable(files):\n                    sep_root = root.split(os.sep)\n                    name = get_root_of_unsplitable(root.split(os.sep))\n                    while (sep_root[(- 1)] != name):\n                        sep_root.pop()\n                    path = os.path.join(*sep_root)\n                    logger.debug(('Found unsplitable path %r' % path))\n                    unsplitable_paths.add(path)\n            logger.info(('Done preliminary scanning %s' % root_path))\n    for root_path in paths:\n        logger.info(('Scanning %s' % root_path))\n        for (root, dirs, files) in os.walk(root_path):\n            unsplitable = False\n            if (self.unsplitable_mode or self.exact_mode):\n                sep_root = root.split(os.sep)\n                while sep_root:\n                    if (os.path.join(*sep_root) in unsplitable_paths):\n                        break\n                    sep_root.pop()\n                if sep_root:\n                    unsplitable = True\n                    if self.unsplitable_mode:\n                        unsplitable_name = sep_root[(- 1)]\n                        logger.info(('Looks like we found a unsplitable release in %r' % os.sep.join(sep_root)))\n                        for f in files:\n                            self.insert_into_database(root, f, 'unsplitable', unsplitable_name=unsplitable_name)\n                        continue\n            if (not unsplitable):\n                if self.normal_mode:\n                    for f in files:\n                        if self.skip_file(f):\n                            continue\n                        self.insert_into_database(root, f, 'normal')\n                if self.exact_mode:\n                    for f in files:\n                        self.insert_into_database(root, f, 'exact', 'f')\n                    for d in dirs:\n                        self.insert_into_database(root, d, 'exact', 'd')\n            if (self.hash_name_mode or self.hash_size_mode or self.hash_slow_mode):\n                for f in files:\n                    if (self.hash_size_mode or self.hash_slow_mode):\n                        self.insert_into_database(root, f, 'hash_store_size')\n                    if self.hash_name_mode:\n                        self.insert_into_database(root, f, 'hash_store_name')\n        logger.info(('Done scanning %s' % root_path))\n    self.db.sync()",
    "nl": "Scans the paths for files and rebuilds the database.",
    "original_nl": "Scans the paths for files and rebuilds the database."
  },
  {
    "code": "def closest_split_pair(Px, Py, delta, distance=euclidean_distance):\n    big_x = Px[(- 1)]\n    Sy = [x for x in Px if ((x[0] >= (big_x[0] - delta)) or (x[0] <= (big_x[0] + delta)))]\n    best = delta\n    best_pair = None\n    m = ((len(Sy) - 7) if (len(Sy) > 7) else len(Sy))\n    n = (7 if (len(Sy) > 7) else len(Sy))\n    for i in range(m):\n        for j in range(n):\n            if ((Sy[i] != Sy[j]) and (distance(Sy[i], Sy[j]) < best)):\n                best = distance(Sy[i], Sy[j])\n                best_pair = (Sy[i], Sy[j])\n    return best_pair",
    "nl": "Subroutine which checks if there is a pair of point such that one\npoint is in Px, the other is in Py and the distance between them is less\nthan delta.\n",
    "original_nl": "Subroutine which checks if there is a pair of point such that one\npoint is in Px, the other is in Py and the distance between them is less\nthan delta.\n\nReturns:\n    None, if no split pair is discovered which w/ dist smaller than delta.\n    tuple, of points (tuples) representing point w/ smallest distance.\n        Format: ((x1, y1), (x2, y2))"
  },
  {
    "code": "def ap_date(value):\n    if (not value):\n        return ''\n    bits = unicode(value).split('/')\n    (month, day, year) = bits\n    output = AP_MONTHS[(int(month) - 1)]\n    output += (' ' + unicode(int(day)))\n    output += (', ' + year)\n    return output",
    "nl": "Converts a date string in m/d/yyyy format into AP style.",
    "original_nl": "Converts a date string in m/d/yyyy format into AP style."
  },
  {
    "code": "def query_versions_pypi(self, package_name):\n    if (not (package_name in self.pkg_list)):\n        self.logger.debug(('Package %s not in cache, querying PyPI...' % package_name))\n        self.fetch_pkg_list()\n    versions = []\n    for pypi_pkg in self.pkg_list:\n        if (pypi_pkg.lower() == package_name.lower()):\n            if self.debug:\n                self.logger.debug(('DEBUG: %s' % package_name))\n            versions = self.package_releases(pypi_pkg)\n            package_name = pypi_pkg\n            break\n    return (package_name, versions)",
    "nl": "Fetch list of available versions for a package from The CheeseShop",
    "original_nl": "Fetch list of available versions for a package from The CheeseShop"
  },
  {
    "code": "def save_ready(self, service_name):\n    self._load_ready_file()\n    self._ready.add(service_name)\n    self._save_ready_file()",
    "nl": "Save an indicator that the given service is now data_ready.",
    "original_nl": "Save an indicator that the given service is now data_ready."
  },
  {
    "code": "def _get_localzone(_root='/'):\n    tzenv = os.environ.get('TZ')\n    if tzenv:\n        try:\n            return _tz_from_env(tzenv)\n        except pytz.UnknownTimeZoneError:\n            pass\n    tzpath = os.path.join(_root, 'etc/timezone')\n    if os.path.exists(tzpath):\n        with open(tzpath, 'rb') as tzfile:\n            data = tzfile.read()\n            if (data[:5] != 'TZif2'):\n                etctz = data.strip().decode()\n                if (' ' in etctz):\n                    (etctz, dummy) = etctz.split(' ', 1)\n                if ('#' in etctz):\n                    (etctz, dummy) = etctz.split('#', 1)\n                return pytz.timezone(etctz.replace(' ', '_'))\n    zone_re = re.compile('\\\\s*ZONE\\\\s*=\\\\s*\"')\n    timezone_re = re.compile('\\\\s*TIMEZONE\\\\s*=\\\\s*\"')\n    end_re = re.compile('\"')\n    for filename in ('etc/sysconfig/clock', 'etc/conf.d/clock'):\n        tzpath = os.path.join(_root, filename)\n        if (not os.path.exists(tzpath)):\n            continue\n        with open(tzpath, 'rt') as tzfile:\n            data = tzfile.readlines()\n        for line in data:\n            match = zone_re.match(line)\n            if (match is None):\n                match = timezone_re.match(line)\n            if (match is not None):\n                line = line[match.end():]\n                etctz = line[:end_re.search(line).start()]\n                return pytz.timezone(etctz.replace(' ', '_'))\n    tzpath = os.path.join(_root, 'etc/localtime')\n    if (os.path.exists(tzpath) and os.path.islink(tzpath)):\n        tzpath = os.path.realpath(tzpath)\n        start = (tzpath.find('/') + 1)\n        while (start is not 0):\n            tzpath = tzpath[start:]\n            try:\n                return pytz.timezone(tzpath)\n            except pytz.UnknownTimeZoneError:\n                pass\n            start = (tzpath.find('/') + 1)\n    for filename in ('etc/localtime', 'usr/local/etc/localtime'):\n        tzpath = os.path.join(_root, filename)\n        if (not os.path.exists(tzpath)):\n            continue\n        with open(tzpath, 'rb') as tzfile:\n            return pytz.tzfile.build_tzinfo('local', tzfile)\n    raise pytz.UnknownTimeZoneError('Can not find any timezone configuration')",
    "nl": "Tries to find the local timezone configuration.\n\nThis method prefers finding the timezone name and passing that to pytz,\nover passing in the localtime file, as in the later case the zoneinfo\nname is unknown.\n\nThe parameter _root makes the function look for files like /etc/localtime\nbeneath the _root directory. This is primarily used by the tests.\nIn normal usage you call the function without parameters.",
    "original_nl": "Tries to find the local timezone configuration.\n\nThis method prefers finding the timezone name and passing that to pytz,\nover passing in the localtime file, as in the later case the zoneinfo\nname is unknown.\n\nThe parameter _root makes the function look for files like /etc/localtime\nbeneath the _root directory. This is primarily used by the tests.\nIn normal usage you call the function without parameters."
  },
  {
    "code": "def handle_token(self, answer):\n    logger.info('Asked a token and got a reply from the API.')\n    bytarray = answer.readAll()\n    content = str(bytarray)\n    parsed_content = json.loads(content)\n    if ('access_token' in parsed_content):\n        logger.info('The API reply is an access token : the request worked as expected.')\n        self.token = ('Bearer ' + parsed_content['access_token'])\n        if (self.savedSearch == 'first'):\n            self.requestStatusClear = True\n            self.set_widget_status()\n        else:\n            self.requestStatusClear = True\n            self.send_request_to_isogeo_api(self.token)\n    elif ('error' in parsed_content):\n        logger.error('The API reply is an error. Id and secret must be invalid. Asking for them again.')\n        QMessageBox.information(iface.mainWindow(), self.tr('Error'), parsed_content['error'])\n        self.requestStatusClear = True\n        self.auth_prompt_form.show()\n    else:\n        self.requestStatusClear = True\n        logger.error('The API reply has an unexpected form : {0}'.format(parsed_content))\n        QMessageBox.information(iface.mainWindow(), self.tr('Error'), self.tr('Unknown error'))",
    "nl": "Handle the API answer when asked for a token.\n\nThis handles the API answer. If it has sent an access token, it calls\nthe initialization function. If not, it raises an error, and ask\nfor new IDs",
    "original_nl": "Handle the API answer when asked for a token.\n\nThis handles the API answer. If it has sent an access token, it calls\nthe initialization function. If not, it raises an error, and ask\nfor new IDs"
  },
  {
    "code": "def put(fromPath=None, toPath=None, content=None):\n    tpl = {\n        'ftp-event': 'request',\n    }\n    if (fromPath is not None):\n        tpl['from-path'] = fromPath\n    if (toPath is not None):\n        tpl['to-path'] = toPath\n    if (content is not None):\n        tpl['content'] = content\n    return tpl",
    "nl": "Construct a template for FTP logging",
    "original_nl": "Construct a template for FTP logging"
  },
  {
    "code": "def match(info, arguments):\n    if (platform.system() != 'Windows'):\n        return None\n    if (arguments.get('distro', None) != 'Windows'):\n        return None\n    return info.kwargs",
    "nl": "Check for matching configuration from DISTRIBUTIONS for arguments.\n\nIn effect, this just means checking if we're on Windows.",
    "original_nl": "Check for matching configuration from DISTRIBUTIONS for arguments.\n\nIn effect, this just means checking if we're on Windows."
  },
  {
    "code": "def test_plugin_build_rule(text_monitor_plugin):\n    config = {\n        'class_id': 'exopy.RejectRule',\n        'id': 'test_reject',\n        'suffixes': repr(['a', 'b']),\n    }\n    rule = text_monitor_plugin.build_rule(config)\n    assert (rule.id == 'test_reject')\n    assert (rule.suffixes == ['a', 'b'])\n    assert (rule.__class__.__name__ == 'RejectRule')\n    rule = text_monitor_plugin.build_rule('test_format')\n    assert (rule.id == 'test_format')\n    rule_name = list(text_monitor_plugin._rule_configs.contributions)[0]\n    rule = text_monitor_plugin.build_rule(rule_name)\n    assert (rule.id == rule_name)\n    assert (text_monitor_plugin.build_rule('__unknown__') is None)\n    config = {\n        'class_id': '__unknown__',\n        'id': 'test_reject',\n        'suffixes': repr(['a', 'b']),\n    }\n    assert (text_monitor_plugin.build_rule(config) is None)",
    "nl": "Test building a rule.",
    "original_nl": "Test building a rule."
  },
  {
    "code": "def build_url(name, **kwargs):\n    return ((reverse(name) + '?') + urlencode(kwargs))",
    "nl": "Build a URL given a view name and kwarg query parameters.",
    "original_nl": "Build a URL given a view name and kwarg query parameters."
  },
  {
    "code": "def apply_flow_changes(self, filepathname):\n    cells = self.read_nb(filepathname).cells\n    blocks_to_save = {\n        \n    }\n    block_cells = []\n    block_meta = {\n        \n    }\n    blocks_meta = []\n    for cell in cells:\n        if ('dagpy' in cell.metadata):\n            if (cell.metadata['dagpy']['cell_type'] == blockio.DELIMITER_CELL_TYPE):\n                if block_meta:\n                    blocks_meta += [block_meta]\n                    blocks_to_save[block_meta['block_id']] = block_cells\n                    block_cells = []\n                block_meta = FlowManager.parse_delimiter_cell(cell)\n        else:\n            block_cells += [cell]\n    if block_meta:\n        blocks_meta += [block_meta]\n        blocks_to_save[block_meta['block_id']] = block_cells\n    if blocks_meta:\n        self._dag.update_blocks(blocks_meta)\n    for (block_id, block_cells) in blocks_to_save.items():\n        blockio.save_block(block_id, block_cells, self._dag)",
    "nl": "Apply changes made in the flow to block files and the DAG.",
    "original_nl": "Apply changes made in the flow to block files and the DAG."
  },
  {
    "code": "def setUp(self):\n    super(SendActivationEmailTestCase, self).setUp()\n    self.student = UserFactory()",
    "nl": "Setup components used by each test.",
    "original_nl": "Setup components used by each test."
  },
  {
    "code": "def SplitKeyPath(key_path, path_seperator=definitions.KEY_PATH_SEPARATOR):\n    return list(filter(None, key_path.split(path_seperator)))",
    "nl": "Splits the key path into path segments.\n\nArgs:\n  key_path (str): key path.\n  path_seperator (Optional[str]): path seperator.\n",
    "original_nl": "Splits the key path into path segments.\n\nArgs:\n  key_path (str): key path.\n  path_seperator (Optional[str]): path seperator.\n\nReturns:\n  list[str]: key path segments without the root path segment, which is an\n      empty string."
  },
  {
    "code": "def test_make_edges_from_predictions(self):\n    predictions = [('gene1', 'gene2'), ('gene2', 'gene3')]\n    organism = MockOrganism('64091', {\n        'gene1': st.Feature('feature1', 'typ1', 'feature_name1', st.Location('contig1', 24, 89, False)),\n        'gene2': st.Feature('feature2', 'typ1', 'feature_name2', st.Location('contig1', 15, 21, False)),\n        'gene3': st.Feature('feature3', 'typ2', 'feature_name3', st.Location('contig1', 100, 154, False)),\n    })\n    edges = mo.make_pairs_from_predictions(predictions, organism)\n    self.assertEquals([('gene2', 'gene1'), ('gene2', 'gene2'), ('gene2', 'gene3')], edges)",
    "nl": "tests the make_edges_from_predictions function",
    "original_nl": "tests the make_edges_from_predictions function"
  },
  {
    "code": "def ballot_item_options_retrieve_doc_template_values(url_root):\n    required_query_parameter_list = [{\n        'name': 'voter_device_id',\n        'value': 'string',\n        'description': 'An 88 character unique identifier linked to a voter record on the server',\n    }, {\n        'name': 'api_key',\n        'value': 'string (from post, cookie, or get (in that order))',\n        'description': 'The unique key provided to any organization using the WeVoteServer APIs',\n    }, {\n        'name': 'google_civic_election_id',\n        'value': 'integer',\n        'description': 'The unique identifier for a particular election. If NOT provided, we instead use the google_civic_election_id for the person who is signed in.',\n    }]\n    optional_query_parameter_list = [{\n        'name': 'search_string',\n        'value': 'string',\n        'description': 'Search words to use to find the candidate or measure for voter guide.',\n    }, {\n        'name': 'state_code',\n        'value': 'string',\n        'description': 'The us state we want ballot item options for. ',\n    }]\n    potential_status_codes_list = [{\n        'code': 'CANDIDATES_RETRIEVED OFFICES_RETRIEVED MEASURES_RETRIEVED',\n        'description': 'Ballot items were found.',\n    }, {\n        'code': 'NO_CANDIDATES_RETRIEVED NO_OFFICES_RETRIEVED NO_MEASURES_RETRIEVED',\n        'description': 'Candidates, offices or measures were not able to be retrieved.',\n    }]\n    try_now_link_variables_dict = {\n        \n    }\n    api_response = '{\\n  \"status\": string,\\n  \"success\": boolean,\\n  \"search_string\": string,\\n  \"ballot_item_list\": candidate list\\n   [\\n      \"kind_of_ballot_item\": string (CANDIDATE),\\n      \"id\": integer,\\n      \"we_vote_id\": string,\\n      \"ballot_item_display_name\": string,\\n      \"candidate_photo_url_large\": string,\\n      \"candidate_photo_url_medium\": string,\\n      \"candidate_photo_url_tiny\": string,\\n      \"order_on_ballot\": integer,\\n      \"google_civic_election_id\": integer,\\n      \"ballotpedia_candidate_id\": integer,\\n      \"ballotpedia_candidate_url\": string,\\n      \"ballotpedia_person_id\": integer,\\n      \"maplight_id\": integer,\\n      \"contest_office_id\": integer,\\n      \"contest_office_we_vote_id\": string,\\n      \"contest_office_name\": string,\\n      \"politician_id\": integer,\\n      \"politician_we_vote_id\": string,\\n      \"party\": string,\\n      \"ocd_division_id\": string,\\n      \"state_code\": string,\\n      \"candidate_url\": string,\\n      \"facebook_url\": string,\\n      \"twitter_url\": string,\\n      \"twitter_handle\": string,\\n      \"google_plus_url\": string,\\n      \"youtube_url\": string,\\n      \"candidate_email\": string,\\n      \"candidate_phone\": string,\\n   ],\\n  \"ballot_item_list\": measure list\\n   [\\n      \"kind_of_ballot_item\": string (MEASURE),\\n      \"id\": integer,\\n      \"we_vote_id\": string,\\n      \"google_civic_election_id\": integer,\\n      \"ballot_item_display_name\": string,\\n      \"measure_subtitle\": string,\\n      \"maplight_id\": integer,\\n      \"vote_smart_id\": string,\\n      \"measure_text\": string,\\n      \"measure_url\": string,\\n      \"ocd_division_id\": string,\\n      \"district_name\": string,\\n      \"state_code\": string,\\n      \"google_civic_election_id\": integer,\\n   ],\\n}'\n    template_values = {\n        'api_name': 'ballotItemOptionsRetrieve',\n        'api_slug': 'ballotItemOptionsRetrieve',\n        'api_introduction': 'Returns measures and candidates based on search terms, so we can help volunteers or staff find offices, candidates or measures when they are building out organizational voter guides. This information is not organized in a hierarchy, but is instead provided in a simple list for browser-side quick search features.',\n        'try_now_link': 'apis_v1:ballotItemOptionsRetrieveView',\n        'try_now_link_variables_dict': try_now_link_variables_dict,\n        'url_root': url_root,\n        'get_or_post': 'GET',\n        'required_query_parameter_list': required_query_parameter_list,\n        'optional_query_parameter_list': optional_query_parameter_list,\n        'api_response': api_response,\n        'api_response_notes': '',\n        'potential_status_codes_list': potential_status_codes_list,\n    }\n    return template_values",
    "nl": "Show documentation about ballotItemOptionsRetrieve",
    "original_nl": "Show documentation about ballotItemOptionsRetrieve"
  },
  {
    "code": "@classmethod\ndef populate(cls, course_descriptor):\n    course_key = course_descriptor.id\n    course_details = cls(course_key.org, course_key.course, course_key.run)\n    course_details.start_date = course_descriptor.start\n    course_details.end_date = course_descriptor.end\n    course_details.enrollment_start = course_descriptor.enrollment_start\n    course_details.enrollment_end = course_descriptor.enrollment_end\n    course_details.pre_requisite_courses = course_descriptor.pre_requisite_courses\n    course_details.course_image_name = course_descriptor.course_image\n    course_details.course_image_asset_path = course_image_url(course_descriptor, 'course_image')\n    course_details.banner_image_name = course_descriptor.banner_image\n    course_details.banner_image_asset_path = course_image_url(course_descriptor, 'banner_image')\n    course_details.video_thumbnail_image_name = course_descriptor.video_thumbnail_image\n    course_details.video_thumbnail_image_asset_path = course_image_url(course_descriptor, 'video_thumbnail_image')\n    course_details.language = course_descriptor.language\n    course_details.self_paced = course_descriptor.self_paced\n    course_details.learning_info = course_descriptor.learning_info\n    course_details.instructor_info = course_descriptor.instructor_info\n    course_details.license = getattr(course_descriptor, 'license', 'all-rights-reserved')\n    course_details.intro_video = cls.fetch_youtube_video_id(course_key)\n    for attribute in ABOUT_ATTRIBUTES:\n        value = cls.fetch_about_attribute(course_key, attribute)\n        if (value is not None):\n            setattr(course_details, attribute, value)\n    return course_details",
    "nl": "Returns a fully populated CourseDetails model given the course descriptor",
    "original_nl": "Returns a fully populated CourseDetails model given the course descriptor"
  },
  {
    "code": "@property\ndef coor(self):\n    return (self.start, self.end)",
    "nl": "returns a tuple with start and stop",
    "original_nl": "returns a tuple with start and stop"
  },
  {
    "code": "def create(self, request):\n    telnet = request.telnet\n    telnet.sendline('group -a')\n    telnet.expect(('Adding a new Group(.+)\\\\n' + INTERACTIVE_PROMPT))\n    if (not ('gid' in request.data)):\n        raise MissingKeyError('Missing gid (group identifier)')\n    telnet.sendline((('gid ' + request.data['gid']) + '\\n'))\n    telnet.expect(INTERACTIVE_PROMPT)\n    telnet.sendline('ok\\n')\n    matched_index = telnet.expect([('.+Successfully added(.+)\\\\[(.+)\\\\][\\\\n\\\\r]+' + STANDARD_PROMPT), ('.+Error: (.+)[\\\\n\\\\r]+' + INTERACTIVE_PROMPT), (((('.+(.*)(' + INTERACTIVE_PROMPT) + '|') + STANDARD_PROMPT) + ')')])\n    if (matched_index == 0):\n        gid = telnet.match.group(2).strip()\n        telnet.sendline('persist\\n')\n        return JsonResponse({\n            'name': gid,\n        })\n    else:\n        raise ActionFailed(telnet.match.group(1))",
    "nl": "Create a group.\nOne POST parameter required, the group identifier (a string)\n---\n# YAML\nomit_serializer: true",
    "original_nl": "Create a group.\nOne POST parameter required, the group identifier (a string)\n---\n# YAML\nomit_serializer: true\nparameters:\n- name: gid\n  description: Group identifier\n  required: true\n  type: string\n  paramType: form"
  },
  {
    "code": "def can_convert(self, from_encoding, to_encoding):\n    from_encoding = self._canonize_name(from_encoding)\n    to_encoding = self._canonize_name(to_encoding)\n    if (from_encoding == to_encoding):\n        return 1\n    try:\n        return self.__map[from_encoding].has_key(to_encoding)\n    except KeyError:\n        return 0",
    "nl": "Returns true if converters to from from_encoding to to_encoding are\nknown. Encoding names follow the syntax specified by the XML rec.",
    "original_nl": "Returns true if converters to from from_encoding to to_encoding are\nknown. Encoding names follow the syntax specified by the XML rec."
  },
  {
    "code": "def geom(start=1, factor=2):\n    n = start\n    while True:\n        (yield n)\n        n *= factor",
    "nl": "iterator to generate geometric series\n",
    "original_nl": "iterator to generate geometric series\n:param start: start number\n:param factor: constant factor to apply\n:return: geometric series, e.g. geom(16, 0.5) -> 16 8 4 2 1 0.5 ..."
  },
  {
    "code": "def enqueue_all_shuffled_tasks(self, options):\n    task_options = ({\n        'routing_key': options['routing_key'],\n    } if options.get('routing_key') else {\n        \n    })\n    for (seq_id, kwargs) in enumerate(self._shuffled_task_kwargs(options)):\n        kwargs['seq_id'] = seq_id\n        result = tasks.compute_grades_for_course_v2.apply_async(kwargs=kwargs, **task_options)\n        log.info('Grades: Created {task_name}[{task_id}] with arguments {kwargs}'.format(task_name=tasks.compute_grades_for_course.name, task_id=result.task_id, kwargs=kwargs))",
    "nl": "Enqueue all tasks, in shuffled order.",
    "original_nl": "Enqueue all tasks, in shuffled order."
  },
  {
    "code": "def test_profiles_filters(self):\n    filter_url = self.profiles_list_url\n    resp = self.api_client.get(filter_url)\n    self.assertValidJSONResponse(resp)\n    self.assertEquals(len(self.deserialize(resp)['objects']), 8)\n    filter_url = (self.profiles_list_url + '?name__icontains=norm')\n    resp = self.api_client.get(filter_url)\n    self.assertValidJSONResponse(resp)\n    self.assertEquals(len(self.deserialize(resp)['objects']), 1)\n    filter_url = (self.profiles_list_url + '?name__icontains=NoRmAN')\n    resp = self.api_client.get(filter_url)\n    self.assertValidJSONResponse(resp)\n    self.assertEquals(len(self.deserialize(resp)['objects']), 1)\n    filter_url = (self.profiles_list_url + '?name__icontains=bar')\n    resp = self.api_client.get(filter_url)\n    self.assertValidJSONResponse(resp)\n    self.assertEquals(len(self.deserialize(resp)['objects']), 0)",
    "nl": "Test profiles filtering",
    "original_nl": "Test profiles filtering"
  },
  {
    "code": "def __init__(self, motifs):\n    self._motifs = motifs\n    self._motif_size = len(self._motifs[0])\n    for motif in self._motifs:\n        if (len(motif) != self._motif_size):\n            raise ValueError(('Motif %s given, expected motif size %s' % (motif, self._motif_size)))",
    "nl": "Initialize an input producer with motifs to look for.\n\nArguments:\n\no motifs - A complete list of motifs, in order, that are to be\nsearched for in a sequence.",
    "original_nl": "Initialize an input producer with motifs to look for.\n\nArguments:\n\no motifs - A complete list of motifs, in order, that are to be\nsearched for in a sequence."
  },
  {
    "code": "@d_func\ndef confirm_delete(self, obj):\n    return None",
    "nl": "Checks whether a specified object can be deleted.\n",
    "original_nl": "Checks whether a specified object can be deleted.\n\nReturns\n-------\n\n- **True** if the object should be deleted with no further prompting.\n- **False** if the object should not be deleted.\n- Anything else: Caller should take its default action (which might\n  include prompting the user to confirm deletion)."
  },
  {
    "code": "def observe_dhcp_packets(input=sys.stdin.buffer, out=sys.stdout):\n    try:\n        pcap = PCAP(input)\n        if (pcap.global_header.data_link_type != 1):\n            return 4\n        for (pcap_header, packet_bytes) in pcap:\n            out.write(str(datetime.now()))\n            out.write('\\n')\n            try:\n                packet = decode_ethernet_udp_packet(packet_bytes, pcap_header)\n                dhcp = DHCP(packet.payload)\n                if (not dhcp.is_valid()):\n                    out.write(dhcp.invalid_reason)\n                out.write(('     Source MAC address: %s\\n' % format_eui(packet.l2.src_eui)))\n                out.write(('Destination MAC address: %s\\n' % format_eui(packet.l2.dst_eui)))\n                if (packet.l2.vid is not None):\n                    out.write(('     Seen on 802.1Q VID: %s\\n' % packet.l2.vid))\n                out.write(('      Source IP address: %s\\n' % packet.l3.src_ip))\n                out.write((' Destination IP address: %s\\n' % packet.l3.dst_ip))\n                dhcp.write(out=out)\n                out.flush()\n            except PacketProcessingError as e:\n                out.write(e.error)\n                out.write('\\n\\n')\n                out.flush()\n    except EOFError:\n        return 3\n    except PCAPError:\n        return 2\n    return None",
    "nl": "Read stdin and look for tcpdump binary DHCP output.\n\n",
    "original_nl": "Read stdin and look for tcpdump binary DHCP output.\n\n:param input: Stream to read PCAP data from.\n:type input: a file or stream supporting `read(int)`\n:param out: Stream to write to.\n:type input: a file or stream supporting `write(str)` and `flush()`."
  },
  {
    "code": "def test_iterable():\n    assert (plist(iter('a')) == plist(iter('a')))",
    "nl": "PLists can be created from iterables even though they can't be len()\nhinted.",
    "original_nl": "PLists can be created from iterables even though they can't be len()\nhinted."
  },
  {
    "code": "def thumbnail(width, height, max_length):\n    max_width = max_height = float(max_length)\n    if (width > max_width):\n        ratio = (max_width / width)\n        width = max_width\n        height = (height * ratio)\n    if (height > max_height):\n        ratio = (max_height / height)\n        height = max_height\n        width = (width * ratio)\n    return (int(width), int(height))",
    "nl": "make the picture size to thumbnail\n",
    "original_nl": "make the picture size to thumbnail\n:param width: picture width\n:param height: picture height\n:param max_length: expect to thumbnail the max length\n:return: new width and height"
  },
  {
    "code": "def test_cross_product():\n    vector1 = Vector([5, 3, (- 2)])\n    vector2 = Vector([(- 1), 0, 3])\n    answer = Vector([9, (- 13), 3])\n    cross_product = vector1.threed_cross_product(vector2)\n    assert (cross_product == answer)\n    assert (cross_product.is_orthogonal(vector1) is True)\n    assert (cross_product.is_orthogonal(vector2) is True)",
    "nl": "Test 3d dimensional cross product of two Vectors",
    "original_nl": "Test 3d dimensional cross product of two Vectors"
  },
  {
    "code": "def test_checks_display_not_warning_force_enqueue(exopy_qtbot, dialog_sleep):\n    dial = ChecksDisplay(errors={\n        'test': 'dummy',\n        'complex': {\n            'rr': 'tt',\n        },\n    })\n    dial.show()\n    wait_for_window_displayed(exopy_qtbot, dial)\n    exopy_qtbot.wait(dialog_sleep)\n    assert (dial.central_widget().widgets()[(- 1)].text == 'Force enqueue')\n    with handle_question(exopy_qtbot, 'yes'):\n        dial.central_widget().widgets()[(- 1)].clicked = True\n\n    def assert_result():\n        assert dial.result\n    exopy_qtbot.wait_until(assert_result)",
    "nl": "Test displaying checks for a situation that do not allow enqueuing.",
    "original_nl": "Test displaying checks for a situation that do not allow enqueuing."
  },
  {
    "code": "@task\n@parallel\ndef change_password():\n    try:\n        passwd = env['passwd']\n    except KeyError:\n        error('Must specify --set=passwd=\"<newpasswd>\" on command line', abort)\n    cpw = crypt(passwd, 'mmmsalt')\n    sudo(cmd('usermod --password %s pi', cpw))",
    "nl": "Change the password on the pi@ accounts",
    "original_nl": "Change the password on the pi@ accounts"
  },
  {
    "code": "def test_history_is_same(self):\n    P1 = axelrod.MindReader()\n    P2 = axelrod.Grudger()\n    P1.history = ['C', 'C']\n    P2.history = ['C', 'D']\n    P1.strategy(P2)\n    self.assertEqual(P1.history, ['C', 'C'])\n    self.assertEqual(P2.history, ['C', 'D'])",
    "nl": "Checks that the history is not altered by the player",
    "original_nl": "Checks that the history is not altered by the player"
  },
  {
    "code": "def feed_item_factory(self, carrier=1, region=1, item_type=feed.FEED_TYPE_APP, **kw):\n    feed_item = FeedItem(carrier=carrier, region=region, item_type=item_type, **kw)\n    if (item_type == feed.FEED_TYPE_APP):\n        feed_item.app = self.feed_app_factory()\n    elif (item_type == feed.FEED_TYPE_BRAND):\n        feed_item.brand = self.feed_brand_factory()\n    elif (item_type == feed.FEED_TYPE_COLL):\n        feed_item.collection = self.feed_collection_factory()\n    elif (item_type == feed.FEED_TYPE_SHELF):\n        feed_item.shelf = self.feed_shelf_factory(carrier=carrier, region=region)\n    feed_item.save()\n    return feed_item",
    "nl": "Creates a single FeedItem of any feed element type specified.",
    "original_nl": "Creates a single FeedItem of any feed element type specified."
  },
  {
    "code": "def get_data(connection, vm_name=None):\n    vms_service = connection.system_service().vms_service()\n    clusters_service = connection.system_service().clusters_service()\n    if vm_name:\n        vm = (vms_service.list(search=('name=%s' % vm_name)) or [None])\n        data = get_dict_of_struct(connection=connection, vm=vm[0])\n    else:\n        vms = dict()\n        data = defaultdict(list)\n        for vm in vms_service.list():\n            name = vm.name\n            vm_service = vms_service.vm_service(vm.id)\n            cluster_service = clusters_service.cluster_service(vm.cluster.id)\n            vms[name] = get_dict_of_struct(connection, vm)\n            cluster_name = connection.follow_link(vm.cluster).name\n            data[('cluster_%s' % cluster_name)].append(name)\n            tags_service = vm_service.tags_service()\n            for tag in tags_service.list():\n                data[('tag_%s' % tag.name)].append(name)\n            data[('status_%s' % vm.status)].append(name)\n            for group in cluster_service.affinity_groups_service().list():\n                if (vm.name in [v.name for v in connection.follow_link(group.vms)]):\n                    data[('affinity_group_%s' % group.name)].append(vm.name)\n            affinity_labels_service = vm_service.affinity_labels_service()\n            for label in affinity_labels_service.list():\n                data[('affinity_label_%s' % label.name)].append(name)\n        data['_meta'] = {\n            'hostvars': vms,\n        }\n    return data",
    "nl": "Obtain data of `vm_name` if specified, otherwise obtain data of all vms.",
    "original_nl": "Obtain data of `vm_name` if specified, otherwise obtain data of all vms."
  },
  {
    "code": "def _TerminateProcess(self, process):\n    pid = process.pid\n    logger.warning('Terminating process: (PID: {0:d}).'.format(pid))\n    process.terminate()\n    process.join(timeout=self._PROCESS_JOIN_TIMEOUT)\n    if process.is_alive():\n        logger.warning('Killing process: (PID: {0:d}).'.format(pid))\n        self._KillProcess(pid)",
    "nl": "Terminate a process.\n\nArgs:\n  process (MultiProcessBaseProcess): process to terminate.",
    "original_nl": "Terminate a process.\n\nArgs:\n  process (MultiProcessBaseProcess): process to terminate."
  },
  {
    "code": "def lookup_prefix(self, prefix):\n    if (prefix == ''):\n        return self.traverse()\n    (head, tail) = self.split(prefix)\n    if (head not in self.children):\n        return []\n    pairs = self.children[head].lookup_prefix(tail)\n    pairs = map((lambda p: ((head + p[0]), p[1])), pairs)\n    return pairs",
    "nl": "Returns a list of all [(key, value)] pairs where key starts with\nprefix.\n\nTwo phases:\n1. traverse the prefix\n2. once the prefix has been traversed, return the entire subtree.\n\nArgs:\n    prefix: str, the prefix to find all corresponding keys.",
    "original_nl": "Returns a list of all [(key, value)] pairs where key starts with\nprefix.\n\nTwo phases:\n1. traverse the prefix\n2. once the prefix has been traversed, return the entire subtree.\n\nArgs:\n    prefix: str, the prefix to find all corresponding keys.\nReturns:\n    list, of pairs, format [(key, value)]"
  },
  {
    "code": "def unrepr(s):\n    if (not s):\n        return s\n    if (sys.version_info < (3, 0)):\n        b = _Builder2()\n    else:\n        b = _Builder3()\n    obj = b.astnode(s)\n    return b.build(obj)",
    "nl": "Return a Python object compiled from a string.",
    "original_nl": "Return a Python object compiled from a string."
  },
  {
    "code": "def load(self, config_file):\n    module_name = filepath.splitext(filepath.basename(config_file))[0]\n    if (self.deployer_object and self.deployer_object.loaded):\n        raise deployer.DeployerError('Tried to load {module} deployer that is already loaded!'.format(module=module_name))\n    self.deployer_name = module_name\n    self.deployer_module = deployer.deployer_import(self.deployer_name, config_file)\n    (docstring, new_style, classic, default) = fabric_main.load_tasks_from_module(self.deployer_module)\n    self.tasks = {\n        'docstring': docstring,\n        'functions': (new_style if state.env.new_style_tasks else classic),\n        'default': default,\n    }\n    state.commands.update(self.tasks.get('functions', {\n        \n    }))\n    if (not state.commands):\n        log.err('No commands found ...aborting')\n    else:\n        for name in state.commands:\n            execute(name)",
    "nl": "Load the workflow rules from a Mamba .dc Python file\n\n",
    "original_nl": "Load the workflow rules from a Mamba .dc Python file\n\n:param config_file: The file where to load the configuration from\n:type config_file: str"
  },
  {
    "code": "def create_random_samples(count=30, seed=1):\n    if (seed is not None):\n        numpy.random.seed(seed)\n    sample_a = create_random_sample(10.0, 1.0, count=count, seed=None)\n    sample_b = create_random_sample(12.0, 1.0, count=count, seed=None)\n    return (sample_a, sample_b)",
    "nl": "Create a sample of measurements.",
    "original_nl": "Create a sample of measurements."
  },
  {
    "code": "def sleep(self):\n    curr_time = time()\n    sleep(self._remaining(curr_time))\n    self.last_time = (self.last_time + self.sleep_dur)\n    if ((curr_time - self.last_time) > (self.sleep_dur * 2)):\n        self.last_time = curr_time",
    "nl": "Attempt sleep at the specified rate. sleep() takes into account the time elapsed since the last successful sleep().",
    "original_nl": "Attempt sleep at the specified rate. sleep() takes into account the time elapsed since the last successful sleep()."
  },
  {
    "code": "def test_over_upper_boundary(self):\n    self.assertRaises(AssertionError, COLOR_CONVERTER.hex_to_int, '100')",
    "nl": "Rainmeter only supports up to 255.",
    "original_nl": "Rainmeter only supports up to 255."
  },
  {
    "code": "def as_sql(self, compiler, connection):\n    (lhs, lhs_params) = self.process_lhs(compiler, connection)\n    (rhs, rhs_params) = self.process_rhs(compiler, connection)\n    params = (lhs_params + rhs_params)\n    return ((f'%s <> %s' % (lhs, rhs)), params)",
    "nl": "Handle as SQL method for not equal lookup.",
    "original_nl": "Handle as SQL method for not equal lookup."
  },
  {
    "code": "def add_heat(self, heatmap, bbox_list):\n    for box in bbox_list:\n        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n    return heatmap",
    "nl": "Add heat according to bounding box list\nAttr:\n    heatmap: heat map initiliazed to image size\n    bbox_list: bounding boxes",
    "original_nl": "Add heat according to bounding box list\nAttr:\n    heatmap: heat map initiliazed to image size\n    bbox_list: bounding boxes\nReturns:\n    resulted heat map image"
  },
  {
    "code": "def getRepoMetadata(self):\n    try:\n        self.payload.updateBaseRepo(fallback=False, checkmount=False)\n    except (OSError, PayloadError) as err:\n        LOG.error('Error: %s', err)\n        self.errors.append(_('Failed to set up installation source'))\n    else:\n        self.payload.gatherRepoMetadata()\n        self.payload.release()\n        if (not self.payload.baseRepo):\n            self.errors.append(_('Error downloading package metadata'))\n        else:\n            try:\n                self.payload.environments\n                self.payload.groups\n            except MetadataError:\n                self.errors.append(_('No installation source available'))",
    "nl": "Pull down yum repo metadata",
    "original_nl": "Pull down yum repo metadata"
  },
  {
    "code": "def begin(self):\n    mid = self._device.readU16BE(MCP9808_REG_MANUF_ID)\n    did = self._device.readU16BE(MCP9808_REG_DEVICE_ID)\n    self._logger.debug('Read manufacturer ID: {0:04X}'.format(mid))\n    self._logger.debug('Read device ID: {0:04X}'.format(did))\n    return ((mid == 84) and (did == 1024))",
    "nl": "Start taking temperature measurements. Returns True if the device is \nintialized, False otherwise.",
    "original_nl": "Start taking temperature measurements. Returns True if the device is \nintialized, False otherwise."
  },
  {
    "code": "def test_add_remove_fact_type(self):\n    model = Model()\n    fact = FactType(name='F1')\n    fact.add_role(player=ObjectType(name='O1'))\n    model.add(fact)\n    self.assertEquals(model.fact_types.count(), 1)\n    self.assertEquals(model.fact_types.get('F1'), fact)\n    with self.assertRaises(NotImplementedError):\n        model.remove(fact)\n    '\\n        model.remove(fact)\\n        self.assertEquals(model.fact_types.count(), 0)\\n        self.assertEquals(model.fact_types.get(\"F1\"), None)\\n        '",
    "nl": "Test adding and removing a fact type from the model.",
    "original_nl": "Test adding and removing a fact type from the model."
  },
  {
    "code": "def distMatProcessor(distances, nFlagVal=1e+305, nFlag=False, ignoreNodes=[]):\n    read_fl = False\n    dist_matrix = []\n    node_order = []\n    matrix = None\n    if (isinstance(distances, basestring) and os.path.exists(distances)):\n        distances = open(distances, 'rU')\n    distances = distances.read()\n    distances_lines = distances.splitlines()\n    if ('<?xml' in distances_lines[0]):\n        (matrix, node_order) = parseFastPhyloXml(StringIO(distances), nFlagVal, nFlag)\n    else:\n        x_ind = 0\n        for line in distances_lines:\n            line = line.strip()\n            if line:\n                if (not read_fl):\n                    read_fl = True\n                else:\n                    x_ind += 1\n                    line_list = [getFloatValue(x.strip(), x_ind, y_ind, nFlagVal, nFlag) for (y_ind, x) in enumerate(line.split())]\n                    dist_matrix.append(line_list[1:])\n                    node_order.append(line_list[0])\n        matrix = np.array(dist_matrix, dtype=np.float)\n    if ignoreNodes:\n        for n in ignoreNodes:\n            ind = node_order.index(n)\n            if (ind > (- 1)):\n                matrix = remove_ij(matrix, ind, ind)\n                node_order.remove(n)\n    return (matrix, node_order)",
    "nl": "Formating distance matrix from a file or string input and node order for\nUPGMA or NJ join",
    "original_nl": "Formating distance matrix from a file or string input and node order for\nUPGMA or NJ join"
  },
  {
    "code": "def on_frontmatter_loaded(self, source_file, frontmatter):\n    if (not self._is_post(frontmatter)):\n        return\n    self._validate_post(source_file, frontmatter)\n    post = BlogPost(date=frontmatter['date'], source_file=source_file, summary=frontmatter.get('summary'), title=frontmatter['title'], route=self._resolver.as_route(source_file), url=self._resolver.as_url(source_file), posts=self.posts)\n    frontmatter['post'] = post\n    if (post != self.posts.get(source_file)):\n        self.posts[source_file] = post\n        self._should_generate = True",
    "nl": "Record any new blog posts.",
    "original_nl": "Record any new blog posts."
  },
  {
    "code": "def sign_str(data, key):\n    return sign(key, data, HASH_TYPE)",
    "nl": "Sign a string blob",
    "original_nl": "Sign a string blob"
  },
  {
    "code": "def execute_plugin(self, event_action):\n    self.logger.info(('Firing incident for [%s] [%s] [%s] [%s]' % (self.incident.id, self.incident.event.id, self.incident.element, event_action.plugin)))\n    if (not event_action.plugin.status):\n        return\n    incident_info = self.incident.incidentInfo\n    plugin_json = json_formatter.create_json_parameters(event_action, self.incident, self.incident_message)\n    try:\n        response = requests.post((event_action.plugin.server.url + '/runplugin'), data=plugin_json, verify=event_action.plugin.server.ssl_verify)\n        EventActionLog.objects.create(eventAction=event_action, text=response.text, incident=self.incident)\n    except Exception as e:\n        event_action_error = ('Error executing Plugin:%s: ErrMsg:%s' % (event_action.plugin.name, e))\n        EventActionLog.objects.create(eventAction=event_action, text=event_action_error, incident=self.incident)\n        self.logger.error(event_action_error)\n    finally:\n        return",
    "nl": "Fires a relevant action based on cito_engine.models.EventAction settings",
    "original_nl": "Fires a relevant action based on cito_engine.models.EventAction settings"
  },
  {
    "code": "def SU(n=1):\n    return ('\\x1b[%sS' % n)",
    "nl": "Return terminal code to scroll up n lines.",
    "original_nl": "Return terminal code to scroll up n lines."
  },
  {
    "code": "@property\ndef C(self):\n    return self.single",
    "nl": "Bitname for 'Single' in original equation derivation.",
    "original_nl": "Bitname for 'Single' in original equation derivation."
  },
  {
    "code": "def _policy_evaluation(mdp, policy, max_iter=200, epsilon=1e-05):\n    finished = False\n    iteration = 0\n    value = np.zeros(len(mdp.S))\n    while ((iteration < max_iter) and (not finished)):\n        v_old = np.array(value)\n        delta = 0\n        for s in mdp.S:\n            value[s] = (mdp.R(s, None) + (mdp.gamma * np.sum([(p * value[s1]) for (p, s1) in mdp.T(s, policy[s])])))\n            delta = max(delta, np.fabs((value[s] - v_old[s])))\n        if (delta < epsilon):\n            finished = True\n        iteration += 1\n    return value",
    "nl": "Compute the value of a policy\n\nPerform Bellman backups to find the value of a policy for all states in the\nMDP",
    "original_nl": "Compute the value of a policy\n\nPerform Bellman backups to find the value of a policy for all states in the\nMDP"
  },
  {
    "code": "def update(self, iterable):\n    set = self.set\n    for index in iterable:\n        set(index)",
    "nl": "Takes an iterable of integers representing positions, and turns\non the bits at those positions.",
    "original_nl": "Takes an iterable of integers representing positions, and turns\non the bits at those positions."
  },
  {
    "code": "def symbol_by_name(name, aliases={\n    \n}, imp=None, package=None, sep='.', default=None, **kwargs):\n    if (imp is None):\n        imp = importlib.import_module\n    if (not isinstance(name, string_t)):\n        return name\n    name = (aliases.get(name) or name)\n    sep = (':' if (':' in name) else sep)\n    (module_name, _, cls_name) = name.rpartition(sep)\n    if (not module_name):\n        (cls_name, module_name) = (None, (package if package else cls_name))\n    try:\n        try:\n            module = imp(module_name, package=package, **kwargs)\n        except ValueError as exc:\n            reraise(ValueError, ValueError(\"Couldn't import {0!r}: {1}\".format(name, exc)), sys.exc_info()[2])\n        return (getattr(module, cls_name) if cls_name else module)\n    except (ImportError, AttributeError):\n        if (default is None):\n            raise\n    return default",
    "nl": "Get symbol by qualified name.\n\nThe name should be the full dot-separated path to the class::\n\n    modulename.ClassName\n\nExample::\n\n    celery.concurrency.processes.TaskPool\n                                ^- class name\n\nor using ':' to separate module and symbol::\n\n    celery.concurrency.processes:TaskPool\n\nIf `aliases` is provided, a dict containing short name/long name\nmappings, the name is looked up in the aliases first.\n\nExamples:\n\n    >>> symbol_by_name('celery.concurrency.processes.TaskPool')\n    <class 'celery.concurrency.processes.TaskPool'>\n\n    >>> symbol_by_name('default', {\n    ...     'default': 'celery.concurrency.processes.TaskPool'})\n    <class 'celery.concurrency.processes.TaskPool'>\n\n    # Does not try to look up non-string names.\n    >>> from celery.concurrency.processes import TaskPool\n    >>> symbol_by_name(TaskPool) is TaskPool\n    True",
    "original_nl": "Get symbol by qualified name.\n\nThe name should be the full dot-separated path to the class::\n\n    modulename.ClassName\n\nExample::\n\n    celery.concurrency.processes.TaskPool\n                                ^- class name\n\nor using ':' to separate module and symbol::\n\n    celery.concurrency.processes:TaskPool\n\nIf `aliases` is provided, a dict containing short name/long name\nmappings, the name is looked up in the aliases first.\n\nExamples:\n\n    >>> symbol_by_name('celery.concurrency.processes.TaskPool')\n    <class 'celery.concurrency.processes.TaskPool'>\n\n    >>> symbol_by_name('default', {\n    ...     'default': 'celery.concurrency.processes.TaskPool'})\n    <class 'celery.concurrency.processes.TaskPool'>\n\n    # Does not try to look up non-string names.\n    >>> from celery.concurrency.processes import TaskPool\n    >>> symbol_by_name(TaskPool) is TaskPool\n    True"
  },
  {
    "code": "def transform_translatable_fields(model, fields):\n    if (not hasattr(model, 'i18n')):\n        return fields\n    ret = {\n        'i18n': fields.pop('i18n', {\n            \n        }),\n    }\n    has_translated_fields = (len(ret['i18n'].items()) > 0)\n    for (field_name, value) in fields.items():\n        try:\n            field = model._meta.get_field(field_name)\n        except FieldDoesNotExist:\n            ret[field_name] = value\n            continue\n        if isinstance(field, TranslatedVirtualField):\n            has_translated_fields = True\n            if (field.get_language() == get_default_language()):\n                if (field.original_name in fields):\n                    raise ValueError('Attempted override of \"{}\" with \"{}\". Only one of the two is allowed.'.format(field.original_name, field_name))\n                ret[field.original_name] = value\n            else:\n                ret['i18n'][field.name] = value\n        else:\n            ret[field_name] = value\n    if (not has_translated_fields):\n        return fields\n    return ret",
    "nl": "Transform the kwargs for a <Model>.objects.create() or <Model>()\nto allow passing translated field names.\n\nArguments:\n    fields (dict): kwargs to a model __init__ or Model.objects.create() method\n        for which the field names need to be translated to values in the i18n field",
    "original_nl": "Transform the kwargs for a <Model>.objects.create() or <Model>()\nto allow passing translated field names.\n\nArguments:\n    fields (dict): kwargs to a model __init__ or Model.objects.create() method\n        for which the field names need to be translated to values in the i18n field"
  },
  {
    "code": "def is_tuple(obj):\n    return (True if isinstance(obj, tuple) else False)",
    "nl": "Checks if a given sequence is a tuple.\n",
    "original_nl": "Checks if a given sequence is a tuple.\n\nParameters\n----------\nobj : object\n    The input array.\n\nReturns\n-------\ntest result : bool\n    The test result of whether seq is a tuple or not.\n\n>>> is_tuple(('a', 'b'))\nTrue\n\n>>> is_tuple(['a', 'b'])\nFalse\n\n>>> is_tuple(4)\nFalse"
  },
  {
    "code": "def testPushNoSave(self):\n    obj = self.model()\n    push_back = (lambda : obj.names.push_back('this should fail'))\n    push_front = (lambda : obj.names.push_front('this should also fail'))\n    self.assertRaises(StructureFieldError, push_back)\n    self.assertRaises(StructureFieldError, push_front)",
    "nl": "Push a new value to a list field should rise an error if the object\nis not saved on databse.",
    "original_nl": "Push a new value to a list field should rise an error if the object\nis not saved on databse."
  },
  {
    "code": "def denorm_image(image):\n    return tf.cast(((image + 1.0) * 127.5), tf.uint8)",
    "nl": "Undo normalization of a tensor of range [-1.0, 1.0] to [0, 255]\n\n",
    "original_nl": "Undo normalization of a tensor of range [-1.0, 1.0] to [0, 255]\n\n:param image: input tensor"
  },
  {
    "code": "def get_hidden_feature_matrix_SVD(user_rate_dict, k):\n    (user_index, song_index, rating_matrix) = full_rating_matrix_with_index(user_rate_dict)\n    (U, s, V) = numpy.linalg.svd(rating_matrix, full_matrices=True)\n    V_bar = V[0:k]\n    for i in range(0, k):\n        V_bar[i] = (s[i] * V_bar[i])\n    hidden_feature = V_bar\n    user_weight = U[:, 0:k]\n    return (user_weight, hidden_feature)",
    "nl": "Get hidden feature matrix by SVD method\n\n",
    "original_nl": "Get hidden feature matrix by SVD method\n\n:param user_rate_dict: each user's rating score\n:param k: number of hidden features\n:return data: hidden feature dataset\n:rtype: ndarray"
  },
  {
    "code": "def __init__(self, name, generator_class, vehicles, net_params, initial_config=None):\n    self.left_len = net_params.additional_params['horizontal_length_in']\n    self.right_len = net_params.additional_params['horizontal_length_out']\n    self.bottom_len = net_params.additional_params['vertical_length_in']\n    self.top_len = net_params.additional_params['vertical_length_out']\n    self.horizontal_junction_len = (2.9 + (3.3 * net_params.additional_params['vertical_lanes']))\n    self.vertical_junction_len = (2.9 + (3.3 * net_params.additional_params['horizontal_lanes']))\n    self.inner_space_len = 0.28\n    net_params.additional_params['length'] = (((((self.left_len + self.right_len) + self.horizontal_junction_len) + self.bottom_len) + self.top_len) + self.vertical_junction_len)\n    if ('horizontal_lanes' not in net_params.additional_params):\n        raise ValueError('number of horizontal lanes not supplied')\n    if ('vertical_lanes' not in net_params.additional_params):\n        raise ValueError('number of vertical lanes not supplied')\n    self.lanes = {\n        'top': net_params.additional_params['vertical_lanes'],\n        'bottom': net_params.additional_params['vertical_lanes'],\n        'left': net_params.additional_params['horizontal_lanes'],\n        'right': net_params.additional_params['horizontal_lanes'],\n    }\n    self.enter_lane = {\n        'horizontal': 'left',\n        'vertical': 'bottom',\n    }\n    if ('speed_limit' not in net_params.additional_params):\n        raise ValueError('speed limit not supplied')\n    if (isinstance(net_params.additional_params['speed_limit'], int) or isinstance(net_params.additional_params['speed_limit'], float)):\n        self.speed_limit = {\n            'horizontal': net_params.additional_params['speed_limit'],\n            'vertical': net_params.additional_params['speed_limit'],\n        }\n    elif (('vertical' in net_params.additional_params['speed_limit']) and ('horizontal' in net_params.additional_params['speed_limit'])):\n        self.speed_limit = {\n            'horizontal': net_params.additional_params['speed_limit']['horizontal'],\n            'vertical': net_params.additional_params['speed_limit']['vertical'],\n        }\n    else:\n        raise ValueError('speed limit must contain a number or a dict with keys: \"vertical\" and \"horizontal\"')\n    super().__init__(name, generator_class, vehicles, net_params, initial_config)",
    "nl": "Initializes a two-way intersection scenario. Required net_params: horizontal_length_before,\nhorizontal_length_after, horizontal_lanes, vertical_length_before, vertical_length_after, vertical_lanes,\nspeed_limit. Required initial_config: positions.\n\nSee Scenario.py for description of params.",
    "original_nl": "Initializes a two-way intersection scenario. Required net_params: horizontal_length_before,\nhorizontal_length_after, horizontal_lanes, vertical_length_before, vertical_length_after, vertical_lanes,\nspeed_limit. Required initial_config: positions.\n\nSee Scenario.py for description of params."
  },
  {
    "code": "def shuffle(reader, buf_size):\n\n    def data_reader():\n        buf = []\n        for e in reader():\n            buf.append(e)\n            if (len(buf) >= buf_size):\n                random.shuffle(buf)\n                for b in buf:\n                    (yield b)\n                buf = []\n        if (len(buf) > 0):\n            random.shuffle(buf)\n            for b in buf:\n                (yield b)\n    return data_reader",
    "nl": "Creates a data reader whose data output is shuffled.\n\nOutput from the iterator that created by original reader will be\nbuffered into shuffle buffer, and then shuffled. The size of shuffle buffer\nis determined by argument buf_size.\n\n",
    "original_nl": "Creates a data reader whose data output is shuffled.\n\nOutput from the iterator that created by original reader will be\nbuffered into shuffle buffer, and then shuffled. The size of shuffle buffer\nis determined by argument buf_size.\n\n:param reader: the original reader whose output will be shuffled.\n:type reader: callable\n:param buf_size: shuffle buffer size.\n:type buf_size: int\n\n:return: the new reader whose output is shuffled.\n:rtype: callable"
  },
  {
    "code": "def add_tag(self, tag):\n    return self.conn.transaction([], [operation.OperationAddTag(node=self.id, tag=tag)])",
    "nl": "Adds a tag to this object.  Returns an awaitable of None.",
    "original_nl": "Adds a tag to this object.  Returns an awaitable of None."
  },
  {
    "code": "def head(self, view, view_args=None, view_kwargs=None, path='/', data={\n    \n}, follow=False, **extra):\n    return super(Client, self).head(path, data=data, follow=follow, **lang.updated(extra, self._view_request(view, view_args, view_kwargs)))",
    "nl": "Runs the given view with a HEAD request.",
    "original_nl": "Runs the given view with a HEAD request."
  },
  {
    "code": "def stop_acmeair(self):\n    if hasattr(self, 'proc'):\n        os.kill(self.proc.pid, signal.SIGTERM)\n        print('AcmeAir is shutdown')",
    "nl": "Stop the AcmeAir App that was started by this util",
    "original_nl": "Stop the AcmeAir App that was started by this util"
  },
  {
    "code": "def altitude_from_radius(radius, body):\n    return (radius - body.mean_radius)",
    "nl": "Return altitude for a given radius.",
    "original_nl": "Return altitude for a given radius."
  },
  {
    "code": "def test_001_parent_exact(self):\n    q = BoundsQuery(self.p, parent='test1_parent', parent_regex=False)\n    qbounds = sorted(q.results())\n    self.assertEqual(1, len(qbounds))\n    b = qbounds[0]\n    self.assertEqual(BoundsRuletype.typebounds, b.ruletype)\n    self.assertEqual('test1_parent', b.parent)\n    self.assertEqual('test1_child', b.child)",
    "nl": "Bounds query with exact parent match.",
    "original_nl": "Bounds query with exact parent match."
  },
  {
    "code": "def run(self):\n    updates = {\n        \n    }\n    output = shellout('cut -d, -f1,4- {changes} > {output}', changes=self.input().get('changes').path)\n    with open(output) as handle:\n        for line in handle:\n            fields = [s.strip() for s in line.split(',')]\n            updates[fields[0]] = fields[1:]\n    os.remove(output)\n    logging.debug('%s changes staged', len(updates))\n    with self.input().get('file').open() as handle:\n        with self.output().open('w') as output:\n            for line in handle:\n                doc = json.loads(line)\n                identifier = doc['finc.record_id']\n                if (identifier in updates):\n                    doc['x.labels'] = updates[identifier]\n                output.write((json.dumps(doc) + '\\n'))",
    "nl": "Keep a dictionary in memory with ids as keys and a list of ISILs as\nvalues.",
    "original_nl": "Keep a dictionary in memory with ids as keys and a list of ISILs as\nvalues."
  },
  {
    "code": "def release_seat_group(seat_group: SeatGroup) -> None:\n    if (not seat_group.is_occupied()):\n        raise ValueError('Seat group is not occupied.')\n    for ticket in seat_group.occupancy.ticket_bundle.tickets:\n        ticket.occupied_seat = None\n    db.session.delete(seat_group.occupancy)\n    db.session.commit()",
    "nl": "Release a seat group so it becomes available again.",
    "original_nl": "Release a seat group so it becomes available again."
  },
  {
    "code": "def test_tutorial_tested():\n    tutorial_test_file = os.path.join(os.path.dirname(__file__), 'test_tutorials.py')\n    f = open(tutorial_test_file, 'r')\n    tutorial_test_text = '\\n'.join(f.readlines())\n    tutorial_path = os.path.join(os.path.dirname(__file__), '..', '..', 'docs', 'tutorials')\n    tutorials = glob.glob(os.path.join(tutorial_path, '**', '*.md'))\n    tested_tutorials = set(re.findall(\"assert _test_tutorial_nb\\\\('(.*)'\\\\)\", tutorial_test_text))\n    for tutorial in tutorials:\n        friendly_name = '/'.join(tutorial.split('/')[(- 2):]).split('.')[0]\n        if ((friendly_name not in tested_tutorials) and ((friendly_name + '.md') not in whitelist_set)):\n            assert False, '{} has not been added to the tests/tutorials/test_tutorials.py test_suite'.format(friendly_name)",
    "nl": "Make sure that every tutorial that isn't in the whitelist\nhas been added to the tutorial test file",
    "original_nl": "Make sure that every tutorial that isn't in the whitelist\nhas been added to the tutorial test file"
  },
  {
    "code": "def bin_output(func):\n    func.argtypes = [GEOM_PTR, POINTER(c_size_t)]\n    func.errcheck = check_sized_string\n    return func",
    "nl": "Generates a prototype for the routines that return a a sized string.",
    "original_nl": "Generates a prototype for the routines that return a a sized string."
  },
  {
    "code": "def test_nameprepTrailingDot(self):\n    self.assertEqual(nameprep.prepare('example.com.'), 'example.com.')",
    "nl": "A trailing dot in domain names is preserved.",
    "original_nl": "A trailing dot in domain names is preserved."
  },
  {
    "code": "def get_database_version(self):\n    try:\n        cursor = self.connection.cursor()\n        cursor.execute('SELECT VERSION()')\n        version = cursor.fetchone()\n        print('Database version : {0} '.format(version))\n    except MySQLdb.Warning as e:\n        print(((Fore.YELLOW + 'Warning {0}: {1}'.format(e.args[0], e.args[1])) + Style.RESET_ALL))\n    except MySQLdb.Error as e:\n        print(((Fore.RED + 'Error {0}: {1}'.format(e.args[0], e.args[1])) + Style.RESET_ALL))\n        if (e.args[0] == 2003):\n            print(e)\n            return False\n        else:\n            sys.exit(1)\n    finally:\n        if cursor:\n            cursor.close\n    return True",
    "nl": "Method that returns the database version.\n\n",
    "original_nl": "Method that returns the database version.\n\n:return: The version of remote database, if it can connect with her, or else return False."
  },
  {
    "code": "@classmethod\ndef get_resources(cls):\n    attr_map = apidef.RESOURCE_ATTRIBUTE_MAP[apidef.COLLECTION_NAME]\n    collection_name = apidef.COLLECTION_NAME.replace('_', '-')\n    controller = base.create_resource(collection_name, apidef.RESOURCE_NAME, servicetype_db.ServiceTypeManager.get_instance(), attr_map)\n    return [extensions.ResourceExtension(collection_name, controller, attr_map=attr_map)]",
    "nl": "Returns Extended Resource for service type management.",
    "original_nl": "Returns Extended Resource for service type management."
  },
  {
    "code": "@view_config(context=APIPackageFileResource, request_method='DELETE', subpath=(), permission='write')\ndef delete_package(context, request):\n    package = request.db.fetch(context.filename)\n    if (package is None):\n        return HTTPBadRequest(('Could not find %s' % context.filename))\n    request.db.delete(package)\n    return request.response",
    "nl": "Delete a package",
    "original_nl": "Delete a package"
  },
  {
    "code": "def fernet_encrypt_psk(message, raw=False):\n    fernet = _get_fernet_context()\n    if isinstance(message, str):\n        message = message.encode('utf-8')\n    token = fernet.encrypt(message)\n    if (raw is True):\n        token = urlsafe_b64decode(token)\n    return token",
    "nl": "Encrypts the specified message using the Fernet format.\n",
    "original_nl": "Encrypts the specified message using the Fernet format.\n\nReturns the encrypted token, as a byte string.\n\nNote that a Fernet token includes the current time. Users decrypting a\nthe token can specify a TTL (in seconds) indicating how long the encrypted\nmessage should be valid. So the system clock must be correct before calling\nthis function.\n\n:param message: The message to encrypt.\n:type message: Must be of type 'bytes' or a UTF-8 'str'.\n:param raw: if True, returns the decoded base64 bytes representing the\n    Fernet token. The bytes must be converted back to base64 to be\n    decrypted. (Or the 'raw' argument on the corresponding\n    fernet_decrypt_psk() function can be used.)\n:return: the encryption token, as a base64-encoded byte string."
  },
  {
    "code": "def getProxyName(self):\n    return self.proxyName",
    "nl": "Get the Proxy name\n\n",
    "original_nl": "Get the Proxy name\n\n@return: the proxy name"
  },
  {
    "code": "@pytest.mark.django_db\ndef test_kolibri_listen_port_env(monkeypatch):\n    with patch('kolibri.content.utils.annotation.update_channel_metadata'):\n        from kolibri.utils import server\n\n        def start_mock(port, *args, **kwargs):\n            assert (port == test_port)\n        activate_log_logger(monkeypatch)\n        monkeypatch.setattr(server, 'start', start_mock)\n        test_port = 1234\n        os.environ['KOLIBRI_HTTP_PORT'] = str(test_port)\n        from kolibri.utils import conf\n        conf.OPTIONS.update(options.read_options_file(conf.KOLIBRI_HOME))\n        server.start = start_mock\n        cli.start(daemon=False)\n        with pytest.raises(SystemExit) as excinfo:\n            cli.stop()\n            assert (excinfo.code == 0)\n        with pytest.raises(SystemExit) as excinfo:\n            cli.stop()\n            assert (excinfo.code == 0)\n        assert ('Already stopped' in LOG_LOGGER[(- 1)][1])\n\n        def status_starting_up():\n            raise server.NotRunning(server.STATUS_STARTING_UP)\n        monkeypatch.setattr(server, 'get_status', status_starting_up)\n        with pytest.raises(SystemExit) as excinfo:\n            cli.stop()\n            assert (excinfo.code == server.STATUS_STARTING_UP)\n        assert ('Not stopped' in LOG_LOGGER[(- 1)][1])",
    "nl": "Starts and stops the server, mocking the actual server.start()\nChecks that the correct fallback port is used from the environment.",
    "original_nl": "Starts and stops the server, mocking the actual server.start()\nChecks that the correct fallback port is used from the environment."
  },
  {
    "code": "def __repr__(self):\n    s = '<MultiGraph: '\n    keys = sorted(self.__adjacency_list.keys())\n    for key in keys:\n        values = sorted(self.__adjacency_list[key].list())\n        s += (((('(' + repr(key)) + ': ') + ','.join(map(repr, values))) + ')')\n    return (s + '>')",
    "nl": "Returns an unique string representation of this graph.",
    "original_nl": "Returns an unique string representation of this graph."
  },
  {
    "code": "def get_supported_structures(self, corpname):\n    raise NotImplementedError()",
    "nl": "Return a list of structure names the plug-in\nand its data support for the 'corpname' corpus.\n\narguments:\ncorpname -- a corpus identifier\n",
    "original_nl": "Return a list of structure names the plug-in\nand its data support for the 'corpname' corpus.\n\narguments:\ncorpname -- a corpus identifier\n\nreturns:\na list of structures (e.g. ['doc', 'p'])"
  },
  {
    "code": "def paint_houses(n, m, cost):\n\n    def get_cost(i, j):\n        if (i in cost):\n            if (j in cost[i]):\n                return cost[i][j]\n        return 0\n    A = [([0] * m) for _ in range(n)]\n    B = [([None] * m) for _ in range(n)]\n    for i in range(1, n):\n        for j in range(0, m):\n            min_k = float('inf')\n            min_partial_cost = float('inf')\n            for k in range(0, m):\n                if (k == j):\n                    continue\n                partial_cost = (A[(i - 1)][k] + get_cost(k, j))\n                if (partial_cost < min_partial_cost):\n                    min_partial_cost = partial_cost\n                    min_k = k\n            A[i][j] = min_partial_cost\n            B[i][j] = min_k\n    (min_k, min_cost) = min(enumerate([A[(n - 1)][k] for k in range(0, m)]), key=(lambda t: t[1]))\n    colors = []\n    i = (n - 1)\n    while (i >= 0):\n        colors.append(min_k)\n        min_k = B[i][min_k]\n        i -= 1\n    colors.reverse()\n    return (min_cost, colors)",
    "nl": "Paint a list of N houses and M colors, each combination has cost,\nminimize the total cost without color in row.\n\nArgs\n    n - number of houses\n    m - number of colors\n    cost - cost matrix of each combination. Format {color1: {color2: cost}}\n",
    "original_nl": "Paint a list of N houses and M colors, each combination has cost,\nminimize the total cost without color in row.\n\nArgs\n    n - number of houses\n    m - number of colors\n    cost - cost matrix of each combination. Format {color1: {color2: cost}}\n\nReturns:\n    (min_cost, colors), min_cost - the total min cost of the coloring.\n                        colors - the order of the coloring."
  },
  {
    "code": "def get_function_by_ifname(ifname):\n    try:\n        dev_path = ('/sys/class/net/%s/device' % ifname)\n        dev_info = os.listdir(dev_path)\n        for dev_file in dev_info:\n            if _VIRTFN_RE.match(dev_file):\n                return (os.readlink(dev_path).strip('./'), True)\n        else:\n            return (os.readlink(dev_path).strip('./'), False)\n    except Exception:\n        LOG.error((_LE('PCI device %s not found') % ifname))\n        return (None, False)",
    "nl": "Given the device name, returns the PCI address of a an device\nand returns True if the address in a physical function.",
    "original_nl": "Given the device name, returns the PCI address of a an device\nand returns True if the address in a physical function."
  },
  {
    "code": "def empty(self):\n    self._tokens = float(0)\n    return True",
    "nl": "Sets the current token count to zero",
    "original_nl": "Sets the current token count to zero"
  },
  {
    "code": "def im_resize(maxwidth, path_in, path_out=None):\n    path_out = (path_out or temp_file_for(path_in))\n    log.debug('artresizer: ImageMagick resizing {0} to {1}', util.displayable_path(path_in), util.displayable_path(path_out))\n    try:\n        util.command_output([b'convert', util.syspath(path_in, prefix=False), b'-resize', b'{0}x^>'.format(maxwidth), util.syspath(path_out, prefix=False)])\n    except subprocess.CalledProcessError:\n        log.warn('artresizer: IM convert failed for {0}', util.displayable_path(path_in))\n        return path_in\n    return path_out",
    "nl": "Resize using ImageMagick's ``convert`` tool.",
    "original_nl": "Resize using ImageMagick's ``convert`` tool.\nReturn the output path of resized image."
  },
  {
    "code": "def concave(n, coef):\n    return convexity_(n, coef, convex=False)",
    "nl": "Builds a penalty matrix for P-Splines with continuous features.\nPenalizes violation of a concave feature function.\n",
    "original_nl": "Builds a penalty matrix for P-Splines with continuous features.\nPenalizes violation of a concave feature function.\n\nParameters\n----------\nn : int\n    number of splines\ncoef : array-like\n    coefficients of the feature function\n\nReturns\n-------\npenalty matrix : sparse csc matrix of shape (n,n)"
  },
  {
    "code": "def startSuite(name):\n    pass",
    "nl": "Deprecated in Twisted 8.0.\n\nSuites which wish to appear in reporter output should call this\nbefore running their tests.",
    "original_nl": "Deprecated in Twisted 8.0.\n\nSuites which wish to appear in reporter output should call this\nbefore running their tests."
  },
  {
    "code": "def get_areas_for_party(party_id: PartyID) -> Area:\n    return Area.query.for_party_id(party_id).all()",
    "nl": "Return all areas for that party.",
    "original_nl": "Return all areas for that party."
  },
  {
    "code": "@property\ndef raw(self):\n    return self._raw_specs",
    "nl": "The raw port specifications that created this PortSpec.",
    "original_nl": "The raw port specifications that created this PortSpec."
  },
  {
    "code": "def create_manager(self):\n    self.job_queue = multiprocessing.JoinableQueue()\n    self.result_queue = multiprocessing.JoinableQueue()\n    self.todo_counter = multiprocessing.Manager().Value('i', (- 1))\n    self.work_done_flag = multiprocessing.Manager().Event()\n    self.work_done_flag.clear()\n\n    class ServerQueueManager(multiprocessing.managers.SyncManager):\n        pass\n    ServerQueueManager.register('get_job_queue')\n    ServerQueueManager.register('get_result_queue')\n    ServerQueueManager.register('get_todo_counter')\n    ServerQueueManager.register('get_work_done_event')\n    self.manager = ServerQueueManager(address=(self.server_address, self.port), authkey=self.key)\n    self.manager.connect()\n    self.job_queue = self.manager.get_job_queue()\n    self.result_queue = self.manager.get_result_queue()\n    self.todo_counter = self.manager.get_todo_counter()\n    self.work_done_flag = self.manager.get_work_done_event()\n    return self.manager",
    "nl": "creates a multiprocessing.SyncManager",
    "original_nl": "creates a multiprocessing.SyncManager"
  },
  {
    "code": "def guard(exceptions, suppress=False, ignore_ready=False):\n\n    def guard_decorator(func):\n        ' Decorator to handle raised exceptions for drivers.\\n\\n        :param func: Function to guard.\\n        :type func: callable\\n        :returns: Function wrapper.\\n        :rtype: callable\\n        '\n\n        def wrapper(*args, **kwargs):\n            ' Wrap function and handle raised exceptions for drivers. '\n            self = args[0]\n            if ((not self._ready) and (not ignore_ready)):\n                if (not suppress):\n                    raise DriverError('Driver not ready')\n                return\n            try:\n                return func(*args, **kwargs)\n            except exceptions as err:\n                self._log.error(str(err))\n                self._on_error(err)\n                if (not suppress):\n                    raise DriverError(err)\n        return wrapper\n    return guard_decorator",
    "nl": "Create decorator to handle raised exceptions for drivers.\n\n",
    "original_nl": "Create decorator to handle raised exceptions for drivers.\n\n:param exceptions: List of exceptions to catch.\n:type exceptions: tuple\n:param suppress: Don't reraise exception if True.\n:type suppress: bool\n:param ignore_ready: Ignore ready state of driver.\n:type ignore_ready: bool\n:returns: Created decorator\n:rtype: callable"
  },
  {
    "code": "def __init__(self, configuration):\n    self._libvirt_version = None\n    self._driver_version = None\n    self.configuration = configuration\n    self.local_libvirt_uri = self.configuration.get('GLOBAL', 'libvirt_uri')\n    self.libvirt_connection = None\n    if self.configuration.has_option('GLOBAL', 'libvirt_need_authentication'):\n        self.need_auth = self.configuration.getboolean('GLOBAL', 'libvirt_need_authentication')\n    else:\n        self.need_auth = None",
    "nl": "Initialize the TNArchipelLibvirtEntity.",
    "original_nl": "Initialize the TNArchipelLibvirtEntity."
  },
  {
    "code": "def copy(self):\n    self.log.copy()",
    "nl": "Copy selected text from the log.",
    "original_nl": "Copy selected text from the log."
  },
  {
    "code": "def get_recommended_tbb_version():\n    tbb_versions_url = 'https://www.torproject.org/projects/torbrowser/RecommendedTBBVersions'\n    versions = ut.read_url(tbb_versions_url)\n    for line in versions.split():\n        if ('Linux' in line):\n            return line.split('-')[0].lstrip('\"')\n    raise cm.TBBGetRecommendedVersionError()",
    "nl": "Get the recommended TBB version from RecommendedTBBVersions file.",
    "original_nl": "Get the recommended TBB version from RecommendedTBBVersions file."
  },
  {
    "code": "def __str__(self):\n    st = '{} whose path is \"{}\" and group is \"{}\" declaring :\\n{}'\n    return st.format(type(self).__name__, self.path, self.group, '\\n'.join((' - {}'.format(c) for c in self.children)))",
    "nl": "Identify the declarator by its path and group.",
    "original_nl": "Identify the declarator by its path and group."
  },
  {
    "code": "def run_migrations_online():\n    set_mysql_engine()\n    connection = config.attributes.get('connection')\n    with DBConnection(neutron_config.database.connection, connection) as conn:\n        context.configure(connection=conn, target_metadata=target_metadata, include_object=include_object, process_revision_directives=autogen.process_revision_directives)\n        with context.begin_transaction():\n            context.run_migrations()",
    "nl": "Run migrations in 'online' mode.\n\nIn this scenario we need to create an Engine\nand associate a connection with the context.",
    "original_nl": "Run migrations in 'online' mode.\n\nIn this scenario we need to create an Engine\nand associate a connection with the context."
  },
  {
    "code": "def size(self):\n    return QSize(self.width, self.height)",
    "nl": "Return our size (QSize).",
    "original_nl": "Return our size (QSize)."
  },
  {
    "code": "def rename_preset(old_name, new_name):\n    os.rename(preset_path(old_name), preset_path(new_name))",
    "nl": "Rename an existing preset, if the new name is not already used.\n\n",
    "original_nl": "Rename an existing preset, if the new name is not already used.\n\n:param old_name: The preset (old) name\n:param new_name: The new preset name\n:return: True if the preset as been renamed successfully, False otherwise\n:rtype: bool"
  },
  {
    "code": "def set_working(self, state, insensitive=False):\n    self.is_working = state",
    "nl": "Set the working state.",
    "original_nl": "Set the working state."
  },
  {
    "code": "def finish(self):\n    self.cov.stop()\n    self.cov.save()\n    self.cov = self.combining_cov\n    self.cov.load()\n    self.cov.combine()\n    self.cov.save()",
    "nl": "Combines coverage data and sets the list of coverage objects to report on.",
    "original_nl": "Combines coverage data and sets the list of coverage objects to report on."
  },
  {
    "code": "def timerfd_gettime(fd):\n    if hasattr(fd, 'fileno'):\n        fd = fd.fileno()\n    assert isinstance(fd, int), 'fd must be an integer'\n    curr_val = ffi.new('struct itimerspec *')\n    ret = C.timerfd_gettime(fd, curr_val)\n    if (ret < 0):\n        err = ffi.errno\n        if (err == errno.EBADF):\n            raise ValueError('fd is not a valid file descriptor')\n        elif (err == errno.EFAULT):\n            raise InternalError('curr_val is not a valid pointer (internal/bug, let us know)')\n        elif (err == errno.EINVAL):\n            raise ValueError('fd is not a valid timerfd')\n        else:\n            raise UnknownError(err)\n    curr_val = TimerVal(timerspec=curr_val)\n    return curr_val",
    "nl": "Get the current expiry time of a timerfd\n\nArguments\n----------\n",
    "original_nl": "Get the current expiry time of a timerfd\n\nArguments\n----------\n:param int fd: File descriptor representing the timerfd\n\nReturns\n--------\n:return: The file descriptor representing the timerfd\n:rtype: int\n\nExceptions\n-----------\n:raises ValueError: Invalid value in flags\n:raises OSError: Max per process FD limit reached\n:raises OSError: Max system FD limit reached\n:raises OSError: Could not mount (internal) anonymous inode device\n:raises MemoryError: Insufficient kernel memory"
  },
  {
    "code": "def test_chunks(self):\n    q = self.queue()\n    for x in range(5):\n        q.push(str(x))\n    chunks = glob.glob(os.path.join(self.qdir, 'q*'))\n    self.assertEqual(len(chunks), ((5 / self.chunksize) + 1))\n    for x in range(5):\n        q.pop()\n    chunks = glob.glob(os.path.join(self.qdir, 'q*'))\n    self.assertEqual(len(chunks), 1)",
    "nl": "Test chunks are created and removed",
    "original_nl": "Test chunks are created and removed"
  },
  {
    "code": "def setup_logger(logger):\n    cfy_config = config.instance\n    additional_log_handlers = [RotatingFileHandler(filename=cfy_config.rest_service_log_path, maxBytes=((cfy_config.rest_service_log_file_size_MB * 1024) * 1024), backupCount=cfy_config.rest_service_log_files_backup_count)]\n    _setup_python_logger(logger=logger, logger_level=cfy_config.rest_service_log_level, handlers=additional_log_handlers, remove_existing_handlers=False)\n    for w in cfy_config.warnings:\n        logger.warning(w)",
    "nl": "Setup the Flask app's logger\n\n",
    "original_nl": "Setup the Flask app's logger\n\n:param logger: Flask app's logger"
  },
  {
    "code": "def get_days_and_hour_totals(hour_ranges: Sequence[DateTimeRange]) -> Iterator[Tuple[(date, int)]]:\n\n    def get_date(dt_range: DateTimeRange) -> date:\n        return dt_range.start.date()\n    for (day, hour_ranges_for_day) in groupby(hour_ranges, key=get_date):\n        hour_total = len(list(hour_ranges_for_day))\n        (yield (day, hour_total))",
    "nl": "Yield (day, relevant hours total) pairs.",
    "original_nl": "Yield (day, relevant hours total) pairs."
  },
  {
    "code": "def feedForward(self, inputs):\n    if (len(inputs) != (self.input - 1)):\n        raise ValueError('Wrong number of inputs you silly goose!')\n    for i in range((self.input - 1)):\n        self.ai[i] = inputs[i]\n    for j in range(self.hidden):\n        sum = 0.0\n        for i in range(self.input):\n            sum += (self.ai[i] * self.wi[i][j])\n        self.ah[j] = tanh(sum)\n    for k in range(self.output):\n        sum = 0.0\n        for j in range(self.hidden):\n            sum += (self.ah[j] * self.wo[j][k])\n        self.ao[k] = sigmoid(sum)\n    return self.ao[:]",
    "nl": "The feedforward algorithm loops over all the nodes in the hidden layer and\nadds together all the outputs from the input layer * their weights\nthe output of each node is the sigmoid function of the sum of all inputs\nwhich is then passed on to the next layer.\n",
    "original_nl": "The feedforward algorithm loops over all the nodes in the hidden layer and\nadds together all the outputs from the input layer * their weights\nthe output of each node is the sigmoid function of the sum of all inputs\nwhich is then passed on to the next layer.\n:param inputs: input data\n:return: updated activation output vector"
  },
  {
    "code": "@command.skill_level('administrator')\n@command.completers(completers.call(corosync.get_all_paths))\ndef do_get(self, context, path):\n    for v in corosync.get_values(path):\n        print(v)",
    "nl": "Get a corosync configuration value",
    "original_nl": "Get a corosync configuration value"
  },
  {
    "code": "def test_get_data_dirs(self):\n    bad_sep = str(filter((lambda x: (x not in os.pathsep)), ':;'))\n    dir_list = ['A', 'B', bad_sep, 'C']\n    self.tweak_env('XDG_DATA_DIRS', os.pathsep.join(dir_list))\n    dirs = basedir.get_xdg_data_dirs()[1:]\n    self.assertEqual(dirs, [x.encode('utf-8') for x in dir_list])",
    "nl": "Check thet get_data_dirs uses pathsep correctly.",
    "original_nl": "Check thet get_data_dirs uses pathsep correctly."
  },
  {
    "code": "@property\ndef printer(self):\n    return self._printer",
    "nl": "Returns the underlying printer where logs are printed to.",
    "original_nl": "Returns the underlying printer where logs are printed to."
  },
  {
    "code": "def iterate(self, debug):\n    pass",
    "nl": "Perform a single cycle of execution.\n\n",
    "original_nl": "Perform a single cycle of execution.\n\n:param debug: whether to print out information about internal state"
  },
  {
    "code": "def click_export(self):\n    self.q(css='a.action-export').click()",
    "nl": "Click the export button.",
    "original_nl": "Click the export button."
  },
  {
    "code": "def get_precision(self):\n    check_is_fitted(self, 'components_')\n    n_features = self.components_.shape[1]\n    if (self.n_components == 0):\n        return np.diag((1.0 / self.noise_variance_))\n    if (self.n_components == n_features):\n        return linalg.inv(self.get_covariance())\n    components_ = self.components_\n    precision = np.dot((components_ / self.noise_variance_), components_.T)\n    precision.flat[::(len(precision) + 1)] += 1.0\n    precision = np.dot(components_.T, np.dot(linalg.inv(precision), components_))\n    precision /= self.noise_variance_[:, np.newaxis]\n    precision /= (- self.noise_variance_[np.newaxis, :])\n    precision.flat[::(len(precision) + 1)] += (1.0 / self.noise_variance_)\n    return precision",
    "nl": "Compute data precision matrix with the FactorAnalysis model.\n",
    "original_nl": "Compute data precision matrix with the FactorAnalysis model.\n\nReturns\n-------\nprecision : array, shape (n_features, n_features)\n    Estimated precision of data."
  },
  {
    "code": "def accessmanager():\n    global _accessmanager\n    try:\n        _accessmanager\n    except NameError:\n        _accessmanager = NetworkAccessManager()\n    return _accessmanager",
    "nl": "Returns a global NetworkAccessManager.",
    "original_nl": "Returns a global NetworkAccessManager."
  },
  {
    "code": "def test_001_name_exact(self):\n    q = RoleQuery(self.p, name='test1')\n    roles = sorted((str(r) for r in q.results()))\n    self.assertListEqual(['test1'], roles)",
    "nl": "Role query with exact name match.",
    "original_nl": "Role query with exact name match."
  },
  {
    "code": "@abc.abstractmethod\ndef is_polling_enabled(self):\n    pass",
    "nl": "Check if the polling monitor is enabled.",
    "original_nl": "Check if the polling monitor is enabled."
  },
  {
    "code": "def _validate_enough_space(self):\n    if (self.partition_table is not None):\n        available_size = self.partition_table.get_available_size(ignore_partitions=[self])\n        if (available_size < self.size):\n            adjusted_size = (self.size - self.get_block_size())\n            if (available_size < adjusted_size):\n                if (self.id is not None):\n                    raise ValidationError({\n                        'size': [('Partition %s cannot be resized to fit on the block device; not enough free space.' % self.id)],\n                    })\n                else:\n                    raise ValidationError({\n                        'size': ['Partition cannot be saved; not enough free space on the block device.'],\n                    })\n            else:\n                self.size = adjusted_size\n        if ((self.partition_table.table_type == PARTITION_TABLE_TYPE.MBR) and (self.size > get_max_mbr_partition_size())):\n            if (self.id is not None):\n                raise ValidationError({\n                    'size': [('Partition %s cannot be resized to fit on the block device; size is larger than the MBR 2TiB maximum.' % self.id)],\n                })\n            else:\n                raise ValidationError({\n                    'size': ['Partition cannot be saved; size is larger than the MBR 2TiB maximum.'],\n                })",
    "nl": "Validate that the partition table has enough space for this\npartition.",
    "original_nl": "Validate that the partition table has enough space for this\npartition."
  },
  {
    "code": "def setUp(self):\n    Timings.Defaults()\n    self.app = Application()\n    self.app.start('Notepad.exe')\n    self.dlg = self.app.Notepad",
    "nl": "Set some data and ensure the application is in the state we want",
    "original_nl": "Set some data and ensure the application is in the state we want"
  },
  {
    "code": "def test_get_ipc_map_1():\n    files = None\n    my_dir = os.path.dirname(__file__)\n    with open(os.path.join(my_dir, 'lsof-test-output-linux-1.txt'), 'r') as lsof_output:\n        files = px_file.lsof_to_files(lsof_output.read(), None, None)\n    ipc_map = testutils.create_ipc_map(1997, files)\n    keys = list(ipc_map.keys())\n    assert (len(keys) == 2)\n    peer0 = keys[0]\n    assert (len(ipc_map[peer0]) == 1)\n    peer1 = keys[1]\n    assert (len(ipc_map[peer1]) == 1)",
    "nl": "Tyre kick IpcMap with some real world data",
    "original_nl": "Tyre kick IpcMap with some real world data"
  },
  {
    "code": "def get_magnet(self, detailed=True):\n    result = ('magnet:?xt=urn:btih:' + self.info_hash)\n\n    def add_tr():\n        urls = self.announce_urls\n        if (not urls):\n            return\n        trackers = []\n        urls = urls[0]\n        for url in urls:\n            trackers.append(('tr', url))\n        if trackers:\n            return urlencode(trackers)\n\n    def add_ws():\n        webseeds = [('ws', url) for url in self.webseeds]\n        if webseeds:\n            return urlencode(webseeds)\n    params_map = {\n        'tr': add_tr,\n        'ws': add_ws,\n    }\n    if detailed:\n        details = []\n        if isinstance(detailed, _ITERABLE_TYPES):\n            requested_params = detailed\n        else:\n            requested_params = params_map.keys()\n        for param in requested_params:\n            param_val = params_map[param]()\n            (param_val and details.append(param_val))\n        if details:\n            result += ('&%s' % '&'.join(details))\n    return result",
    "nl": "Returns torrent magnet link, consisting of BTIH (BitTorrent Info Hash) URN\nanr optional other information.\n\n",
    "original_nl": "Returns torrent magnet link, consisting of BTIH (BitTorrent Info Hash) URN\nanr optional other information.\n\n:param bool|list|tuple|set detailed:\n    For boolean - whether additional info (such as trackers) should be included.\n    For iterable - expected allowed parameter names:\n        tr - trackers\n        ws - webseeds"
  },
  {
    "code": "def uniques(self, collapse=False):\n    uniques = set()\n    for (name, view) in self:\n        for sv in view.uniques(collapse=collapse):\n            uniques.add(sv)\n    return uniques",
    "nl": "Return the set of unique BrainData objects contained by this dataset",
    "original_nl": "Return the set of unique BrainData objects contained by this dataset"
  },
  {
    "code": "def gateway_connect(self, netaddress):\n    self.http_post(('/gateway/connect/' + netaddress))",
    "nl": "Connects the gateway to a peer.",
    "original_nl": "Connects the gateway to a peer."
  },
  {
    "code": "def sync_remotes(self, force=False):\n    connectors = juicer.utils.get_login_info()[0]\n    for (repo, items) in self.iterrepos():\n        repoid = ('%s-%s' % (repo, self.current_env))\n        for rpm in items:\n            if ((not rpm.path.startswith(juicer.utils.pulp_repo_path(connectors[self.current_env], repoid))) or force):\n                rpm.sync_to(self.remotes_storage)\n            else:\n                juicer.utils.Log.log_debug((\"Not syncing %s because it's already in pulp\" % rpm.path))",
    "nl": "Pull down all non-local items and save them into remotes_storage.",
    "original_nl": "Pull down all non-local items and save them into remotes_storage."
  },
  {"END_OF_MANUAL": null},
  {
    "code": "@abc.abstractmethod\ndef verify(self, query_response):\n    pass",
    "nl": "Verify whether the cache key maches a piece of service discovery\ninformation.\n\n",
    "original_nl": "Verify whether the cache key maches a piece of service discovery\ninformation.\n\n:param query_response: The full :xep:`30` disco#info query response.\n:type query_response: :class:`~.disco.xso.InfoQuery`\n:rtype: :class:`bool`\n:return: true if the key matches and false otherwise."
  },
  {
    "code": "def retrieve(self, time, otherchunk, actrvariables, buffers, extra_tests, model_parameters):\n    model_parameters = model_parameters.copy()\n    model_parameters.update(self.model_parameters)\n    if (actrvariables == None):\n        actrvariables = {\n            \n        }\n    try:\n        mod_attr_val = {x[0]: utilities.check_bound_vars(actrvariables, x[1], negative_impossible=False) for x in otherchunk.removeunused()}\n    except utilities.ACTRError as arg:\n        raise utilities.ACTRError((\"Retrieving the chunk '%s' is impossible; %s\" % (otherchunk, arg)))\n    chunk_tobe_matched = chunks.Chunk(otherchunk.typename, **mod_attr_val)\n    max_A = float('-inf')\n    retrieved = None\n    for chunk in self.dm:\n        try:\n            if ((extra_tests['recently_retrieved'] == False) or (extra_tests['recently_retrieved'] == 'False')):\n                if (self.__finst and (chunk in self.recent)):\n                    continue\n            elif (self.__finst and (chunk not in self.recent)):\n                continue\n        except KeyError:\n            pass\n        if model_parameters['subsymbolic']:\n            A_pm = 0\n            if model_parameters['partial_matching']:\n                A_pm = chunk_tobe_matched.match(chunk, partialmatching=True, mismatch_penalty=model_parameters['mismatch_penalty'])\n            elif (not (chunk_tobe_matched <= chunk)):\n                continue\n            try:\n                A_bll = utilities.baselevel_learning(time, self.dm[chunk], model_parameters['baselevel_learning'], model_parameters['decay'], self.dm.activations.get(chunk), optimized_learning=model_parameters['optimized_learning'])\n            except UnboundLocalError:\n                continue\n            if math.isnan(A_bll):\n                raise utilities.ACTRError(('The following chunk cannot receive base activation: %s. The reason is that one of its traces did not appear in a past moment.' % chunk))\n            A_sa = utilities.spreading_activation(chunk, buffers, self.dm, model_parameters['buffer_spreading_activation'], model_parameters['strength_of_association'], model_parameters['spreading_activation_restricted'], model_parameters['association_only_from_chunks'])\n            inst_noise = utilities.calculate_instantanoues_noise(model_parameters['instantaneous_noise'])\n            A = (((A_bll + A_sa) + A_pm) + inst_noise)\n            if (utilities.retrieval_success(A, model_parameters['retrieval_threshold']) and (max_A < A)):\n                max_A = A\n                self.activation = max_A\n                retrieved = chunk\n                extra_time = utilities.retrieval_latency(A, model_parameters['latency_factor'], model_parameters['latency_exponent'])\n                if model_parameters['activation_trace']:\n                    print('(Partially) matching chunk:', chunk)\n                    print('Base level learning:', A_bll)\n                    print('Spreading activation', A_sa)\n                    print('Partial matching', A_pm)\n                    print('Noise:', inst_noise)\n                    print('Total activation', A)\n                    print('Time to retrieve', extra_time)\n        elif ((chunk_tobe_matched <= chunk) and (self.dm[chunk][0] != time)):\n            retrieved = chunk\n            extra_time = model_parameters['rule_firing']\n    if (not retrieved):\n        if model_parameters['subsymbolic']:\n            extra_time = utilities.retrieval_latency(model_parameters['retrieval_threshold'], model_parameters['latency_factor'], model_parameters['latency_exponent'])\n        else:\n            extra_time = model_parameters['rule_firing']\n    if self.__finst:\n        self.recent.append(retrieved)\n        if (self.__finst < len(self.recent)):\n            self.recent.popleft()\n    return (retrieved, extra_time)",
    "nl": "Retrieve a chunk from declarative memory that matches otherchunk.",
    "original_nl": "Retrieve a chunk from declarative memory that matches otherchunk."
  },
  {
    "code": "def _OnGet(self, node):\n    self._AdjustAperture(1)",
    "nl": "Invoked by the parent class when a node has been retrieved from the pool\nand is about to be used.\nIncreases the load average of the pool, and adjust the aperture if needed.",
    "original_nl": "Invoked by the parent class when a node has been retrieved from the pool\nand is about to be used.\nIncreases the load average of the pool, and adjust the aperture if needed."
  },
  {
    "code": "def get_absolute_url(self, **kwargs):\n    if self.is_gift_card:\n        return url_for('product.product.render_gift_card', uri=self.uri, **kwargs)\n    return super(Product, self).get_absolute_url(**kwargs)",
    "nl": "Return gift card URL if product is a gift card",
    "original_nl": "Return gift card URL if product is a gift card"
  },
  {
    "code": "def addFacesGivenBinary(stlData, triangleMesh, vertexIndexTable):\n    numberOfVertexes = ((len(stlData) - 84) / 50)\n    vertexes = []\n    for vertexIndex in xrange(numberOfVertexes):\n        byteIndex = (84 + (vertexIndex * 50))\n        vertexes.append(getVertexGivenBinary((byteIndex + 12), stlData))\n        vertexes.append(getVertexGivenBinary((byteIndex + 24), stlData))\n        vertexes.append(getVertexGivenBinary((byteIndex + 36), stlData))\n    addFacesGivenVertexes(triangleMesh, vertexIndexTable, vertexes)",
    "nl": "Add faces given stl binary.",
    "original_nl": "Add faces given stl binary."
  },
  {
    "code": "def wrap(ob, iid=None, usePolicy=None, useDispatcher=None):\n    if (usePolicy is None):\n        usePolicy = policy.DefaultPolicy\n    if (useDispatcher == 1):\n        import win32com.server.dispatcher\n        useDispatcher = win32com.server.dispatcher.DefaultDebugDispatcher\n    if ((useDispatcher is None) or (useDispatcher == 0)):\n        ob = usePolicy(ob)\n    else:\n        ob = useDispatcher(usePolicy, ob)\n    ob = pythoncom.WrapObject(ob)\n    if (iid is not None):\n        ob = ob.QueryInterface(iid)\n    return ob",
    "nl": "Wraps an object in a PyGDispatch gateway.\n",
    "original_nl": "Wraps an object in a PyGDispatch gateway.\n\nReturns a client side PyI{iid} interface.\n\nInterface and gateway support must exist for the specified IID, as\nthe QueryInterface() method is used."
  },
  {
    "code": "def add_xml_filter(self, callback):\n    self.xml_filters.append(callback)\n    return self",
    "nl": "Add an xml filter for in tree post processing",
    "original_nl": "Add an xml filter for in tree post processing"
  },
  {
    "code": "@invoke.task(name='deps-compile')\ndef deps_compile(ctx):\n    env = create_env('tools', requirements=True)\n    for file in glob.glob(os.path.join(BASE, 'requirements-*.in')):\n        invoke.run(('%s/bin/pip-compile %s > %s' % (env, os.path.abspath(file), (os.path.abspath(file)[:(- 3)] + '.txt'))))",
    "nl": "Compile new requirements-*.txt",
    "original_nl": "Compile new requirements-*.txt"
  },
  {
    "code": "def get_id(self, conn_looker=None):\n    account_name = 'None'\n    player_name = 'None'\n    if self.account:\n        account_name = self.account.name\n    if self.player:\n        player_name = self.player.code\n    if ((not conn_looker) or (conn_looker and conn_looker.account and (conn_looker.account.trust >= TRUST.IMPLEMENTOR))):\n        return ('%s %s.%s' % (self.ip, account_name, player_name))\n    else:\n        (ip_number, port) = self.ip.split(':')\n        return ('*.*.*.%s:%s %s.%s' % (ip_number.split('.')[3], port, account_name, player_name))",
    "nl": "Ritorna una o tutte tra le seguenti informazioni: l'ip della\nconnessione, il nome dell'account e il nome del personaggio.\nMolto utile da utilizzare nei messaggi di log.\nQuesto metodo fa a coppia con quello nella classe Account.",
    "original_nl": "Ritorna una o tutte tra le seguenti informazioni: l'ip della\nconnessione, il nome dell'account e il nome del personaggio.\nMolto utile da utilizzare nei messaggi di log.\nQuesto metodo fa a coppia con quello nella classe Account."
  },
  {
    "code": "def __new__(cls, distro_type, **kwargs):\n    kwargs.update({\n        'distro': distro_type,\n        'pkgsys': LinuxLocalInfo.PACKAGE_SYSTEMS[distro_type],\n        'installation': 'local',\n    })\n    return DistroInfo.__new__(cls, create_func=create, get_func=container_for_directory, match_func=match, enumerate_func=enumerate_all, kwargs=kwargs)",
    "nl": "Create DistroInfo tuple using provided arguments.",
    "original_nl": "Create DistroInfo tuple using provided arguments."
  },
  {
    "code": "def glob_to_path_regex(glob):\n    canonical_glob = str(PurePosixPath(glob))\n    regex = '^'\n    components = canonical_glob.split('/')\n    for (i, component) in enumerate(components):\n        if (component == '**'):\n            if (i == (len(components) - 1)):\n                raise GlobError(glob, '** may not be the last component in a path.')\n            else:\n                regex += '(?:[^/]+/)*'\n        elif ('**' in component):\n            raise GlobError(glob, '** must be an entire path component.')\n        else:\n            if (component == '*'):\n                regex += '[^/]+'\n            else:\n                star_parts = split_on_stars_interpreting_backslashes(component)\n                escaped_parts = map(re.escape, star_parts)\n                regex += '[^/]*'.join(escaped_parts)\n            if (i < (len(components) - 1)):\n                regex += '/'\n    regex += '$'\n    return regex",
    "nl": "Supports * and **. Backslashes can escape stars or other backslashes. As\nin pathlib, ** may not adjoin any characters other than slash. Unlike\npathlib, because we're not talking to the actual filesystem, ** will match\nfiles as well as directories. Paths get canonicalized before they're\nconverted, so duplicate and trailing slashes get dropped. You should make\nsure the other paths you try to match are in canonical (Posix) form as\nwell.",
    "original_nl": "Supports * and **. Backslashes can escape stars or other backslashes. As\nin pathlib, ** may not adjoin any characters other than slash. Unlike\npathlib, because we're not talking to the actual filesystem, ** will match\nfiles as well as directories. Paths get canonicalized before they're\nconverted, so duplicate and trailing slashes get dropped. You should make\nsure the other paths you try to match are in canonical (Posix) form as\nwell."
  },
  {
    "code": "def pitman_yor(min_alpha=0.1, max_alpha=100, min_d=0, max_d=0.5, alpha_count=20, d_count=10):\n    min_alpha = float(min_alpha)\n    max_alpha = float(max_alpha)\n    min_d = float(min_d)\n    max_d = float(max_d)\n    lower_triangle = [(x, y) for x in center_heavy(0, 1, alpha_count) for y in left_heavy(0, 1, d_count) if ((x + y) < 1)]\n    alpha = (lambda x: (min_alpha * ((max_alpha / min_alpha) ** x)))\n    d = (lambda y: (min_d + ((max_d - min_d) * y)))\n    grid = [{\n        'alpha': alpha(x),\n        'd': d(y),\n    } for (x, y) in lower_triangle]\n    return grid",
    "nl": "For d = 0, this degenerates to the CRP, where the expected number of\ntables is:\n    E[table_count] = O(alpha log(customer_count))",
    "original_nl": "For d = 0, this degenerates to the CRP, where the expected number of\ntables is:\n    E[table_count] = O(alpha log(customer_count))"
  },
  {
    "code": "def make_gecos_field(fullname=None, room=None, worktel=None, hometel=None, other=None):\n    fields = (fullname, room, worktel, hometel, other)\n\n    def clean(string):\n        if (string is None):\n            return ''\n        else:\n            return string.replace(',', '_').replace(':', '_').encode('ascii', 'replace').decode('ascii').strip()\n    return ','.join(map(clean, fields))",
    "nl": "Construct a GECOS field.\n\nBased on a reading of chfn(1).\n\nAll strings passed in will be coerced to US-ASCII, replacing non-ASCII\ncharacters with question marks. Colons and commas will be replaced with\nunderscores, and leading and trailing whitespace is stripped.\n\n",
    "original_nl": "Construct a GECOS field.\n\nBased on a reading of chfn(1).\n\nAll strings passed in will be coerced to US-ASCII, replacing non-ASCII\ncharacters with question marks. Colons and commas will be replaced with\nunderscores, and leading and trailing whitespace is stripped.\n\n:param fullname: The user's full name.\n:param room: The user's room number.\n:param worktel: The user's work telephone number.\n:param hometel: The user's home telephone number.\n:param other: Other useful information.\n\n:return: A string suitable for use as the GECOS field in ``/etc/passwd``."
  },
  {
    "code": "def summarize_activations(name_filter=None, summarizer=summarize_activation):\n    return summarize_collection(ops.GraphKeys.ACTIVATIONS, name_filter, summarizer)",
    "nl": "Summarize activations, using `summarize_activation` to summarize.",
    "original_nl": "Summarize activations, using `summarize_activation` to summarize."
  },
  {
    "code": "def _resolve_serializer(self, serializer):\n    if isinstance(serializer, Serializer):\n        return serializer\n    if (serializer in self._serializers):\n        return self._serializers[serializer]\n    raise RuntimeError('Unsupported serializer')",
    "nl": "Resolve the given serializer.\n\n",
    "original_nl": "Resolve the given serializer.\n\n:param serializer: The serializer to resolve\n:type serializer: str or Serializer\n\n:rtype: Serializer"
  },
  {
    "code": "def to_dict(self) -> Dict:\n    return {\n        'name': self.name,\n        'count': self.count,\n        'values': [(val if (val != 24) else 0) for val in self.values],\n        'img': self.img_src,\n        'id': self.id,\n    }",
    "nl": "Returns Card as dictionary",
    "original_nl": "Returns Card as dictionary"
  },
  {
    "code": "@abstractmethod\ndef otp_validate(self, form, field):\n    raise NotImplementedError",
    "nl": "This should call the appropriate OTP validation method.\n\n",
    "original_nl": "This should call the appropriate OTP validation method.\n\n:return: :data:`True` on success\n:raises: :class:`OATHError` on failure"
  },
  {
    "code": "def writeNetwork(self, net, netroot):\n    netroot.setAttribute('name', net.name)\n    netroot.setAttribute('class', canonicClassString(net))\n    if net.argdict:\n        self.writeArgs(netroot, net.argdict)\n    mods = self.newChild(netroot, 'Modules')\n    for im in net.inmodules:\n        self.writeModule(mods, im, True, (im in net.outmodules))\n    for om in net.outmodules:\n        if (om not in net.inmodules):\n            self.writeModule(mods, om, False, True)\n    for m in net.modulesSorted:\n        if ((m not in net.inmodules) and (m not in net.outmodules)):\n            self.writeModule(mods, m, False, False)\n    if (len(net.motherconnections) > 0):\n        mothers = self.newChild(netroot, 'MotherConnections')\n        for m in net.motherconnections:\n            self.writeBuildable(mothers, m)\n    conns = self.newChild(netroot, 'Connections')\n    for m in net.modulesSorted:\n        for c in net.connections[m]:\n            self.writeConnection(conns, c, False)\n    if isinstance(net, RecurrentNetwork):\n        for c in net.recurrentConns:\n            self.writeConnection(conns, c, True)",
    "nl": "write a Network into a new XML node",
    "original_nl": "write a Network into a new XML node"
  },
  {
    "code": "def get_default_release(self):\n    return ''",
    "nl": "Gets the default release to use when a release is not\nexplicit.",
    "original_nl": "Gets the default release to use when a release is not\nexplicit."
  },
  {
    "code": "def __getitem__(self, index):\n    item = (self.names.get(index) or self.indices.get(index))\n    if (item is None):\n        name = (('0x%X' % index) if isinstance(index, int) else index)\n        raise KeyError(('%s was not found in Object Dictionary' % name))\n    return item",
    "nl": "Get object from object dictionary by name or index.",
    "original_nl": "Get object from object dictionary by name or index."
  },
  {
    "code": "@classmethod\ndef replace_unicode(cls, replacement_string):\n    e = cls()\n    output = []\n    surrogate_character = None\n    if settings.EMOJI_REPLACE_HTML_ENTITIES:\n        replacement_string = cls.replace_html_entities(replacement_string)\n    for (i, character) in enumerate(replacement_string):\n        if (character in cls._unicode_modifiers):\n            continue\n        if ((not UNICODE_WIDE) and (not surrogate_character) and (ord(character) >= UNICODE_SURROGATE_MIN) and (ord(character) <= UNICODE_SURROGATE_MAX)):\n            surrogate_character = character\n            continue\n        if surrogate_character:\n            character = convert_unicode_surrogates((surrogate_character + character))\n            surrogate_character = None\n        name = e.name_for(character)\n        if name:\n            if settings.EMOJI_ALT_AS_UNICODE:\n                character = e._image_string(name, alt=character)\n            else:\n                character = e._image_string(name)\n        output.append(character)\n    return ''.join(output)",
    "nl": "This method will iterate over every character in\n``replacement_string`` and see if it mathces any of the\nunicode codepoints that we recognize. If it does then it will\nreplace that codepoint with an image just like ``replace``.\n\nNOTE: This will only work with Python versions built with wide\nunicode caracter support. Python 3 should always work but\nPython 2 will have to tested before deploy.",
    "original_nl": "This method will iterate over every character in\n``replacement_string`` and see if it mathces any of the\nunicode codepoints that we recognize. If it does then it will\nreplace that codepoint with an image just like ``replace``.\n\nNOTE: This will only work with Python versions built with wide\nunicode caracter support. Python 3 should always work but\nPython 2 will have to tested before deploy."
  },
  {
    "code": "def test_output(self):\n    scriptFile = self.makeSourceFile(['import sys', \"for s in b'hello world\\\\n':\", \"    if hasattr(sys.stdout, 'buffer'):\", '        # Python 3', '        s = bytes([s])', '        sys.stdout.buffer.write(s)', '    else:', '        # Python 2', '        sys.stdout.write(s)', '    sys.stdout.flush()'])\n    d = utils.getProcessOutput(self.exe, ['-u', scriptFile])\n    return d.addCallback(self.assertEqual, b'hello world\\n')",
    "nl": "L{getProcessOutput} returns a L{Deferred} which fires with the complete\noutput of the process it runs after that process exits.",
    "original_nl": "L{getProcessOutput} returns a L{Deferred} which fires with the complete\noutput of the process it runs after that process exits."
  },
  {
    "code": "def test_extract_zip_non_verbose():\n    mox = Mox()\n\n    class MyFs(io.FileSystem):\n        stack = []\n        abspath = mox.CreateMockAnything()\n        pushd = mox.CreateMockAnything()\n        popd = mox.CreateMockAnything()\n        open_raw = mox.CreateMockAnything()\n        mkdir = mox.CreateMockAnything()\n    mox.StubOutWithMock(io, 'zipfile')\n    filename = 'modafoca.zip'\n    base_path = '../to/project'\n    full_path = '/full/path/to/project'\n    MyFs.abspath(base_path).AndReturn(full_path)\n    MyFs.pushd(full_path)\n    zip_mock = mox.CreateMockAnything()\n    io.zipfile.ZipFile(filename).AndReturn(zip_mock)\n    file_list = ['settings.yml', 'app', 'app/controllers.py']\n    zip_mock.namelist().AndReturn(file_list)\n    zip_mock.read('settings.yml').AndReturn('settings.yml content')\n    zip_mock.read('app/controllers.py').AndReturn('controllers.py content')\n    file_mock1 = mox.CreateMockAnything()\n    MyFs.open_raw('settings.yml', 'w').AndReturn(file_mock1)\n    file_mock1.write('settings.yml content')\n    file_mock1.close()\n    MyFs.open_raw('app', 'w').AndRaise(IOError('it is a directory, dumb ass!'))\n    MyFs.mkdir('app')\n    file_mock2 = mox.CreateMockAnything()\n    MyFs.open_raw('app/controllers.py', 'w').AndReturn(file_mock2)\n    file_mock2.write('controllers.py content')\n    file_mock2.close()\n    MyFs.popd()\n    mox.ReplayAll()\n    try:\n        MyFs.extract_zip('modafoca.zip', base_path)\n        mox.VerifyAll()\n    finally:\n        mox.UnsetStubs()",
    "nl": "FileSystem.extract_zip non-verbose",
    "original_nl": "FileSystem.extract_zip non-verbose"
  },
  {
    "code": "def add_signal_handler(self, signum, handler):\n    if (handler is None):\n        handler = 0\n    if (handler in (signal.SIG_IGN, 0)):\n        previous = signal.signal(signum, handler)\n        self._signal_handler_mappings[signum] = handler\n    else:\n\n        def call_signal_handler(*a):\n            self.call_from_executor(handler)\n        previous = signal.signal(signum, call_signal_handler)\n        self._signal_handler_mappings[signum] = handler\n    return self._signal_handler_mappings.get(signum, previous)",
    "nl": "Register a signal handler. Call `handler` when `signal` was received.\nThe given handler will always be called in the same thread as the\neventloop. (Like `call_from_executor`.)",
    "original_nl": "Register a signal handler. Call `handler` when `signal` was received.\nThe given handler will always be called in the same thread as the\neventloop. (Like `call_from_executor`.)"
  },
  {
    "code": "def save(self, must_create=False):\n    obj = Session(session_key=self._get_or_create_session_key(), session_data=self.encode(self._get_session(no_load=must_create)), expire_date=self.get_expiry_date())\n    using = router.db_for_write(Session, instance=obj)\n    sid = transaction.savepoint(using=using)\n    try:\n        obj.save(force_insert=must_create, using=using)\n    except IntegrityError:\n        if must_create:\n            transaction.savepoint_rollback(sid, using=using)\n            raise CreateError\n        raise",
    "nl": "Saves the current session data to the database. If 'must_create' is\nTrue, a database error will be raised if the saving operation doesn't\ncreate a *new* entry (as opposed to possibly updating an existing\nentry).",
    "original_nl": "Saves the current session data to the database. If 'must_create' is\nTrue, a database error will be raised if the saving operation doesn't\ncreate a *new* entry (as opposed to possibly updating an existing\nentry)."
  },
  {
    "code": "def isRunning(self):\n    return self.__workflow.isRunning()",
    "nl": "@rtype: bool",
    "original_nl": "@rtype: bool"
  },
  {
    "code": "def get_channels_dict(self):\n    return self.channels",
    "nl": "Takes no arguments, returns the dictionary that contains all valid channels.",
    "original_nl": "Takes no arguments, returns the dictionary that contains all valid channels."
  },
  {
    "code": "def text(self, dims, symbol, color):\n    raise FormatError('not implemented.')",
    "nl": "Draw text.",
    "original_nl": "Draw text."
  },
  {
    "code": "def unmnt_dev(self):\n    if (not self.mounted):\n        return\n    self.flush_dev()\n    LOG.debug('Umount %s', self.mapped_device)\n    utils.execute('umount', self.mapped_device, run_as_root=True)\n    self.mounted = False",
    "nl": "Unmount the device from the file system.",
    "original_nl": "Unmount the device from the file system."
  },
  {
    "code": "def transformGeometryOutput(self, geometryOutput):\n    if (self.getMatrix4X4() != None):\n        matrix.transformVector3sByMatrix(self.getMatrix4X4().tetragrid, matrix.getVertexes(geometryOutput))",
    "nl": "Transform the geometry output by the local matrix4x4.",
    "original_nl": "Transform the geometry output by the local matrix4x4."
  },
  {
    "code": "def set_wakeup():\n    if (platform.system() == 'Windows'):\n        asyncio.async(wakeup())",
    "nl": "Workaround suppression of `KeyboardInterrrupt` on Windows.",
    "original_nl": "Workaround suppression of `KeyboardInterrrupt` on Windows."
  },
  {
    "code": "def setup_remote_oauth(cache_client, retrieve_access_token=get_access_token):\n    access_token = cache_client.get('diy_crate.auth.access_token')\n    refresh_token = cache_client.get('diy_crate.auth.refresh_token')\n    oauth = RemoteOAuth2(client_id='', client_secret='', access_token=(access_token.decode('utf-8') if access_token else access_token), refresh_token=(refresh_token.decode('utf-8') if refresh_token else refresh_token), retrieve_access_token=retrieve_access_token)\n    return oauth",
    "nl": "sets up the oauth instance with credentials and runtime callback.\n",
    "original_nl": "sets up the oauth instance with credentials and runtime callback.\n:param cache_client:\n:param retrieve_access_token:\n:return:"
  },
  {
    "code": "def __init__(self, props=None, base_obj=None):\n    self._base = None\n    if (base_obj is not None):\n        self._base = base_obj\n    self._column_id = Number()\n    self._direction = EnumeratedValue(SortDirection)\n    if props:\n        deserialize(self, props)",
    "nl": "Initialize the SortCriterion model.",
    "original_nl": "Initialize the SortCriterion model."
  },
  {
    "code": "@classmethod\ndef __build_match_title_full(cls, details):\n    title = details.get('home', {\n        \n    }).get('name_full')\n    title += ' - '\n    title += details.get('away', {\n        \n    }).get('name_full')\n    return title",
    "nl": "Generates a long match title\n\n",
    "original_nl": "Generates a long match title\n\n:param details: Item details\n:type details: dict\n:returns:  string -- Match title (long)"
  },
  {
    "code": "def testReadAscii(self):\n    original_stdin = sys.stdin\n    sys.stdin = io.BytesIO(self._TEST_DATA)\n    input_reader = tools.StdinInputReader(encoding='ascii')\n    string = input_reader.Read()\n    self.assertEqual(string, 'A first string\\n')\n    string = input_reader.Read()\n    self.assertEqual(string, 'A 2nd string\\n')\n    string = input_reader.Read()\n    self.assertEqual(string, '\ufffd\ufffdri\ufffd\ufffdja string\\n')\n    string = input_reader.Read()\n    expected_string = '\ufffd\ufffdf\\x00j\\x00\ufffd\\x00r\\x00\ufffd\\x00a\\x00 \\x00b\\x00a\\x00n\\x00d\\x00'\n    self.assertEqual(string, expected_string)\n    sys.stdin = original_stdin",
    "nl": "Tests the Read function with ASCII encoding.",
    "original_nl": "Tests the Read function with ASCII encoding."
  },
  {
    "code": "def register(self, function, observer):\n    event = function.__name__\n    if (event not in self.observers):\n        self.observers[event] = {\n            \n        }\n    try:\n        self.observers[event][observer] = 1\n    except KeyError:\n        op = False\n    else:\n        op = True\n    return op",
    "nl": "register an observer to a specific event\n\n",
    "original_nl": "register an observer to a specific event\n\n:param function: the function to observe\n:param observer: the observer function\n:return:"
  },
  {
    "code": "def getPeer(self):\n    return self.conn.transport.getPeer()",
    "nl": "See: L{ITransport.getPeer}\n\n",
    "original_nl": "See: L{ITransport.getPeer}\n\n@return: The remote address of this connection.\n@rtype: L{SSHTransportAddress}."
  },
  {
    "code": "def load(self, polyfile):\n    ET = pg.optImport('xml.etree.cElementTree', 'read in XML files')\n    self.doc = ET.parse(polyfile)\n    self.parse()",
    "nl": "Read polygon info from XML file (see example.xml).",
    "original_nl": "Read polygon info from XML file (see example.xml)."
  },
  {
    "code": "@classmethod\ndef _parse_cmd_save(cls, args):\n    return 'SAVE'",
    "nl": "Parse a packet containing the \"save\" command and ",
    "original_nl": "Parse a packet containing the \"save\" command and \nreturn it as a human readable string."
  },
  {
    "code": "def parameters(mat):\n    (xx, yy, zz, yz, xz, xy) = _matsplit(mat)\n    e1 = _e1(xx, yy, zz)\n    e2 = _e2(xx, yy)\n    e3 = _e3(xx, yy, zz)\n    e4 = _e4(yz)\n    e5 = _e5(xz)\n    e6 = _e6(xy)\n    return (e1, e2, e3, e4, e5, e6)",
    "nl": "Calculate the six strain order parameters\nfor the given strain metric\n\n:mat: ndarray\n",
    "original_nl": "Calculate the six strain order parameters\nfor the given strain metric\n\n:mat: ndarray\n:returns: float,float,float,float,float,float"
  },
  {
    "code": "def __init__(self, data='', encode=True):\n    Reply.__init__(self, data, encode)",
    "nl": "Initialize the `Response` object.",
    "original_nl": "Initialize the `Response` object."
  },
  {
    "code": "def is_owner(self, nick):\n    return (nick in self.ownerdict)",
    "nl": "Check whether a user has owner status in the channel.",
    "original_nl": "Check whether a user has owner status in the channel."
  },
  {
    "code": "@hook.command(permissions=['op_op', 'op', 'chanop'])\ndef op(text, conn, chan, notice, nick, admin_log):\n    mode_cmd('+o', 'op', text, chan, conn, notice, nick, admin_log)",
    "nl": "[channel] <user> - ops <user> in [channel], or in the caller's channel if no channel is specified",
    "original_nl": "[channel] <user> - ops <user> in [channel], or in the caller's channel if no channel is specified"
  },
  {
    "code": "@patch('bulk_email.models.html_to_text', Mock(return_value='Mocking CourseEmail.text_message', autospec=True))\ndef test_send_to_self(self):\n    test_email = {\n        'action': 'send',\n        'send_to': 'myself',\n        'subject': 'test subject for myself',\n        'message': 'test message for myself',\n    }\n    response = self.client.post(self.send_mail_url, test_email)\n    self.assertEquals(json.loads(response.content), self.success_content)\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(len(mail.outbox[0].to), 1)\n    self.assertEquals(mail.outbox[0].to[0], self.instructor.email)\n    self.assertEquals(mail.outbox[0].subject, 'test subject for myself')",
    "nl": "Make sure email send to myself goes to myself.",
    "original_nl": "Make sure email send to myself goes to myself."
  },
  {
    "code": "def add_if_op(if_net, cond_blob, lexical_scope, then_net, else_net=None):\n    (then_input_blob_names, then_output_blob_names) = get_external_blob_names(then_net, lexical_scope)\n    else_input_blob_names = set()\n    else_output_blob_names = set()\n    if else_net:\n        (else_input_blob_names, else_output_blob_names) = get_external_blob_names(else_net, lexical_scope)\n    input_blob_names = (then_input_blob_names | else_input_blob_names)\n    output_blob_names = (then_output_blob_names | else_output_blob_names)\n    if_inputs = [cond_blob]\n    if_inputs += [core.BlobReference(name=b, net=None) for b in input_blob_names]\n    if_outputs = [core.BlobReference(name=b, net=None) for b in output_blob_names]\n    do_then_net = core.Net('do_then_net')\n    then_input_blobs = [core.BlobReference(name=b, net=None) for b in then_input_blob_names]\n    then_output_blobs = [core.BlobReference(name=b, net=None) for b in then_output_blob_names]\n    then_input_output_names_ordered = [str(b) for b in (then_input_blobs + then_output_blobs)]\n    then_outer_blob_names = list((then_input_blob_names | then_output_blob_names))\n    then_outer_blob_names_idx = [then_input_output_names_ordered.index(b) for b in then_outer_blob_names]\n    do_then_workspace_blob = if_net.NextScopedBlob((if_net.Name() + '/workspace_if_then'))\n    then_input_blobs.append(do_then_workspace_blob)\n    then_output_blobs.append(do_then_workspace_blob)\n    if_inputs.append(do_then_workspace_blob)\n    if_outputs.append(do_then_workspace_blob)\n    do_then_net.Do(then_input_blobs, then_output_blobs, net=then_net.Proto(), inner_blobs=then_outer_blob_names, outer_blobs_idx=then_outer_blob_names_idx)\n    do_then_net.AddExternalOutput(*then_output_blobs)\n    if_args = {\n        \n    }\n    if_args['then_net'] = do_then_net.Proto()\n    do_else_workspace_blob = None\n    if else_net:\n        do_else_net = core.Net('do_else_net')\n        else_input_blobs = [core.BlobReference(name=b, net=None) for b in else_input_blob_names]\n        else_output_blobs = [core.BlobReference(name=b, net=None) for b in else_output_blob_names]\n        else_input_output_names_ordered = [str(b) for b in (else_input_blobs + else_output_blobs)]\n        else_outer_blob_names = list((else_input_blob_names | else_output_blob_names))\n        else_outer_blob_names_idx = [else_input_output_names_ordered.index(b) for b in else_outer_blob_names]\n        do_else_workspace_blob = if_net.NextScopedBlob((if_net.Name() + '/workspace_if_else'))\n        else_input_blobs.append(do_else_workspace_blob)\n        else_output_blobs.append(do_else_workspace_blob)\n        if_inputs.append(do_else_workspace_blob)\n        if_outputs.append(do_else_workspace_blob)\n        do_else_net.Do(else_input_blobs, else_output_blobs, net=else_net.Proto(), inner_blobs=else_outer_blob_names, outer_blobs_idx=else_outer_blob_names_idx)\n        do_else_net.AddExternalOutput(*else_output_blobs)\n        if_args['else_net'] = do_else_net.Proto()\n    if_net.CreateScope([], [do_then_workspace_blob])\n    if do_else_workspace_blob:\n        if_net.CreateScope([], [do_else_workspace_blob])\n    if_net.If(if_inputs, if_outputs, **if_args)\n    if_net.AddExternalOutput(*if_outputs)",
    "nl": "A helper function to add an If op to the net.\nAutomatically determines whether blobs in the then/else subnets are external\n(from the outer workspace) or local (visible only inside subnet's workspace)\nbased on lexical scope - set of all outer blob names visible to the 'If'\noperator. All the blobs in then/else subnets with names matching a name in lexical\nscope and all the blobs that are first used as the operators' inputs are\nconsidered outer blobs - these blobs must exist in the outer workspace,\nthen/else subnets can read their values and new values written into these blobs\nwill be visible outside of the 'If' operator. All other blobs are local - exist\nonly within inner workspaces for then/else.\nInputs:\n    if_net - net to add an If op to;\n    cond_blob - scalar bool blob reference, used as If condition;\n    lexical_scope - a set of outer blob names visible to then/else branches;\n    then_net/else_net - nets (core.Net) for then/else branches",
    "original_nl": "A helper function to add an If op to the net.\nAutomatically determines whether blobs in the then/else subnets are external\n(from the outer workspace) or local (visible only inside subnet's workspace)\nbased on lexical scope - set of all outer blob names visible to the 'If'\noperator. All the blobs in then/else subnets with names matching a name in lexical\nscope and all the blobs that are first used as the operators' inputs are\nconsidered outer blobs - these blobs must exist in the outer workspace,\nthen/else subnets can read their values and new values written into these blobs\nwill be visible outside of the 'If' operator. All other blobs are local - exist\nonly within inner workspaces for then/else.\nInputs:\n    if_net - net to add an If op to;\n    cond_blob - scalar bool blob reference, used as If condition;\n    lexical_scope - a set of outer blob names visible to then/else branches;\n    then_net/else_net - nets (core.Net) for then/else branches"
  },
  {
    "code": "@click.command('switch-to-develop')\ndef switch_to_develop(upgrade=False):\n    from bench.app import switch_to_develop\n    switch_to_develop(apps=['frappe', 'erpnext'])\n    print()\n    print('Switched to develop')\n    print('Please run `bench update --patch` to be safe from any differences in database schema')",
    "nl": "Switch frappe and erpnext to develop branch",
    "original_nl": "Switch frappe and erpnext to develop branch"
  },
  {
    "code": "def fetch_66ip(https):\n    proxyes = []\n    try:\n        if https:\n            url = 'http://www.66ip.cn/nmtq.php?getnum=10&isp=0&anonymoustype=3&start=&ports=&export=&ipaddress=&area=1&proxytype=1&api=66ip'\n        else:\n            url = 'http://www.66ip.cn/nmtq.php?getnum=10&isp=0&anonymoustype=3&start=&ports=&export=&ipaddress=&area=1&proxytype=0&api=66ip'\n        content = get_html(url)\n        content = str(content)\n        urls = content.split('</script>')[1].split('</div>')[0].split('<br />')\n        for u in urls:\n            u = u.split('\\\\t')[(- 1)]\n            if u.strip():\n                if https:\n                    proxyes.append(('https://' + u.strip()))\n                else:\n                    proxyes.append(('http://' + u.strip()))\n    except Exception as e:\n        logger.warning(('fail to fetch from 66ip: %s' % e))\n    return proxyes",
    "nl": "http://www.66ip.cn/\n\u6bcf\u6b21\u6253\u5f00\u6b64\u94fe\u63a5\u90fd\u80fd\u5f97\u5230\u4e00\u6279\u4ee3\u7406, \u901f\u5ea6\u4e0d\u4fdd\u8bc1",
    "original_nl": "http://www.66ip.cn/\n\u6bcf\u6b21\u6253\u5f00\u6b64\u94fe\u63a5\u90fd\u80fd\u5f97\u5230\u4e00\u6279\u4ee3\u7406, \u901f\u5ea6\u4e0d\u4fdd\u8bc1"
  },
  {
    "code": "@property\ndef count(self):\n    return len(self)",
    "nl": "The number of items in the collection. Bindable.",
    "original_nl": "The number of items in the collection. Bindable."
  },
  {
    "code": "@property\ndef max_export_tiles_count(self):\n    return int(self._editor.get_element_value(self._max_export_tiles_count_element))",
    "nl": "Gets an integer value that describes how many tiles can be expored.",
    "original_nl": "Gets an integer value that describes how many tiles can be expored."
  },
  {
    "code": "def parse_stockholm(fobj):\n    names = OrderedDict()\n    found_eof = False\n    for line in fobj:\n        line = line.strip()\n        if (line == '//'):\n            found_eof = True\n        elif (line.startswith('#') or (not line.strip())):\n            continue\n        else:\n            (name, __) = line.split(None, 1)\n            names[name] = None\n    if (not found_eof):\n        raise ValueError('Invalid Stockholm format: no file terminator')\n    return list(names.keys())",
    "nl": "Return a list of names from an Stockholm-format sequence alignment\nfile. ``fobj`` is an open file or another object representing a\nsequence of lines.",
    "original_nl": "Return a list of names from an Stockholm-format sequence alignment\nfile. ``fobj`` is an open file or another object representing a\nsequence of lines."
  },
  {
    "code": "def renderGL(self):\n    glClear(GL_COLOR_BUFFER_BIT)\n    glUseProgram(self.__shaderProgram)\n    glBindVertexArray(self.__vao)\n    glDrawArrays(GL_TRIANGLES, 0, 6)\n    glBindVertexArray(0)\n    glUseProgram(0)",
    "nl": "opengl render method",
    "original_nl": "opengl render method"
  },
  {
    "code": "def test_get_by_name(self, tag_manager, stored_tag):\n    result = tag_manager.GetByName(stored_tag.name)\n    result = helpers.dbus_to_hamster_tag(result)\n    assert (result.pk == stored_tag.pk)\n    assert (result.name == stored_tag.name)",
    "nl": "Make sure a matching tag is returned.",
    "original_nl": "Make sure a matching tag is returned."
  },
  {
    "code": "def index(self):\n    parser = argparse.ArgumentParser(description='Create index of output tiles.', formatter_class=argparse.ArgumentDefaultsHelpFormatter, usage='mapchete index <mapchete_file>')\n    parser.add_argument('mapchete_file', type=str, help='Mapchete file')\n    parser.add_argument('--out_dir', type=str, help='output directory (default: output path in mapchete file')\n    parser.add_argument('--geojson', action='store_true', help='write GeoJSON index')\n    parser.add_argument('--gpkg', action='store_true', help='write GeoPackage index')\n    parser.add_argument('--txt', action='store_true', help='write text file with paths')\n    parser.add_argument('--fieldname', type=str, default='location', help='take boundaries from WKT geometry in tile pyramid CRS', metavar='<str>')\n    parser.add_argument('--basepath', type=str, help='use other base path than process output path', metavar='<str>')\n    parser.add_argument('--for_gdal', action='store_true', help='make remote paths readable by GDAL (not applied for txt output)')\n    parser.add_argument('--zoom', '-z', type=int, nargs='*', help='either minimum and maximum zoom level or just one zoom level', metavar='<int>')\n    parser.add_argument('--bounds', '-b', type=float, nargs=4, help='left, bottom, right, top bounds in tile pyramid CRS', metavar='<float>')\n    parser.add_argument('--wkt_geometry', '-g', type=str, help='take boundaries from WKT geometry in tile pyramid CRS', metavar='<str>')\n    parser.add_argument('--tile', '-t', type=int, nargs=3, help='zoom, row, column of single tile', metavar='<int>')\n    parser.add_argument('--verbose', '-v', action='store_true', help='print info for each process tile')\n    parser.add_argument('--debug', '-d', action='store_true', help='deactivate progress bar and print debug log output')\n    index(parser.parse_args(self.args[2:]))",
    "nl": "Parse params and run index command.",
    "original_nl": "Parse params and run index command."
  },
  {
    "code": "def getSequenceIdFieldIdx(self):\n    return _getFieldIndexBySpecial(self.getFields(), FieldMetaSpecial.sequence)",
    "nl": ":returns: (int) index of the ``sequenceId`` field.",
    "original_nl": ":returns: (int) index of the ``sequenceId`` field."
  },
  {
    "code": "def access(self, mode):\n    assert (mode in ['counting', 'exclusive'])\n    return LockAccess(self, mode)",
    "nl": "Express how the lock should be accessed",
    "original_nl": "Express how the lock should be accessed"
  },
  {
    "code": "def __init__(self, actions=()):\n    self.actions = tuple(actions)",
    "nl": "Represents a PDDL-like Problem Domain\n@arg actions : list of Action objects",
    "original_nl": "Represents a PDDL-like Problem Domain\n@arg actions : list of Action objects"
  },
  {
    "code": "def cmd(self):\n    settings = Linter.get_view_settings(self)\n    if ('cmd' in settings):\n        logger.warning('The setting `cmd` has been deprecated. Use `executable` instead.')\n        command = [settings.get('cmd')]\n    else:\n        command = ['php']\n    command.append('-l')\n    command.append('-n')\n    command.append('-d')\n    command.append('display_errors=On')\n    command.append('-d')\n    command.append('log_errors=Off')\n    return command",
    "nl": "Read cmd from inline settings.",
    "original_nl": "Read cmd from inline settings."
  },
  {
    "code": "def exportPostgresqltoGeojson(dbname, username, password, output_filename):\n    bash = 'ogr2ogr -f \"GeoJSON\" {geojson}.geojson PG:\"dbname={dbname} user={user} password={password}\" \"temp\"'.format(dbname=dbname, user=username, password=password, geojson=output_filename)\n    system(bash)",
    "nl": "Using bash, export a postgresql table to GeoJSON format",
    "original_nl": "Using bash, export a postgresql table to GeoJSON format"
  },
  {
    "code": "def save_profile(backend, user, response, request, *args, **kwargs):\n    if (backend.name == 'github'):\n        handle = user.username\n        sync_profile(handle, user, hide_profile=False)\n        setup_lang(request, user)",
    "nl": "Associate a Profile with a User.",
    "original_nl": "Associate a Profile with a User."
  },
  {
    "code": "def _ReshapeToInput(op, grad):\n    return array_ops.reshape(grad, array_ops.shape(op.inputs[0]))",
    "nl": "Reshapes the gradient to the shape of the original input.",
    "original_nl": "Reshapes the gradient to the shape of the original input."
  },
  {
    "code": "def test_descending():\n    limit = 10\n    images = [image for image in Search().descending().limit(limit)]\n    assert (len(images) == limit)\n    for image in images:\n        if (image is not images[(- 1)]):\n            next_image = images[(images.index(image) + 1)]\n            assert (image.id_number > next_image.id_number)",
    "nl": "Tests whether descending search is in the correct order",
    "original_nl": "Tests whether descending search is in the correct order"
  },
  {
    "code": "@db_access_point\ndef update(self, session, lookup_keys, updates, *args, **kwargs):\n    model = self._get_model(lookup_keys, session)\n    model = self._set_values_on_model(model, updates, fields=self.update_fields)\n    session.commit()\n    return self.serialize_model(model)",
    "nl": "Updates the model with the specified lookup_keys and returns\nthe dictified object.\n\n",
    "original_nl": "Updates the model with the specified lookup_keys and returns\nthe dictified object.\n\n:param Session session: The SQLAlchemy session to use\n:param dict lookup_keys: A dictionary mapping the fields\n    and their expected values\n:param dict updates: The columns and the values to update\n    them to.\n:return: The dictionary of keys and values for the retrieved\n    model.  The only values returned will be those specified by\n    fields attrbute on the class\n:rtype: dict\n:raises: NotFoundException"
  },
  {
    "code": "def rmsprop(obj_fun, grad_fun, x0, max_iters=100, callback=None, lr_fun=const_lr(0.01), gamma=0.9, eps=(10 ** (- 8))):\n    x = np.copy(x0)\n    avg_sq_grad = np.ones(len(x))\n    grad_fun = maybe_add_iter_arg_to_fun(grad_fun)\n    if (callback is not None):\n        callback(x)\n    for i in range(max_iters):\n        g = grad_fun(x, i)\n        avg_sq_grad = ((avg_sq_grad * gamma) + ((g ** 2) * (1 - gamma)))\n        step_size = lr_fun(i)\n        x -= ((step_size * g) / (np.sqrt(avg_sq_grad) + eps))\n        if callback:\n            callback(x)\n    val = obj_fun(x)\n    return (x, val)",
    "nl": "Root mean squared prop: See Adagrad paper for details.",
    "original_nl": "Root mean squared prop: See Adagrad paper for details."
  },
  {
    "code": "def save_checkpoint(prefix, epoch, arg_params, aux_params):\n    save_dict = {('arg:%s' % k): v for (k, v) in arg_params.items()}\n    save_dict.update({('aux:%s' % k): v for (k, v) in aux_params.items()})\n    param_name = ('%s-%04d.params' % (prefix, epoch))\n    mx.nd.save(param_name, save_dict)",
    "nl": "Checkpoint the model data into file.\n",
    "original_nl": "Checkpoint the model data into file.\n:param prefix: Prefix of model name.\n:param epoch: The epoch number of the model.\n:param arg_params: dict of str to NDArray\n    Model parameter, dict of name to NDArray of net's weights.\n:param aux_params: dict of str to NDArray\n    Model parameter, dict of name to NDArray of net's auxiliary states.\n:return: None\nprefix-epoch.params will be saved for parameters."
  },
  {
    "code": "@staticmethod\ndef rebuild(arr):\n    temp = bytearray()\n    app = temp.append\n    for i in range(len(arr)):\n        if ((arr[i][1] == EXCEPT) or (arr[i][1] == EXCEPT2)):\n            before = False\n            after = True\n            try:\n                before = arr[i][0]\n            except IndexError:\n                before = False\n            else:\n                try:\n                    after = arr[(i + 1)][0]\n                except IndexError:\n                    after = False\n            if ((arr[i][1] == EXCEPT) and (type(before) != bool) and (before == after)):\n                app(arr[i][0])\n            elif ((arr[i][1] == EXCEPT2) and (type(before) != bool) and (before == after == EXCEPT)):\n                app(arr[i][0])\n            elif (before and (not after) and ((i + 1) >= len(arr))):\n                app(arr[i][0])\n            else:\n                app(arr[i][0])\n                app(arr[i][1])\n        else:\n            app(arr[i][0])\n            app(arr[i][1])\n    return temp",
    "nl": "rebuild original message\nbecause on original we've use '0'\nfor duplicate letters or single char\n\n",
    "original_nl": "rebuild original message\nbecause on original we've use '0'\nfor duplicate letters or single char\n\n@param list arr    result of decrypt 2D\n@return: byte"
  },
  {
    "code": "def send_datagram(self, message):\n    (host, port) = message.destination\n    logger.debug(('send_datagram - ' + str(message)))\n    serializer = Serializer()\n    raw_message = serializer.serialize(message)\n    try:\n        self._socket.sendto(raw_message, (host, port))\n    except Exception as e:\n        if ((self._cb_ignore_write_exception is not None) and callable(self._cb_ignore_write_exception)):\n            if (not self._cb_ignore_write_exception(e, self)):\n                raise\n    if ((self._receiver_thread is None) or (not self._receiver_thread.isAlive())):\n        self._receiver_thread = threading.Thread(target=self.receive_datagram)\n        self._receiver_thread.start()",
    "nl": "Send a message over the UDP socket.\n\n",
    "original_nl": "Send a message over the UDP socket.\n\n:param message: the message to send"
  },
  {
    "code": "def _exclusively_processing(self):\n    with self._aTB_lock:\n        if self._aTB_processing:\n            return False\n        self._aTB_processing = True\n        return True",
    "nl": "Protects critical sections by only allowing a single thread entry",
    "original_nl": "Protects critical sections by only allowing a single thread entry"
  },
  {
    "code": "@app.route('/')\ndef index():\n    t = env.get_template('index.html')\n    return t.render(using_openshift=int(using_openshift))",
    "nl": "Return a default document if no path was specified.",
    "original_nl": "Return a default document if no path was specified."
  },
  {
    "code": "def create_event(event_type: str, order_id: OrderID, data: OrderEventData) -> None:\n    event = _build_event(event_type, order_id, data)\n    db.session.add(event)\n    db.session.commit()",
    "nl": "Create an order event.",
    "original_nl": "Create an order event."
  },
  {
    "code": "def category(self):\n    if self.category_links:\n        return self.category_links[0].to_node\n    else:\n        return None",
    "nl": "Returns the category this node belongs to.\n\nFor example, 'leftmost' belongs to 'string_position_category'. This code\nassumes that each node belongs to at most one category.",
    "original_nl": "Returns the category this node belongs to.\n\nFor example, 'leftmost' belongs to 'string_position_category'. This code\nassumes that each node belongs to at most one category."
  },
  {
    "code": "def PDF(src, target, lineNumbers=False, args=''):\n    print(('Building %s from %s ...' % (target, src)))\n    HTML(src, '.tmp.html')\n    wk = WkHtmlToPdf(css=css, margins=pdfMargins, spacing=pdfSpacing, header=pdfHeader, footer=pdfFooter, verbose=True, args=args)\n    if lineNumbers:\n        _target = '.tmp.pdf'\n    else:\n        _target = target\n    wk.parse('.tmp.html', _target)\n    if lineNumbers:\n        tools.addLineNumbersToPDF(_target, target)\n        os.remove(_target)",
    "nl": "desc:\n        Builds a PDF file from a Markdown source.\n\narguments:\n        src:\n                desc:   \"%arg_src\"\n                type:   [str, unicode]\n        target:\n                desc:   The name of a PDF target file.\n                type:   [str, unicode]\n\nkeywords:\n        lineNumbers:\n                desc:   Determines whether line numbers should be added. This is\n                                currently quite a complicated process, which may break.\n                type:   bool\n        args:\n                desc:   Indicates extra arguments to be passed onto wkhtmltopdf.\n                type:   [str, unicode]",
    "original_nl": "desc:\n        Builds a PDF file from a Markdown source.\n\narguments:\n        src:\n                desc:   \"%arg_src\"\n                type:   [str, unicode]\n        target:\n                desc:   The name of a PDF target file.\n                type:   [str, unicode]\n\nkeywords:\n        lineNumbers:\n                desc:   Determines whether line numbers should be added. This is\n                                currently quite a complicated process, which may break.\n                type:   bool\n        args:\n                desc:   Indicates extra arguments to be passed onto wkhtmltopdf.\n                type:   [str, unicode]"
  },
  {
    "code": "def generate_project(args):\n    src = os.path.join(dirname(abspath(__file__)), 'project')\n    project_name = args.get('<project>')\n    if (not project_name):\n        logger.warning('Project name cannot be empty.')\n        return\n    dst = os.path.join(os.getcwd(), project_name)\n    if os.path.isdir(dst):\n        logger.warning('Project directory already exists.')\n        return\n    logger.info('Start generating project files.')\n    _mkdir_p(dst)\n    for (src_dir, sub_dirs, filenames) in os.walk(src):\n        relative_path = src_dir.split(src)[1].lstrip(os.path.sep)\n        dst_dir = os.path.join(dst, relative_path)\n        if (src != src_dir):\n            _mkdir_p(dst_dir)\n        for filename in filenames:\n            if (filename in ['development.py', 'production.py']):\n                continue\n            src_file = os.path.join(src_dir, filename)\n            dst_file = os.path.join(dst_dir, filename)\n            if filename.endswith(REWRITE_FILE_EXTS):\n                _rewrite_and_copy(src_file, dst_file, project_name)\n            else:\n                shutil.copy(src_file, dst_file)\n            logger.info(('New: %s' % dst_file))\n            if (filename in ['development_sample.py', 'production_sample.py']):\n                dst_file = os.path.join(dst_dir, ('%s.py' % filename.split('_')[0]))\n                _rewrite_and_copy(src_file, dst_file, project_name)\n                logger.info(('New: %s' % dst_file))\n    logger.info('Finish generating project files.')",
    "nl": "New project.",
    "original_nl": "New project."
  },
  {
    "code": "def list_files(tag=None, sat_id=None, data_path=None, format_str=None):\n    if (data_path is not None):\n        if (tag == ''):\n            if (format_str is None):\n                format_str = 'kp{year:2d}{month:02d}.tab'\n            out = pysat.Files.from_os(data_path=data_path, format_str=format_str, two_digit_year_break=94)\n            if (not out.empty):\n                out.ix[((out.index[(- 1)] + pds.DateOffset(months=1)) - pds.DateOffset(days=1))] = out.iloc[(- 1)]\n                out = out.asfreq('D', 'pad')\n                out = ((out + '_') + out.index.strftime('%Y-%m-%d'))\n            return out\n        else:\n            raise ValueError('Unrecognized tag name for Space Weather Index Kp')\n    else:\n        raise ValueError('A data_path must be passed to the loading routine for Kp')",
    "nl": "Return a Pandas Series of every file for chosen satellite data\n",
    "original_nl": "Return a Pandas Series of every file for chosen satellite data\n\nParameters\n-----------\ntag : (string or NoneType)\n    Denotes type of file to load.  Accepted types are '1min' and '5min'.\n    (default=None)\nsat_id : (string or NoneType)\n    Specifies the satellite ID for a constellation.  Not used.\n    (default=None)\ndata_path : (string or NoneType)\n    Path to data directory.  If None is specified, the value previously\n    set in Instrument.files.data_path is used.  (default=None)\nformat_str : (string or NoneType)\n    User specified file format.  If None is specified, the default\n    formats associated with the supplied tags are used. (default=None)\n\nReturns\n--------\npysat.Files.from_os : (pysat._files.Files)\n    A class containing the verified available files\n\nNotes\n-----\nCalled by pysat. Not intended for direct use by user."
  },
  {
    "code": "@classmethod\ndef _create_flag_file(cls, box, name, raw_token, description, value):\n    token = cls.digest(raw_token)\n    if (cls.by_token(token) is not None):\n        raise ValidationError('Flag token already exists in database')\n    return cls(box_id=box.id, name=name, token=token, description=description, value=value)",
    "nl": "Check flag file specific parameters",
    "original_nl": "Check flag file specific parameters"
  },
  {
    "code": "def test_disconnect(spawn, shell):\n    shell.connect()\n    shell.disconnect()\n    assert (not shell.is_connected())\n    with raises(AlreadyDisconnectedError):\n        shell.disconnect()\n    shell.connect(connection='1')\n    shell.disconnect(connection='1')\n    assert (not shell.is_connected(connection='1'))\n    with raises(AlreadyDisconnectedError):\n        shell.disconnect(connection='1')",
    "nl": "Test that disconnect works properly.",
    "original_nl": "Test that disconnect works properly."
  },
  {
    "code": "@transactional\ndef register_all_triggers():\n    from maasserver.triggers.system import register_system_triggers\n    from maasserver.triggers.websocket import register_websocket_triggers\n    register_system_triggers()\n    register_websocket_triggers()",
    "nl": "Register all triggers into the database.",
    "original_nl": "Register all triggers into the database."
  },
  {
    "code": "def logged(func=None, *, level=logging.DEBUG, name=None, message=None):\n    if (func is None):\n        return partial(logged, level=level, name=name, message=message)\n    logname = (name if name else func.__module__)\n    log = logging.getLogger(logname)\n    logmsg = (message if message else func.__name__)\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        log.log(level, logmsg)\n        return func(*args, **kwargs)\n\n    @attach_wrapper(wrapper)\n    def set_level(newlevel):\n        nonlocal level\n        level = newlevel\n\n    @attach_wrapper(wrapper)\n    def set_message(newmsg):\n        nonlocal logmsg\n        logmsg = newmsg\n    return wrapper",
    "nl": "Decorator to add logging to a function.\n",
    "original_nl": "Decorator to add logging to a function.\n\nParameters\n----------\nfunc : callable\n    Decorated function\nlevel : :class:`~python:int`\n    Logging level\nname : :class:`~python:str`, optional\n    Logger name\nmessage : :class:`~python:str`, optional\n    Log message"
  },
  {
    "code": "def test_missing_config():\n    error = MissingEnvVar('client_id')\n    assert (str(error) == 'Missing client_id environment variable')",
    "nl": "Test error string is formatted correctly",
    "original_nl": "Test error string is formatted correctly"
  },
  {
    "code": "def __init__(self, server='/usr/bin/Xorg', xinitrc='/etc/X11/xinit/Xclients', resolution='1024x768x16'):\n    testBinary(server)\n    self.server = server\n    self._exitCode = None\n    self.xinit = '/usr/bin/xinit'\n    self.display = None\n    self.xinitrc = xinitrc\n    self.resolution = resolution",
    "nl": "resolution is only used with Xvfb.",
    "original_nl": "resolution is only used with Xvfb."
  },
  {
    "code": "def add_link(self, filename, line, column, destination):\n    self._links[filename][(line, column)].append(destination)",
    "nl": "Add a link.\n\nfilename, line and column, describe the position in the source file.\n\ndestination can be any object that describes where the link points to.",
    "original_nl": "Add a link.\n\nfilename, line and column, describe the position in the source file.\n\ndestination can be any object that describes where the link points to."
  },
  {
    "code": "def update(self, event):\n    lib = self._library\n    path = os.path.join(event.path, event.name)\n    if event.dir:\n        print_d(('Checking directory %s...' % path))\n        to_reload = []\n        for filename in lib._contents:\n            if filename.startswith(path):\n                item = lib.get(filename, None)\n                if item:\n                    to_reload.append(item)\n        print_d(('Reloading %d matching songs(s)' % len(to_reload)))\n        for item in to_reload:\n            lib.reload(item)\n    else:\n        item = lib.get(path, None)\n        if item:\n            lib.reload(item)\n    return False",
    "nl": "Update a library / file. Typically this means deleting it",
    "original_nl": "Update a library / file. Typically this means deleting it"
  },
  {
    "code": "def sample_n(self, n, seed=None, name='sample_n'):\n    with ops.name_scope(self.name):\n        with ops.op_scope([self.p, n], name):\n            n = ops.convert_to_tensor(n, name='n')\n            new_shape = array_ops.concat(0, ([n], self.batch_shape()))\n            uniform = random_ops.random_uniform(new_shape, seed=seed, dtype=dtypes.float32)\n            sample = math_ops.less(uniform, self.p)\n            sample.set_shape(tensor_shape.vector(tensor_util.constant_value(n)).concatenate(self.get_batch_shape()))\n            return math_ops.cast(sample, self.dtype)",
    "nl": "Generate `n` samples.\n\nArgs:\n  n: scalar.  Number of samples to draw from each distribution.\n  seed: Python integer seed for RNG.\n  name: name to give to the op.\n",
    "original_nl": "Generate `n` samples.\n\nArgs:\n  n: scalar.  Number of samples to draw from each distribution.\n  seed: Python integer seed for RNG.\n  name: name to give to the op.\n\nReturns:\n  samples: a `Tensor` of shape `(n,) + self.batch_shape` with values of type\n      `self.dtype`."
  },
  {
    "code": "def _new_train(self):\n    if (0 in self.training_points[None]):\n        x = self.training_points[None][0][0]\n        y = self.training_points[None][0][1]\n    if (x.shape[0] < (((self.nx + 1) * (self.nx + 2)) / 2.0)):\n        raise Exception(('Number of training points should be greater or equal to %d.' % (((self.nx + 1) * (self.nx + 2)) / 2.0)))\n    X = self._response_surface(x)\n    self.coef = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))",
    "nl": "Train the model",
    "original_nl": "Train the model"
  },
  {
    "code": "def buildSolrQuery(si, solrQuery, query, orClauses):\n    (ordered, unordered) = ([], [])\n    clazz = query.__class__\n    for criteria in namesForQuery(clazz):\n        if (getattr(clazz, criteria) not in query):\n            continue\n        if (criteria in si.schema.fields):\n            field = criteria\n        else:\n            upperCriteria = (criteria[0].upper() + criteria[1:])\n            if (upperCriteria in si.schema.fields):\n                field = upperCriteria\n            else:\n                continue\n        crt = getattr(query, criteria)\n        if isinstance(crt, AsBoolean):\n            if (AsBoolean.value in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        field: crt.value,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        field: crt.value,\n                    })\n        elif isinstance(crt, AsLike):\n            if (AsLike.like in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        field: crt.like,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        field: crt.like,\n                    })\n            elif (AsLike.ilike in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        field: crt.ilike,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        field: crt.ilike,\n                    })\n        elif isinstance(crt, AsEqual):\n            if (AsEqual.equal in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        field: crt.equal,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        field: crt.equal,\n                    })\n        elif isinstance(crt, (AsDate, AsTime, AsDateTime, AsRange)):\n            if (crt.__class__.start in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        (field + '__gte'): crt.start,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        (field + '__gte'): crt.start,\n                    })\n            elif (crt.__class__.until in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        (field + '__lt'): crt.until,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        (field + '__lt'): crt.until,\n                    })\n            if (crt.__class__.end in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        (field + '__lte'): crt.end,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        (field + '__lte'): crt.end,\n                    })\n            elif (crt.__class__.since in crt):\n                if (solrQuery is None):\n                    solrQuery = si.query(**{\n                        (field + '__gt'): crt.since,\n                    })\n                else:\n                    solrQuery = solrQuery.query(**{\n                        (field + '__gt'): crt.since,\n                    })\n        elif isinstance(crt, AsLikeExpression):\n            if (AsLikeExpression.inc in crt):\n                for value in crt.inc:\n                    if (solrQuery is None):\n                        solrQuery = si.query(**{\n                            field: value,\n                        })\n                    else:\n                        solrQuery = solrQuery.query(**{\n                            field: value,\n                        })\n            if (crt and (AsLikeExpression.ext in crt)):\n                for value in crt.ext:\n                    orClauses.append(si.Q(**{\n                        field: value,\n                    }))\n            if (crt and (AsLikeExpression.exc in crt)):\n                for value in crt.exc:\n                    if (solrQuery is None):\n                        solrQuery = si.exclude(**{\n                            field: value,\n                        })\n                    else:\n                        solrQuery = solrQuery.exclude(**{\n                            field: value,\n                        })\n        if isinstance(crt, AsOrdered):\n            assert isinstance(crt, AsOrdered)\n            if (AsOrdered.ascending in crt):\n                if ((AsOrdered.priority in crt) and crt.priority):\n                    ordered.append((field, crt.ascending, crt.priority))\n                else:\n                    unordered.append((field, crt.ascending, None))\n        ordered.sort(key=(lambda pack: pack[2]))\n        for (field, asc, __) in chain(ordered, unordered):\n            if asc:\n                if (solrQuery is None):\n                    solrQuery = si.sort_by(field)\n                else:\n                    solrQuery = solrQuery.sort_by(field)\n            elif (solrQuery is None):\n                solrQuery = si.sort_by(('-' + field))\n            else:\n                solrQuery = solrQuery.sort_by(('-' + field))\n    return solrQuery",
    "nl": "Builds the Solr query based on a given REST query.\n\n",
    "original_nl": "Builds the Solr query based on a given REST query.\n\n@param si: SorlInterface\n    The current connection to Solr server.\n@param solrQuery: SolrSearch\n    The solr query to use.\n@param query: query\n    The REST query object to provide querying on.\n@param mapped: List\n    The list of OR clauses."
  },
  {
    "code": "def get(self, key):\n    if (key in self.binds):\n        return self.binds[key]\n    elif self.parent:\n        return self.parent.get(key)\n    else:\n        raise ValueError(('Invalid symbol ' + key))",
    "nl": "Getting a binding potentially requires the traversal\nof the parent link.",
    "original_nl": "Getting a binding potentially requires the traversal\nof the parent link."
  },
  {
    "code": "def _indent_line(self, line, stripspace=''):\n    return re.sub(('^%s' % stripspace), (self.indentstring * self.indent), line)",
    "nl": "indent the given line according to the current indent level.\n\nstripspace is a string of space that will be truncated from the\nstart of the line before indenting.",
    "original_nl": "indent the given line according to the current indent level.\n\nstripspace is a string of space that will be truncated from the\nstart of the line before indenting."
  },
  {
    "code": "def print_memory_usage():\n    try:\n        max_rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        logging.info('Executor Memory usage: {} MB'.format((max_rss / __rusage_denom_mb)))\n    except Exception:\n        logging.exception('Error in logging memory usage')",
    "nl": "Logs the memory usage of the executor.",
    "original_nl": "Logs the memory usage of the executor."
  },
  {
    "code": "@property\ndef created_at(self):\n    return snowflake_time(self.id)",
    "nl": "Returns the role's creation time in UTC.",
    "original_nl": "Returns the role's creation time in UTC."
  },
  {
    "code": "def __init__(self):\n    super().__init__(action_type=ActionType.OFPAT_COPY_TTL_OUT, length=8)",
    "nl": "Create an ActionCopyTTLOut.",
    "original_nl": "Create an ActionCopyTTLOut."
  },
  {
    "code": "def test_analysing_instr_interface_dependencies(monkeypatch, task_workbench, interface_dep_collector, i_profile_dep_collector, i_driver_dep_collector):\n\n    class FalseI(TaskInterface):\n        __slots__ = '__dict__'\n    plugin = task_workbench.get_plugin('exopy.tasks')\n    p_infos = TaskInfos(cls=InstrumentTask, instruments=['test'])\n    plugin._tasks.contributions['exopy.InstrumentTask'] = p_infos\n    p_infos.interfaces['tasks.FalseI'] = InterfaceInfos(cls=FalseI, instruments=['test'], parent=p_infos)\n    dep = set()\n    errors = dict()\n    i = FalseI()\n    t = InstrumentTask(selected_instrument=('test', 'dummy', 'c', None))\n    i.task = t\n    run = interface_dep_collector.analyse(task_workbench, i, getattr, dep, errors)\n    assert (run == {INSTR_RUNTIME_INTERFACE_DRIVERS_ID, INSTR_RUNTIME_INTERFACE_PROFILES_ID})\n    assert ('exopy.InstrumentTask:tasks.FalseI' in dep)\n    assert (not errors)\n    dep.clear()\n    i_profile_dep_collector.analyse(task_workbench, i, dep, errors)\n    assert ('test' in dep)\n    assert (not errors)\n    dep.clear()\n    i_driver_dep_collector.analyse(task_workbench, i, dep, errors)\n    assert ('dummy' in dep)\n    assert (not errors)\n    i.selected_instrument = ('test2', 'dummy2', 'c', None)\n    dep.clear()\n    i_profile_dep_collector.analyse(task_workbench, i, dep, errors)\n    assert ('test2' in dep)\n    assert (not errors)\n    dep.clear()\n    i_driver_dep_collector.analyse(task_workbench, i, dep, errors)\n    assert ('dummy2' in dep)\n    assert (not errors)",
    "nl": "Test analysing the dependencies of an interface.",
    "original_nl": "Test analysing the dependencies of an interface."
  },
  {
    "code": "def test_error_reporting(self):\n    expected_makerbot_driver = ['CRCMismatchError', 'CRCMismatchError', 'CRCMismatchError', 'GenericError', 'GenericError']\n    response_payload = bytearray()\n    response_payload.append(makerbot_driver.response_code_dict['CRC_MISMATCH'])\n    for i in range(3):\n        self.outputstream.write(makerbot_driver.Encoder.encode_payload(response_payload))\n    response_payload = bytearray()\n    response_payload.append(makerbot_driver.response_code_dict['GENERIC_PACKET_ERROR'])\n    for i in range(2):\n        self.outputstream.write(makerbot_driver.Encoder.encode_payload(response_payload))\n    self.outputstream.seek(0)\n    payload = 'asdf'\n    try:\n        self.w.send_command(payload)\n    except makerbot_driver.TransmissionError as e:\n        self.assertEqual(expected_makerbot_driver, e.value)",
    "nl": "Tests that StreamWriter records makerbot_driver received correctly\nand stores those values in the TransmissionError Thrown.",
    "original_nl": "Tests that StreamWriter records makerbot_driver received correctly\nand stores those values in the TransmissionError Thrown."
  },
  {
    "code": "def revert(apps, schema_editor):\n    MineralType = apps.get_model('stein', 'MineralType')\n    Cleavage = apps.get_model('stein', 'Cleavage')\n    for m in MineralType.objects.all():\n        cleavages = []\n        for cleav in m.cleavge_two.all():\n            cleavages.append(cleav.cleavage)\n        m.cleavage = cleavages\n        m.save()",
    "nl": "We also want to have the option to revert the above migration.",
    "original_nl": "We also want to have the option to revert the above migration."
  },
  {
    "code": "def __init__(self, center, radius, height, radius2=0, truncation_ratio=0, numSides=16, texture='common/caulk'):\n    size = np.array([(2 * radius), (2 * radius2), height], dtype=np.float)\n    super().__init__(center, size)\n    self.truncation_ratio = truncation_ratio\n    self.numSides = numSides\n    if isinstance(texture, str):\n        self.texture = defaultdict((lambda : texture))\n    else:\n        self.texture = defaultdict((lambda : 'common/caulk'), texture)",
    "nl": "\brief Generate a (truncated) cone with numSides sides\n\\param center center of the cone\n\\param radius radius of the base area\n\\param height height of the cone\n\\param radius2 if != 0 generate a cone with eliptical base area\n\\param truncation_ratio ratio of the radii of top to bottom face\n0 means no truncation at all\n\\param numSides number of sides of the cone\n\\param texture texture of the cone as string (applied to all faces)\nor as a dictionary('top', 'bottom' and 'sides') for individual faces",
    "original_nl": "\brief Generate a (truncated) cone with numSides sides\n\\param center center of the cone\n\\param radius radius of the base area\n\\param height height of the cone\n\\param radius2 if != 0 generate a cone with eliptical base area\n\\param truncation_ratio ratio of the radii of top to bottom face\n0 means no truncation at all\n\\param numSides number of sides of the cone\n\\param texture texture of the cone as string (applied to all faces)\nor as a dictionary('top', 'bottom' and 'sides') for individual faces"
  },
  {
    "code": "def _get_request_token(self):\n    if (self.request_token is None):\n        get_params = {\n            \n        }\n        if self.parameters:\n            get_params.update(self.parameters)\n        get_params['oauth_callback'] = self.request.build_absolute_uri(self.callback_url)\n        rt_url = ((self.request_token_url + '?') + urlencode(get_params))\n        oauth = OAuth1(self.consumer_key, client_secret=self.consumer_secret)\n        response = requests.post(url=rt_url, auth=oauth)\n        if (response.status_code not in [200, 201]):\n            raise OAuthError((_('Invalid response while obtaining request token from \"%s\".') % get_token_prefix(self.request_token_url)))\n        self.request_token = dict(parse_qsl(response.text))\n        self.request.session[('oauth_%s_request_token' % get_token_prefix(self.request_token_url))] = self.request_token\n    return self.request_token",
    "nl": "Obtain a temporary request token to authorize an access token and to\nsign the request to obtain the access token",
    "original_nl": "Obtain a temporary request token to authorize an access token and to\nsign the request to obtain the access token"
  },
  {
    "code": "def set_max_workers_count(worker_count):\n    global MAX_WORKERS_COUNT\n    MAX_WORKERS_COUNT = worker_count",
    "nl": "Set the global `MAX_WORKERS_COUNT`.",
    "original_nl": "Set the global `MAX_WORKERS_COUNT`."
  },
  {
    "code": "def _defaults(self):\n    self.packages = {\n        \n    }",
    "nl": "Set default data to config",
    "original_nl": "Set default data to config"
  },
  {
    "code": "def is64bitprocess(process_id):\n    from .sysinfo import is_x64_OS\n    is32 = True\n    if is_x64_OS():\n        phndl = win32api.OpenProcess(win32con.MAXIMUM_ALLOWED, 0, process_id)\n        if phndl:\n            is32 = win32process.IsWow64Process(phndl)\n    return (not is32)",
    "nl": "Return True if the specified process is a 64-bit process on x64\n",
    "original_nl": "Return True if the specified process is a 64-bit process on x64\n\nReturn False if it is only a 32-bit process running under Wow64.\nAlways return False for x86."
  },
  {
    "code": "@dbus.service.signal('org.freedesktop.Telepathy.Connection.Interface.Anonymity', signature='u')\ndef AnonymityModesChanged(self, Modes):\n    pass",
    "nl": "Emitted when the anonymity mode has changed.",
    "original_nl": "Emitted when the anonymity mode has changed."
  },
  {
    "code": "def specify_internal_edge_starts(self):\n    lanes = self.net_params.additional_params['lanes']\n    if (self.merge_out_len is not None):\n        ring_0_len = (((self.merge_out_angle - self.merge_in_angle) % (2 * pi)) * self.radius)\n        internal_edgestarts = [((':ring_0_%d' % lanes), 0), ((':ring_1_%d' % lanes), (self.ring_0_n_len + ring_0_len)), (':ring_0_0', ((- self.ring_0_0_len) + self.ring_0_n_len)), (':ring_1_0', (1000 * ((2 * pi) * self.radius)))]\n    else:\n        internal_edgestarts = [((':ring_0_%d' % lanes), 0), (':ring_1_0', (self.ring_0_n_len + (pi * self.radius))), (':ring_0_0', ((- self.ring_0_0_len) + self.ring_0_n_len))]\n    return internal_edgestarts",
    "nl": "See parent class",
    "original_nl": "See parent class"
  },
  {
    "code": "def __init__(self, creator, pool_size=10, loop=None, timeout=None, **kw):\n    self._creator = creator\n    self._closed = False\n    self._timeout = timeout\n    self._queue = asyncio.Queue(maxsize=pool_size, loop=loop)\n    self._connecting = 0\n    self._loop = self._queue._loop\n    self._logger = logger\n    self._in_use_connections = set()",
    "nl": "Construct an asynchronous Pool.\n\n",
    "original_nl": "Construct an asynchronous Pool.\n\n:param creator: a callable function that returns a connection object.\n\n:param pool_size: The size of the pool to be maintained,\n  defaults to 10. This is the largest number of connections that\n  will be kept persistently in the pool. Note that the pool\n  begins with no connections; once this number of connections\n  is requested, that number of connections will remain.\n\n:param timeout: The number of seconds to wait before giving up\n  on returning a connection. Defaults to 30."
  },
  {
    "code": "def get_icon(self, obj, is_expanded):\n    if (not self.allows_children(obj)):\n        return self.icon_item\n    if is_expanded:\n        return self.icon_open\n    return self.icon_group",
    "nl": "Returns the icon for a specified object.",
    "original_nl": "Returns the icon for a specified object."
  },
  {
    "code": "def metric(data, **kwargs):\n    return data.get('flops')",
    "nl": "Minimum required number of floating-point operations (flops).\n\nComputed as:\n    flops (atomic metric)\n\nflops:  minimal required mathematical flop count",
    "original_nl": "Minimum required number of floating-point operations (flops).\n\nComputed as:\n    flops (atomic metric)\n\nflops:  minimal required mathematical flop count"
  },
  {
    "code": "def create_column_family(self, keyspace, table_name, *args, **kwargs):\n    self.column_families[table_name] = {\n        'keyspace': keyspace,\n        'columns': {\n            \n        },\n        'indexes': {\n            \n        },\n    }",
    "nl": "Create a column family and record its existence.",
    "original_nl": "Create a column family and record its existence."
  },
  {
    "code": "def cosine_similarity(document1, document2):\n    dot_product = 0\n    for token in document1:\n        dot_product += (document1[token] * document2.get(token, 0))\n    document1_length = euclidian_length(document1)\n    document2_length = euclidian_length(document2)\n    if ((document1_length > 0) and (document2_length > 0)):\n        return (dot_product / (document1_length * document2_length))\n    else:\n        return 0.0",
    "nl": "Computes cosine similarity between two documents. If one document is\nquery (i.e. significantly shorter), put it as :document1:\n\nArgs:\n    document1: dictionary (token -> number of occurences)\n    document2: dictionary (token -> number of occurences)\n    Implementation note: put the shorter document as :document1:",
    "original_nl": "Computes cosine similarity between two documents. If one document is\nquery (i.e. significantly shorter), put it as :document1:\n\nArgs:\n    document1: dictionary (token -> number of occurences)\n    document2: dictionary (token -> number of occurences)\n    Implementation note: put the shorter document as :document1:\nReturns:\n    cosine similarity measure [float]"
  },
  {
    "code": "def get(self, *args, **kwargs):\n    try:\n        if ((len(args) > 2) or (len(args) < 1)):\n            raise ValueError('Invalid url')\n        tenant_id = uuid.UUID(args[0])\n        tenant = RUNTIME.tenants[tenant_id]\n        if (len(args) == 1):\n            self.write_as_json(tenant.lvnfs.values())\n            self.set_status(200, None)\n        else:\n            lvnf_id = uuid.UUID(args[1])\n            lvnf = tenant.lvnfs[lvnf_id]\n            self.write_as_json(lvnf)\n            self.set_status(200, None)\n    except ValueError as ex:\n        self.send_error(400, message=ex)\n    except KeyError as ex:\n        self.send_error(404, message=ex)",
    "nl": "List all Functions.\n\nArgs:\n    tenant_id: the network names of the tenant\n    lvnf_id: the address of the cpp\n\nExample URLs:\n\n    GET /api/v1/tenants/52313ecb-9d00-4b7d-b873-b55d3d9ada26/lvnfs\n    GET /api/v1/tenants/52313ecb-9d00-4b7d-b873-b55d3d9ada26/\n        lvnfs/49313ecb-9d00-4a7c-b873-b55d3d9ada34",
    "original_nl": "List all Functions.\n\nArgs:\n    tenant_id: the network names of the tenant\n    lvnf_id: the address of the cpp\n\nExample URLs:\n\n    GET /api/v1/tenants/52313ecb-9d00-4b7d-b873-b55d3d9ada26/lvnfs\n    GET /api/v1/tenants/52313ecb-9d00-4b7d-b873-b55d3d9ada26/\n        lvnfs/49313ecb-9d00-4a7c-b873-b55d3d9ada34"
  },
  {
    "code": "def calcDependentPoints(self, axispts, axes, posn):\n    s = self.settings\n    if ((axes[0] is None) or (axes[1] is None) or (axes[0].settings.direction != 'horizontal') or (axes[1].settings.direction != 'vertical')):\n        return (None, None)\n    if (axispts is None):\n        return (None, None)\n    compiled = self.document.evaluate.compileCheckedExpression(s.function)\n    if (not compiled):\n        return (None, None)\n    axis2 = (axes[1] if (s.variable == 'x') else axes[0])\n    env = self.initEnviron()\n    env[s.variable] = axispts\n    try:\n        results = (eval(compiled, env) + N.zeros(axispts.shape))\n        resultpts = axis2.dataToPlotterCoords(posn, results)\n    except Exception as e:\n        self.logEvalError(e)\n        results = None\n        resultpts = None\n    return (results, resultpts)",
    "nl": "Calculate the real and screen points to plot for the dependent axis",
    "original_nl": "Calculate the real and screen points to plot for the dependent axis"
  },
  {
    "code": "def refresh_view(self):\n    self._refresh_channels()",
    "nl": "Refresh the current view",
    "original_nl": "Refresh the current view"
  },
  {
    "code": "def generate(self):\n    self.structure_data.clear()\n    layer0 = Atoms()\n    scaling_matrix = [int(np.ceil(self.nx)), 1, int(np.ceil(self.nz))]\n    for atom in SuperCell(self.unit_cell, scaling_matrix):\n        layer0.append(Atom(**atom.todict()))\n    layer0.center_centroid()\n    self.layers = []\n    for nlayer in range(self.nlayers):\n        layer = copy.deepcopy(layer0)\n        layer.translate(Vector([0, (nlayer * self.layer_spacing), 0]))\n        [setattr(atom, 'mol', (nlayer + 1)) for atom in layer]\n        if ((nlayer % 2) != 0):\n            layer.translate(self.layer_shift)\n        layer.rotate(angle=self.layer_rotation_angles[nlayer], axis='z')\n        self.atoms.extend(layer)\n        self.layers.append(layer)",
    "nl": "Generate structure data.",
    "original_nl": "Generate structure data."
  },
  {
    "code": "def __publish__(self, event):\n    self.__publish_to_subscribers__(event)",
    "nl": "Publishes given event for subscribers in the application.\n\n",
    "original_nl": "Publishes given event for subscribers in the application.\n\n:param event: domain event or list of events"
  },
  {
    "code": "def push_broadcast(self):\n    for team_id in self.auth_connections:\n        self.push_team(team_id)",
    "nl": "Push to everyone",
    "original_nl": "Push to everyone"
  },
  {
    "code": "@permission_required('core.change_path')\ndef merge_path(request):\n    response = {\n        \n    }\n    if (request.method == 'POST'):\n        try:\n            ids_path_merge = request.POST.getlist('path[]')\n            if (len(ids_path_merge) == 2):\n                path_a = Path.objects.get(pk=ids_path_merge[0])\n                path_b = Path.objects.get(pk=ids_path_merge[1])\n                if ((not path_a.same_structure(request.user)) or (not path_b.same_structure(request.user))):\n                    response = {\n                        'error': _(\"You don't have the right to change these paths\"),\n                    }\n                elif path_a.merge_path(path_b):\n                    response = {\n                        'success': _('Paths merged successfully'),\n                    }\n                    messages.success(request, _('Paths merged successfully'))\n                else:\n                    response = {\n                        'error': _('No matching points to merge paths found'),\n                    }\n            else:\n                raise\n        except Exception as exc:\n            response = {\n                'error': exc,\n            }\n    return HttpResponse(json.dumps(response), content_type='application/json')",
    "nl": "Path merging view",
    "original_nl": "Path merging view"
  },
  {
    "code": "def all(self, ts=None):\n    [setattr(snapshot, 'selected', True) for snapshot in self.traj]\n    self.traj.nselected = self.traj.Nsnaps\n    self.traj.atom_selection.all()\n    self.print_fraction_selected()",
    "nl": "Select all trajectory snapshots/timesteps.",
    "original_nl": "Select all trajectory snapshots/timesteps."
  },
  {
    "code": "def Verify(self, mempool):\n    if ((self.Gas.value % 100000000) != 0):\n        return False\n    return super(InvocationTransaction, self).Verify(mempool)",
    "nl": "Verify the transaction.\n\nArgs:\n    mempool:\n",
    "original_nl": "Verify the transaction.\n\nArgs:\n    mempool:\n\nReturns:\n    bool: True if verified. False otherwise."
  },
  {
    "code": "def log_in(self, user, is_staff=False):\n    auth_page = AutoAuthPage(self.browser, staff=is_staff, username=user.get('username'), email=user.get('email'), password=user.get('password'))\n    auth_page.visit()",
    "nl": "Log in as the user that created the library.\nBy default the user will not have staff access unless is_staff is passed as True.",
    "original_nl": "Log in as the user that created the library.\nBy default the user will not have staff access unless is_staff is passed as True."
  },
  {
    "code": "def test_json_search_returns_expected_results(client, name_fixtures):\n    query = query_template.format('Personal', 'person')\n    response = client.get((reverse('name:search-json') + query))\n    assert (response.status_code is 200)\n    assert (json.loads(response.content)[0]['name'] == 'test person')",
    "nl": "Test that the search returns the Name object that was searched for\nand that the status code is 200.",
    "original_nl": "Test that the search returns the Name object that was searched for\nand that the status code is 200."
  },
  {
    "code": "def test_multiple_select(self):\n    for n in TestLazySorted.test_lengths:\n        xs = range(n)\n        ks = (2 * range(n))\n        for rep in xrange(10):\n            random.shuffle(xs)\n            random.shuffle(ks)\n            ls = LazySorted(xs)\n            for k in ks:\n                self.assertEqual(ls[k], k, msg=('xs = %s; ks = %s; k = %d' % (xs, ks, k)))",
    "nl": "Selection should work many times in a row",
    "original_nl": "Selection should work many times in a row"
  },
  {
    "code": "def stop(self):\n    if (self.every == (- 1)):\n        return\n    self.worker.stop()",
    "nl": "Stop control loop.",
    "original_nl": "Stop control loop."
  },
  {
    "code": "def pop(self, index=(- 1)):\n    children = list(self)\n    elem = children.pop(index)\n    self.parent.children.remove(elem)\n    return elem",
    "nl": "Remove and return element in children list",
    "original_nl": "Remove and return element in children list"
  },
  {
    "code": "def isvalidhtml(url):\n    if (url is None):\n        return False\n    try:\n        parsed = compat_urllib_parse_urlparse(url)\n        h = compat_http_client.HTTPConnection(parsed.netloc)\n        h.request('HEAD', parsed.path)\n        response = h.getresponse()\n        if (((response.status / 100) == 3) and response.getheader('Location')):\n            parsed = compat_urllib_parse_urlparse(response.getheader('Location'))\n            h = compat_http_client.HTTPConnection(parsed.netloc)\n            h.request('HEAD', parsed.path)\n            response = h.getresponse()\n            if ((response.status / 100) == 3):\n                return False\n        if (response.getheader('content-type') is None):\n            return False\n        if (response.getheader('content-type').find('text/html') != (- 1)):\n            return True\n        return False\n    except Exception as err:\n        print(('Header returned error: %s, skip not a valid HTML' % err))\n        return False",
    "nl": "Verify valid HTML content",
    "original_nl": "Verify valid HTML content"
  },
  {
    "code": "def __init__(self):\n    self.histogram = Histogram.get_biased()\n    self.meter = Meter('calls')",
    "nl": "Creates a new L{Timer} instance.",
    "original_nl": "Creates a new L{Timer} instance."
  },
  {
    "code": "def init(self, data_len):\n    self._t = 0\n    self._data_len = data_len\n    self._data = np.empty((0, data_len))\n    cm = plt.get_cmap('spectral')\n    self._plots = []\n    for i in range(data_len):\n        color = cm(((1.0 * i) / data_len))\n        alpha = (self._alphas[i] if (self._alphas is not None) else 1.0)\n        label = (self._labels[i] if (self._labels is not None) else str(i))\n        self._plots.append(self._ax.plot([], [], color=color, alpha=alpha, label=label)[0])\n    self._ax.set_xlim(0, self._time_window)\n    self._ax.set_ylim(0, 1)\n    self._ax.legend(loc='upper left', bbox_to_anchor=(0, 1.15))\n    self._init = True",
    "nl": "Initialize plots based off the length of the data array.",
    "original_nl": "Initialize plots based off the length of the data array."
  },
  {
    "code": "def locked(self):\n    return self._local.locked",
    "nl": "'Return the greenlet that acquire the lock or None.",
    "original_nl": "'Return the greenlet that acquire the lock or None."
  },
  {
    "code": "def register_procedure(procedure):\n    with closing(connection.cursor()) as cursor:\n        cursor.execute(procedure)",
    "nl": "Register the `procedure` SQL.",
    "original_nl": "Register the `procedure` SQL."
  },
  {
    "code": "def get_dimensions_in_mm(self):\n    return [((self.info['pixel_spacing'][x] * (self.info['pixel_data'].shape[x] - 1)) + self.info['pixel_size'][x]) for x in range(self.info['pixel_data'].ndim)]",
    "nl": "Get the dimensions in millimeters for each axis of the volume.",
    "original_nl": "Get the dimensions in millimeters for each axis of the volume.\nReturns a list of length 3.\n\nFor 3 pixels...\n::\n\n        +---+   +---+   +---+\n        | 1 |   | 2 |   | 3 |\n        +---+   +---+   +---+\n        \n        |-|-------|-------|-|\n         ^    ^       ^    ^\n         |    |       |    |\n         |    |       |    ---- 1/2 pixel_size\n         |    |       --------- 1 pixel_spacing\n         |    ----------------- 1 pixel_spacing\n         ---------------------- 1/2 pixel_size"
  },
  {
    "code": "def get_bought():\n    if (not __api.pc):\n        return []\n    return query(__api.pc.advans).where((lambda x: ((x.type == 'perk') and ((x.cost > 0) or (x.tag == 'merit'))))).to_list()",
    "nl": "return all merits that comes from advancements",
    "original_nl": "return all merits that comes from advancements"
  },
  {
    "code": "def mutUniformInt(individual, low, up, indpb):\n    size = len(individual)\n    if (not isinstance(low, Sequence)):\n        low = repeat(low, size)\n    elif (len(low) < size):\n        raise IndexError(('low must be at least the size of individual: %d < %d' % (len(low), size)))\n    if (not isinstance(up, Sequence)):\n        up = repeat(up, size)\n    elif (len(up) < size):\n        raise IndexError(('up must be at least the size of individual: %d < %d' % (len(up), size)))\n    for (i, xl, xu) in zip(xrange(size), low, up):\n        if (random.random() < indpb):\n            individual[i] = random.randint(xl, xu)\n    return (individual,)",
    "nl": "Mutate an individual by replacing attributes, with probability *indpb*,\nby a integer uniformly drawn between *low* and *up* inclusively.\n\n",
    "original_nl": "Mutate an individual by replacing attributes, with probability *indpb*,\nby a integer uniformly drawn between *low* and *up* inclusively.\n\n:param individual: :term:`Sequence <sequence>` individual to be mutated.\n:param low: The lower bound or a :term:`python:sequence` of\n            of lower bounds of the range from wich to draw the new\n            integer.\n:param up: The upper bound or a :term:`python:sequence` of\n           of upper bounds of the range from wich to draw the new\n           integer.\n:param indpb: Independent probability for each attribute to be mutated.\n:returns: A tuple of one individual."
  },
  {
    "code": "def fix_rounds(lst: list):\n    new_list = []\n    for a in lst:\n        if (len(a) > 0):\n            if isinstance(a[0], str):\n                new_list.append(a)\n        for b in a:\n            if isinstance(b, list):\n                new_list.append(b)\n    return new_list",
    "nl": "Used to clean round stats for matches with more than one map",
    "original_nl": "Used to clean round stats for matches with more than one map"
  },
  {
    "code": "def qft(q):\n    N = (2 ** q.n())\n    indices = range(N)\n    F = QuantumGate(((exp((((- 2j) * pi) / N)) ** outer(indices, indices)) / sqrt(N)))\n    (F * q)",
    "nl": "Performs the quantum Fourier transform on a system of qubits.",
    "original_nl": "Performs the quantum Fourier transform on a system of qubits."
  },
  {
    "code": "def get_hypernyms(synset, use_definitions=False):\n    return _get_lemma_names(synset.hypernyms, use_definitions=use_definitions)",
    "nl": "Extract hypernyms from a synset.\n\n",
    "original_nl": "Extract hypernyms from a synset.\n\n:param (object): The synset instance.\n:param use_definitions (bool, optional):\n    Extract definitions from the synset.\n:rtype list: The results list."
  },
  {
    "code": "def get_period_list(from_fiscal_year, to_fiscal_year, periodicity, accumulated_values=False, company=None, reset_period_on_fy_change=True):\n    fiscal_year = get_fiscal_year_data(from_fiscal_year, to_fiscal_year)\n    validate_fiscal_year(fiscal_year, from_fiscal_year, to_fiscal_year)\n    year_start_date = getdate(fiscal_year.year_start_date)\n    year_end_date = getdate(fiscal_year.year_end_date)\n    months_to_add = {\n        'Yearly': 12,\n        'Half-Yearly': 6,\n        'Quarterly': 3,\n        'Monthly': 1,\n    }[periodicity]\n    period_list = []\n    start_date = year_start_date\n    months = get_months(year_start_date, year_end_date)\n    for i in range((months // months_to_add)):\n        period = frappe._dict({\n            'from_date': start_date,\n        })\n        to_date = add_months(start_date, months_to_add)\n        start_date = to_date\n        if (to_date == get_first_day(to_date)):\n            to_date = add_days(to_date, (- 1))\n        if (to_date <= year_end_date):\n            period.to_date = to_date\n        else:\n            period.to_date = year_end_date\n        period.to_date_fiscal_year = get_fiscal_year(period.to_date, company=company)[0]\n        period.from_date_fiscal_year_start_date = get_fiscal_year(period.from_date, company=company)[1]\n        period_list.append(period)\n        if (period.to_date == year_end_date):\n            break\n    for opts in period_list:\n        key = opts['to_date'].strftime('%b_%Y').lower()\n        if ((periodicity == 'Monthly') and (not accumulated_values)):\n            label = formatdate(opts['to_date'], 'MMM YYYY')\n        elif (not accumulated_values):\n            label = get_label(periodicity, opts['from_date'], opts['to_date'])\n        elif reset_period_on_fy_change:\n            label = get_label(periodicity, opts.from_date_fiscal_year_start_date, opts['to_date'])\n        else:\n            label = get_label(periodicity, period_list[0].from_date, opts['to_date'])\n        opts.update({\n            'key': key.replace(' ', '_').replace('-', '_'),\n            'label': label,\n            'year_start_date': year_start_date,\n            'year_end_date': year_end_date,\n        })\n    return period_list",
    "nl": "Get a list of dict {\"from_date\": from_date, \"to_date\": to_date, \"key\": key, \"label\": label}\nPeriodicity can be (Yearly, Quarterly, Monthly)",
    "original_nl": "Get a list of dict {\"from_date\": from_date, \"to_date\": to_date, \"key\": key, \"label\": label}\nPeriodicity can be (Yearly, Quarterly, Monthly)"
  },
  {
    "code": "def exists(container):\n    if (container in ls()):\n        return True\n    return False",
    "nl": "Check if container exists",
    "original_nl": "Check if container exists"
  },
  {
    "code": "def testGetHasherNamesFromString(self):\n    test_strings = 'md5,sha256,testhash'\n    manager.HashersManager.RegisterHasher(TestHasher)\n    names = manager.HashersManager.GetHasherNamesFromString(test_strings)\n    self.assertEqual(3, len(names))\n    manager.HashersManager.DeregisterHasher(TestHasher)\n    names = manager.HashersManager.GetHasherNamesFromString(test_strings)\n    self.assertEqual(2, len(names))\n    names = manager.HashersManager.GetHasherNamesFromString('all')\n    self.assertEqual(len(names), len(manager.HashersManager._hasher_classes))",
    "nl": "Tests the GetHasherNamesFromString method.",
    "original_nl": "Tests the GetHasherNamesFromString method."
  },
  {
    "code": "def __init__(self, api_key):\n    self.header = {\n        'X-YCHARTSAUTHORIZATION': api_key,\n    }",
    "nl": "Args:\n    api_key (str): The API key that authorizes a client to query security data",
    "original_nl": "Args:\n    api_key (str): The API key that authorizes a client to query security data"
  },
  {
    "code": "def get_ship_health_hull(image):\n    rgb = get_brightest_pixel(image)\n    health = {\n        'red': 25,\n        'orange': 50,\n        'yellow': 75,\n        'green': 100,\n    }\n    for (name, pair) in colors.items():\n        if (get_similarity_pixels(rgb, pair) >= 80):\n            if (name not in health):\n                return None\n            return health[name]\n    return None",
    "nl": "Uses the PIL library to determine the color of the ship icon in the\nUI to make an approximation of the ship hull health.",
    "original_nl": "Uses the PIL library to determine the color of the ship icon in the\nUI to make an approximation of the ship hull health."
  },
  {
    "code": "def get_galaxy_spectrum(self, **params):\n    self.update(**params)\n    spectra = []\n    mass = np.atleast_1d(self.params['mass']).copy()\n    mfrac = np.zeros_like(mass)\n    for (i, m) in enumerate(mass):\n        self.update_component(i)\n        (wave, spec) = self.ssp.get_spectrum(tage=self.ssp.params['tage'], peraa=False)\n        spectra.append(spec)\n        mfrac[i] = self.ssp.stellar_mass\n    if np.all((self.params.get('mass_units', 'mformed') == 'mstar')):\n        mass /= mfrac\n    spectrum = (np.dot(mass, np.array(spectra)) / mass.sum())\n    mfrac_sum = (np.dot(mass, mfrac) / mass.sum())\n    return (wave, np.squeeze((spectra + [spectrum])), np.squeeze((mfrac.tolist() + [mfrac_sum])))",
    "nl": "Update parameters, then loop over each component getting a spectrum\nfor each.  Return all the component spectra, plus the sum.\n\n",
    "original_nl": "Update parameters, then loop over each component getting a spectrum\nfor each.  Return all the component spectra, plus the sum.\n\n:param params:\n    A parameter dictionary that gets passed to the ``self.update``\n    method and will generally include physical parameters that control\n    the stellar population and output spectrum or SED, some of which\n    may be vectors for the different componenets\n\n:returns wave:\n    Wavelength in angstroms.\n\n:returns spectrum:\n    Spectrum in units of Lsun/Hz/solar masses formed.  ndarray of\n    shape(ncomponent+1, nwave).  The last element is the sum of the\n    previous elements.\n\n:returns mass_fraction:\n    Fraction of the formed stellar mass that still exists, ndarray of\n    shape (ncomponent+1,)"
  },
  {
    "code": "def add_words(self, ws, score=1):\n    self.add_scored_words(((w, score) for w in ws))",
    "nl": "Adds a list of words to the backend dictionary.\n\n",
    "original_nl": "Adds a list of words to the backend dictionary.\n\n:param ws: A sequence of words (strings) to add to the dictionary.\n:param score: An optional score to use for ALL the words in 'ws'."
  },
  {
    "code": "def get_spells(ring, mastery, maho=True):\n    including_maho = query(all()).where((lambda x: ((x.element == ring) and (x.mastery == mastery))))\n    if (not maho):\n        return query(including_maho).where((lambda x: ('maho' not in tags(x.id)))).to_list()\n    return including_maho.to_list()",
    "nl": "returns all the maho spells for the given ring and mastery, if maho include maho spells",
    "original_nl": "returns all the maho spells for the given ring and mastery, if maho include maho spells"
  },
  {
    "code": "@staticmethod\ndef minimize(arraylist, axistag):\n    mininds = argmin_stack([data.data_view(axistag) for data in arraylist])\n    minimizedarray = arraylist[0]._template()\n    paramstack = stack([data._dependent_params for data in arraylist])\n    minimizedarray._dependent_params = np.choose(mininds, paramstack)\n    return minimizedarray",
    "nl": "Create a new ThermoArray by taking the values corresponding\nto the minimum values of a certain parameter (e.g. free energy)\n\n:arraylist: [ThermoArray], must match dimensions\n:axistag: string\n",
    "original_nl": "Create a new ThermoArray by taking the values corresponding\nto the minimum values of a certain parameter (e.g. free energy)\n\n:arraylist: [ThermoArray], must match dimensions\n:axistag: string\n:returns: ThermoArray"
  },
  {
    "code": "@pytest.mark.parametrize('key, mat, quad', mats_and_quads)\ndef test_mat(key, mat, quad):\n    test = key[0]\n    trial = key[1]\n    testfunction = (test[0](N, quad=quad), test[1])\n    trialfunction = (trial[0](N, quad=quad), trial[1])\n    mat = mat(testfunction, trialfunction)\n    shenfun.check_sanity(mat, testfunction, trialfunction)",
    "nl": "Test that matrices built by hand equals those automatically generated",
    "original_nl": "Test that matrices built by hand equals those automatically generated"
  },
  {
    "code": "@property\ndef pip_bin(self):\n    return os.path.join(self.virtualenv_dir, 'bin', 'pip')",
    "nl": "Path to pip binary.",
    "original_nl": "Path to pip binary."
  },
  {
    "code": "def test_easybuildlog(self):\n    (fd, tmplog) = tempfile.mkstemp()\n    os.close(fd)\n    depr_ver = int(os.environ['EASYBUILD_DEPRECATED'])\n    older_ver = str((depr_ver - 1))\n    newer_ver = str((depr_ver + 1))\n    setLogFormat('%(name)s [%(levelname)s] :: %(message)s')\n    logToFile(tmplog, enable=True)\n    log = getLogger('test_easybuildlog')\n    self.mock_stderr(True)\n    log.setLevelName('DEBUG')\n    log.debug('123 debug')\n    log.info('foobar info')\n    log.warn('justawarning')\n    log.deprecated('anotherwarning', newer_ver)\n    log.deprecated('onemorewarning', '1.0', '2.0')\n    log.deprecated('lastwarning', '1.0', max_ver='2.0')\n    log.error('kaput')\n    log.error('err: %s', 'msg: %s')\n    stderr = self.get_stderr()\n    self.mock_stderr(False)\n    more_info = 'see http://easybuild.readthedocs.org/en/latest/Deprecated-functionality.html for more information'\n    expected_stderr = ('\\n'.join([('Deprecated functionality, will no longer work in v10000001: anotherwarning; ' + more_info), 'Deprecated functionality, will no longer work in v2.0: onemorewarning', 'Deprecated functionality, will no longer work in v2.0: lastwarning']) + '\\n')\n    self.assertEqual(stderr, expected_stderr)\n    try:\n        log.exception('oops')\n    except EasyBuildError:\n        pass\n    logToFile(tmplog, enable=False)\n    logtxt = read_file(tmplog)\n    root = getRootLoggerName()\n    expected_logtxt = '\\n'.join([('%s.test_easybuildlog \\\\[DEBUG\\\\] :: 123 debug' % root), ('%s.test_easybuildlog \\\\[INFO\\\\] :: foobar info' % root), ('%s.test_easybuildlog \\\\[WARNING\\\\] :: justawarning' % root), ('%s.test_easybuildlog \\\\[WARNING\\\\] :: Deprecated functionality.*anotherwarning.*' % root), ('%s.test_easybuildlog \\\\[WARNING\\\\] :: Deprecated functionality.*onemorewarning.*' % root), ('%s.test_easybuildlog \\\\[WARNING\\\\] :: Deprecated functionality.*lastwarning.*' % root), ('%s.test_easybuildlog \\\\[ERROR\\\\] :: EasyBuild crashed with an error \\\\(at .* in .*\\\\): kaput' % root), (root + '.test_easybuildlog \\\\[ERROR\\\\] :: EasyBuild crashed with an error \\\\(at .* in .*\\\\): err: msg: %s'), ('%s.test_easybuildlog \\\\[ERROR\\\\] :: .*EasyBuild encountered an exception \\\\(at .* in .*\\\\): oops' % root), ''])\n    logtxt_regex = re.compile(('^%s' % expected_logtxt), re.M)\n    self.assertTrue(logtxt_regex.search(logtxt), (\"Pattern '%s' found in %s\" % (logtxt_regex.pattern, logtxt)))\n    self.assertErrorRegex(EasyBuildError, 'DEPRECATED \\\\(since .*: kaput', log.deprecated, 'kaput', older_ver)\n    self.assertErrorRegex(EasyBuildError, 'DEPRECATED \\\\(since .*: 2>1', log.deprecated, '2>1', '2.0', '1.0')\n    self.assertErrorRegex(EasyBuildError, 'DEPRECATED \\\\(since .*: 2>1', log.deprecated, '2>1', '2.0', max_ver='1.0')\n    write_file(tmplog, '')\n    logToFile(tmplog, enable=True)\n    (log.warn('%s', 'bleh'),)\n    log.info('%s+%s = %d', '4', '2', 42)\n    args = ['this', 'is', 'just', 'a', 'test']\n    log.debug('%s %s %s %s %s', *args)\n    log.error('foo %s baz', 'baz')\n    logToFile(tmplog, enable=False)\n    logtxt = read_file(tmplog)\n    expected_logtxt = '\\n'.join([('%s.test_easybuildlog \\\\[WARNING\\\\] :: bleh' % root), ('%s.test_easybuildlog \\\\[INFO\\\\] :: 4\\\\+2 = 42' % root), ('%s.test_easybuildlog \\\\[DEBUG\\\\] :: this is just a test' % root), ('%s.test_easybuildlog \\\\[ERROR\\\\] :: EasyBuild crashed with an error \\\\(at .* in .*\\\\): foo baz baz' % root), ''])\n    logtxt_regex = re.compile(('^%s' % expected_logtxt), re.M)\n    self.assertTrue(logtxt_regex.search(logtxt), (\"Pattern '%s' found in %s\" % (logtxt_regex.pattern, logtxt)))",
    "nl": "Tests for EasyBuildLog.",
    "original_nl": "Tests for EasyBuildLog."
  },
  {
    "code": "@click.command()\n@click.argument('directory', type=click.Path(exists=True), required=True)\ndef upload(directory):\n    if (not AWS_BUCKET):\n        utils.error('AWS_BUCKET environment variable not set. Exiting.')\n        return\n    conn = S3Connection()\n    bucket = get_or_create_bucket(conn, AWS_BUCKET)\n    files = list(utils.get_files(directory))\n    total_size = 0\n    utils.info('Found', len(files), ('files to upload to s3://' + AWS_BUCKET))\n    for path in files:\n        filesize = os.path.getsize(path)\n        total_size += filesize\n        utils.info('Uploading', path, '-', sizeof_fmt(filesize))\n        k = Key(bucket)\n        k.key = path\n        k.set_contents_from_filename(path)\n    utils.success('Done. Uploaded', sizeof_fmt(total_size))",
    "nl": "Upload a directory to S3.\n\nDIRECTORY: Directory to upload. Required.",
    "original_nl": "Upload a directory to S3.\n\nDIRECTORY: Directory to upload. Required."
  },
  {
    "code": "def test_interface(self):\n    observer = FilteringLogObserver((lambda e: None), ())\n    try:\n        verifyObject(ILogObserver, observer)\n    except BrokenMethodImplementation as e:\n        self.fail(e)",
    "nl": "L{FilteringLogObserver} is an L{ILogObserver}.",
    "original_nl": "L{FilteringLogObserver} is an L{ILogObserver}."
  },
  {
    "code": "def test_header_logo_links_to_marketing_site_with_site_override(self):\n    self.use_site(self.site_other)\n    self.client.login(username=self.user.username, password='password')\n    response = self.client.get(reverse('dashboard'))\n    self.assertIn(self.site_configuration_other.values['MKTG_URLS']['ROOT'], response.content)",
    "nl": "Test marketing site root link is included on dashboard page\nif MKTG_URLS['ROOT'] is set in SiteConfiguration",
    "original_nl": "Test marketing site root link is included on dashboard page\nif MKTG_URLS['ROOT'] is set in SiteConfiguration"
  },
  {
    "code": "def create_candidate_selection_rows(xml_root, batch_set_id=0):\n    success = False\n    status = ''\n    candidate_selection_created = False\n    candidate_selection_xml_node = xml_root.findall('CandidateSelection')\n    candidate_selection = ''\n    number_of_batch_rows = 0\n    for one_candidate_selection in candidate_selection_xml_node:\n        candidate_selection_id = one_candidate_selection.attrib['id']\n        contest_office_id_node = one_candidate_selection.find('./CandidateIds')\n        if (contest_office_id_node is None):\n            candidate_selection = CandidateSelection()\n            results = {\n                'success': False,\n                'status': 'CREATE_CANDIDATE_SELECTION_ROWS-CONTEST_OFFICE_ID_NOT_FOUND',\n                'candidate_selection_created': False,\n                'candidate_selection': candidate_selection,\n            }\n            return results\n        contest_office_id = contest_office_id_node.text\n        try:\n            candidate_selection = CandidateSelection.objects.create(batch_set_id=batch_set_id, candidate_selection_id=candidate_selection_id, contest_office_id=contest_office_id)\n            if candidate_selection:\n                candidate_selection_created = True\n                success = True\n                status = 'CANDIDATE_SELECTION_CREATED'\n                number_of_batch_rows += 1\n        except Exception as e:\n            candidate_selection_created = False\n            candidate_selection = CandidateSelection()\n            success = False\n            status = 'CANDIDATE_SELECTION_NOT_CREATED'\n            handle_exception(e, logger=logger, exception_message=status)\n    results = {\n        'success': success,\n        'status': status,\n        'candidate_selection_created': candidate_selection_created,\n        'candidate_selection': candidate_selection,\n        'number_of_batch_rows': number_of_batch_rows,\n    }\n    return results",
    "nl": "Create candidate selection entries in the CandidateSelection table based on CTCL XML CandidateSelection node values\n",
    "original_nl": "Create candidate selection entries in the CandidateSelection table based on CTCL XML CandidateSelection node values\n:param xml_root: \n:param batch_set_id: \n:return:"
  },
  {
    "code": "def mkdir(argc):\n    os.mkdir(os.path.expanduser(argc.args['DIR']))",
    "nl": "mkdir: Make a directory.\n\nUsage:\n   mkdir DIR",
    "original_nl": "mkdir: Make a directory.\n\nUsage:\n   mkdir DIR"
  },
  {
    "code": "def genKaldiDatadir(recordings_path, kaldi_datadir_path):\n    _db = MySQLdb.connect(**dbConst)\n    SAMPLERATE = 16000\n    REC_COUNT_THRESHOLD = 50\n    try:\n        cur = _db.cursor()\n        cur.execute('SELECT recording.id, tokenId, recording.speakerId, sessionId, filename FROM ( SELECT speakerId, COUNT(speakerId)        AS rCnt FROM recording GROUP BY speakerId ) AS t1,      recording WHERE recording.speakerId = t1.speakerId AND t1.rCnt > %s', (REC_COUNT_THRESHOLD,))\n        recordings = cur.fetchall()\n    except MySQLdb.Error as e:\n        msg = 'Error getting recordings'\n        log(msg, e)\n        raise\n    os.makedirs(kaldi_datadir_path, exist_ok=True)\n    text = open('{}/text'.format(kaldi_datadir_path), 'w')\n    wavscp = open('{}/wav.scp'.format(kaldi_datadir_path), 'w')\n    utt2spk = open('{}/utt2spk'.format(kaldi_datadir_path), 'w')\n    spk2utt = open('{}/spk2utt'.format(kaldi_datadir_path), 'w')\n    for rec in recordings:\n        recId = rec[0]\n        tokenId = rec[1]\n        speakerId = rec[2]\n        sessionId = rec[3]\n        filename = rec[4]\n        try:\n            cur = _db.cursor()\n            cur.execute('SELECT inputToken FROM token WHERE id=%s', (tokenId,))\n            token = cur.fetchone()\n            if (token is None):\n                raise ValueError(\"Couldn't find token with id={}\".format(tokenId))\n            else:\n                token = token[0]\n        except MySQLdb.Error as e:\n            msg = 'Error getting tokens'\n            log(msg, e)\n            raise\n        utt_id = '{}-{}'.format(speakerId, recId)\n        spkr = str(speakerId)\n        full_rec_path = os.path.abspath(os.path.join(recordings_path, 'session_{}'.format(sessionId), filename))\n        print(utt_id, token.lower(), file=text)\n        print('{utt_id} sox - -c1 -esigned -r{samplerate} -twav - < {full_rec_path} | '.format(utt_id=utt_id, samplerate=SAMPLERATE, full_rec_path=full_rec_path), file=wavscp)\n        print(utt_id, spkr, file=utt2spk)\n    text.close()\n    wavscp.close()\n    utt2spk.close()\n    os.system('utils/utt2spk_to_spk2utt.pl < {datadir}/utt2spk > {datadir}/spk2utt'.format(datadir=kaldi_datadir_path))\n    os.system('utils/validate_data_dir.sh --no-feats {datadir} || utils/fix_data_dir.sh {datadir}'.format(datadir=kaldi_datadir_path))\n    spk2utt.close()",
    "nl": "See description, the de-script-ion of the script.",
    "original_nl": "See description, the de-script-ion of the script."
  },
  {
    "code": "def constituency_head(self):\n    pass",
    "nl": "Returns a reference to the lowest common ancestor in the\nconstituency tree.",
    "original_nl": "Returns a reference to the lowest common ancestor in the\nconstituency tree."
  },
  {
    "code": "def load_input_reader(input_params, readonly=False):\n    if (not isinstance(input_params, dict)):\n        raise TypeError('input_params must be a dictionary')\n    if ('abstract' in input_params):\n        driver_name = input_params['abstract']['format']\n    elif ('path' in input_params):\n        input_file = input_params['path']\n        driver_name = driver_from_file(input_file)\n    else:\n        raise errors.MapcheteDriverError(('invalid input parameters %s' % input_params))\n    for v in pkg_resources.iter_entry_points(_DRIVERS_ENTRY_POINT):\n        driver_ = v.load()\n        if (hasattr(driver_, 'METADATA') and (driver_.METADATA['driver_name'] == driver_name)):\n            return v.load().InputData(input_params, readonly=readonly)\n    raise errors.MapcheteDriverError((\"no loader for driver '%s' could be found.\" % driver_name))",
    "nl": "Return input class of driver.\n",
    "original_nl": "Return input class of driver.\n\nReturns\n-------\ninput_params : ``InputData``\n    input parameters"
  },
  {
    "code": "def test_7(self):\n    self.qpart.detectSyntax(language='Python')\n    QTest.keyClicks(self.qpart, 'def main():')\n    QTest.keyClick(self.qpart, Qt.Key_Enter)\n    self.assertEqual(self.qpart.cursorPosition, (1, 4))\n    QTest.keyClicks(self.qpart, 'return 7')\n    QTest.keyClick(self.qpart, Qt.Key_Enter)\n    self.assertEqual(self.qpart.cursorPosition, (2, 0))",
    "nl": "Smartly indent python",
    "original_nl": "Smartly indent python"
  },
  {
    "code": "def moderate(self, comment, content_object, request):\n    if models.moderation_enabled():\n        return True\n    return False",
    "nl": "Return ``True`` if the comment should be moderated (marked\nnon-public), ``False`` otherwise.",
    "original_nl": "Return ``True`` if the comment should be moderated (marked\nnon-public), ``False`` otherwise."
  },
  {
    "code": "def test_unpack_http_url_with_urllib_response_without_content_type(data):\n    _real_session = PipSession()\n\n    def _fake_session_get(*args, **kwargs):\n        resp = _real_session.get(*args, **kwargs)\n        del resp.headers['Content-Type']\n        return resp\n    session = Mock()\n    session.get = _fake_session_get\n    uri = path_to_url(data.packages.join('simple-1.0.tar.gz'))\n    link = Link(uri)\n    temp_dir = mkdtemp()\n    try:\n        unpack_http_url(link, temp_dir, download_dir=None, session=session)\n        assert (set(os.listdir(temp_dir)) == set(['PKG-INFO', 'setup.cfg', 'setup.py', 'simple', 'simple.egg-info']))\n    finally:\n        rmtree(temp_dir)",
    "nl": "It should download and unpack files even if no Content-Type header exists",
    "original_nl": "It should download and unpack files even if no Content-Type header exists"
  },
  {
    "code": "def sinval(modes, coord):\n    modes_dims = tuple([di for di in modes.dims if (di is not 'periods')])\n    modes_shape = tuple([modes.dims[di] for di in modes_dims])\n    modes_chunks = tuple((modes.chunks[di][0] for di in modes_dims))\n    if (coord.chunks is None):\n        coord_chunks = (coord.shape[0],)\n    else:\n        coord_chunks = (coord.chunks[0][0],)\n    new_dims = (coord.dims + modes_dims)\n    new_shape = (coord.shape + modes_shape)\n    new_chunks = (coord_chunks + modes_chunks)\n    ones = xr.DataArray(da.ones(new_shape, chunks=new_chunks), dims=new_dims)\n    if pd.core.common.is_datetime64_dtype(coord):\n        t = ((ones * coord.astype('f8')) * 1e-09)\n        pd_periods = pd.to_datetime(modes['periods'], unit=modes['periods'].units)\n        if _utils.is_scalar(modes['periods'].data):\n            periods = (pd_periods.value.astype('f8') * 1e-09)\n        else:\n            periods = (pd_periods.values.astype('f8') * 1e-09)\n    else:\n        t = (ones * coord)\n        periods = modes['periods']\n    res = (ones * modes['offset'])\n    for p in range(len(periods)):\n        modep = (ones * modes.isel(periods=p))\n        res += (modep['amplitude'] * xr.ufuncs.sin(((((2 * np.pi) * t) / periods[p]) + ((modep['phase'] * np.pi) / 180.0))))\n    return res",
    "nl": "Evaluate a sinusoidal function based on a modal decomposition. Each mode is\ndefined by a period, an amplitude and a phase. This function may usually\nbe used after a sinusoidal fit using\n:py:func:`xscale.signal.fitting.sinfit`.\n",
    "original_nl": "Evaluate a sinusoidal function based on a modal decomposition. Each mode is\ndefined by a period, an amplitude and a phase. This function may usually\nbe used after a sinusoidal fit using\n:py:func:`xscale.signal.fitting.sinfit`.\n\nParameters\n----------\nmodes : xarray.Dataset\n        A dataset where the amplitude and phase are stored for each mode\ncoord : xarray.Coordinates\n        A coordinate array at which the sine functions are evaluated\n\nReturns\n-------\nres : xarray.DataArray"
  },
  {
    "code": "def __init__(self, date=None):\n    if (date is None):\n        self.fromDateTime(datetime.now())\n    elif (isinstance(date, str) is True):\n        self.fromString(date)\n    elif (isinstance(date, datetime) is True):\n        self.fromDateTime(date)",
    "nl": "constructor\ndate parameter can be suplied",
    "original_nl": "constructor\ndate parameter can be suplied"
  },
  {
    "code": "def write(self, msg, level=logging.INFO):\n    if ((msg is not None) and (len(msg.strip()) > 0)):\n        self.logger.log(level, msg)",
    "nl": "method implements stream write interface, allowing to redirect stdout to logger",
    "original_nl": "method implements stream write interface, allowing to redirect stdout to logger"
  },
  {
    "code": "def add_resource_specs(self, resource_specs):\n    for (key, value) in resource_specs.items():\n        self.add_resource_spec(key, value)",
    "nl": "Add resources to specification",
    "original_nl": "Add resources to specification"
  },
  {
    "code": "def regex(self, *pattern, **kwargs):\n    set_defaults(self._kwargs, kwargs)\n    set_defaults(self._regex_defaults, kwargs)\n    set_defaults(self._defaults, kwargs)\n    pattern = self.rebulk.build_re(*pattern, **kwargs)\n    part = ChainPart(self, pattern)\n    self.parts.append(part)\n    return part",
    "nl": "Add re pattern\n\n",
    "original_nl": "Add re pattern\n\n:param pattern:\n:type pattern:\n:param kwargs:\n:type kwargs:\n:return:\n:rtype:"
  },
  {
    "code": "def bvshl(a, b):\n    return '(bvshl {} {})'.format(a, b)",
    "nl": "Shift left: a << b",
    "original_nl": "Shift left: a << b"
  },
  {
    "code": "def create_environment_for_seismic(self, x=15, y=15):\n    square = [self.v_p, self.v_s, self.density, self.elasticity_quotient, self.mu_lame]\n    field = np.ndarray(shape=(x, y), dtype=np.dtype(list))\n    field.fill(square)\n    return field",
    "nl": "One of the main functions in class <Environment_properties> which returns the created environment field\nfor seismic task according to the pack of input parameters:\n        (density, v_p, v_c) or (density, elasticity_quotient, mu_lame)\n\nEach element of returned <ndarray> contains list [v_p, v_s, density, elasticity_quotient, mu_lame]\n        which will be used in further calculations\n\n",
    "original_nl": "One of the main functions in class <Environment_properties> which returns the created environment field\nfor seismic task according to the pack of input parameters:\n        (density, v_p, v_c) or (density, elasticity_quotient, mu_lame)\n\nEach element of returned <ndarray> contains list [v_p, v_s, density, elasticity_quotient, mu_lame]\n        which will be used in further calculations\n\n:param x:   Width\n:param y:   Height\n:return field:    numpy.ndarray(shape=(x, y))"
  },
  {
    "code": "def authenticate(self, *args, **kwargs):\n    try:\n        return super(OAuth2AuthenticationAllowInactiveUser, self).authenticate(*args, **kwargs)\n    except AuthenticationFailed:\n        raise\n    except drf_exceptions.AuthenticationFailed as exc:\n        if ('No credentials provided' in exc.detail):\n            error_code = OAUTH2_TOKEN_ERROR_NOT_PROVIDED\n        elif ('Token string should not contain spaces' in exc.detail):\n            error_code = OAUTH2_TOKEN_ERROR_MALFORMED\n        else:\n            error_code = OAUTH2_TOKEN_ERROR\n        raise AuthenticationFailed({\n            'error_code': error_code,\n            'developer_message': exc.detail,\n        })",
    "nl": "Returns two-tuple of (user, token) if access token authentication\nsucceeds, raises an AuthenticationFailed (HTTP 401) if authentication\nfails or None if the user did not try to authenticate using an access\ntoken.\n\nOverrides base class implementation to return edX-style error\nresponses.",
    "original_nl": "Returns two-tuple of (user, token) if access token authentication\nsucceeds, raises an AuthenticationFailed (HTTP 401) if authentication\nfails or None if the user did not try to authenticate using an access\ntoken.\n\nOverrides base class implementation to return edX-style error\nresponses."
  },
  {
    "code": "@classmethod\ndef GetFormatSpecification(cls):\n    format_specification = specification.FormatSpecification(cls.NAME)\n    format_specification.AddNewSignature(b'LfLe', offset=4)\n    return format_specification",
    "nl": "Retrieves the format specification.\n",
    "original_nl": "Retrieves the format specification.\n\nReturns:\n  FormatSpecification: format specification."
  },
  {
    "code": "@pytest.fixture\ndef observatory(config, simulator):\n    obs = Observatory(config=config, simulator=simulator, ignore_local_config=True)\n    return obs",
    "nl": "Return a valid Observatory instance with a specific config.",
    "original_nl": "Return a valid Observatory instance with a specific config."
  },
  {
    "code": "def __str__(self):\n    return ((str(self.lhs) + ' * ') + str(self.rhs))",
    "nl": "Return the string expression of this object.",
    "original_nl": "Return the string expression of this object."
  },
  {
    "code": "def get_course_enrollment(username, course_id):\n    course_key = CourseKey.from_string(course_id)\n    try:\n        enrollment = CourseEnrollment.objects.get(user__username=username, course_id=course_key)\n        return CourseEnrollmentSerializer(enrollment).data\n    except CourseEnrollment.DoesNotExist:\n        return None",
    "nl": "Retrieve an object representing all aggregated data for a user's course enrollment.\n\nGet the course enrollment information for a specific user and course.\n\nArgs:\n    username (str): The name of the user to retrieve course enrollment information for.\n    course_id (str): The course to retrieve course enrollment information for.\n",
    "original_nl": "Retrieve an object representing all aggregated data for a user's course enrollment.\n\nGet the course enrollment information for a specific user and course.\n\nArgs:\n    username (str): The name of the user to retrieve course enrollment information for.\n    course_id (str): The course to retrieve course enrollment information for.\n\nReturns:\n    A serializable dictionary representing the course enrollment."
  },
  {
    "code": "def set_cell_sizes(context, ob, desired_cell_sizes, snap_to_origin=True, poisson_restriction=True):\n    (current_xbs, msg) = geometry.to_fds.ob_to_xbs_bbox(context, ob)\n    (x0, x1, y0, y1, z0, z1) = current_xbs[0]\n    current_ijk = ob.bf_mesh_ijk\n    new_ijk = ((round((abs((x1 - x0)) / desired_cell_sizes[0])) or 1), (round((abs((y1 - y0)) / desired_cell_sizes[1])) or 1), (round((abs((z1 - z0)) / desired_cell_sizes[2])) or 1))\n    if poisson_restriction:\n        new_ijk = get_good_ijk(new_ijk)\n    if snap_to_origin:\n        x0 = (round((x0 / desired_cell_sizes[0])) * desired_cell_sizes[0])\n        y0 = (round((y0 / desired_cell_sizes[1])) * desired_cell_sizes[1])\n        z0 = (round((z0 / desired_cell_sizes[2])) * desired_cell_sizes[2])\n    x1 = (x0 + (new_ijk[0] * desired_cell_sizes[0]))\n    y1 = (y0 + (new_ijk[1] * desired_cell_sizes[1]))\n    z1 = (z0 + (new_ijk[2] * desired_cell_sizes[2]))\n    ob.bf_mesh_ijk = new_ijk\n    geometry.from_fds.xbs_to_ob(((x0, x1, y0, y1, z0, z1),), context, ob, bf_xb='BBOX', update_center=False)\n    ob_moved = any(map((lambda x, y: ((x - y) > epsilon)), (x0, x1, y0, y1, z0, z1), current_xbs[0]))\n    return ob_moved",
    "nl": "Set exact MESH cell size to Blender object by adapting its dimensions.\nApply Poisson Solver restriction on IJK and snap to global origin of axis, if requested.",
    "original_nl": "Set exact MESH cell size to Blender object by adapting its dimensions.\nApply Poisson Solver restriction on IJK and snap to global origin of axis, if requested."
  },
  {
    "code": "def uses_module(module, verbose=None):\n\n    def wrapped_func(func):\n\n        def real_func(self, *args, **kargs):\n            if (verbose == None):\n                vvv = (ALL_VERBOSE or TEST_VERBOSE)\n            else:\n                vvv = verbose\n            if vvv:\n                print(('Test %s: retrieving current resources' % func.__name__))\n            resources = module._resource_manager.get_current_resources()\n            if vvv:\n                print(('Test %s: retrieved resources: %s' % (func.__name__, resources)))\n                print(('Test %s: running test...' % func.__name__))\n            try:\n                func(self, *args, **kargs)\n            finally:\n                if vvv:\n                    print(('Test %s: Test run...' % func.__name__))\n                    resources_after = module._resource_manager.get_current_resources()\n                    print(('Test %s: Resources after running test: %s' % (func.__name__, resources_after)))\n                module._resource_manager.remove_resources_from(resources)\n                if vvv:\n                    print(('Test %s: resources cleaned' % func.__name__))\n                    new_resources = module._resource_manager.get_current_resources()\n                    print(('Test %s: remaining resources: %s' % (func.__name__, new_resources)))\n        real_func.__name__ = wrapped_func.__name__\n        real_func.__doc___ = wrapped_func.__doc__\n        return real_func\n    return wrapped_func",
    "nl": "Decorator. Sample:\n\n@uses_module(UserProcessingServer)\ndef test_foo(self):\n    # Something related with the UserProcessingServer\n\nuses_module will use the '_resource_manager' variable in that module\n(which is an instance of voodoo.resources_manager), will check how many\nresources are used before calling to the test and how many after\ncalling the test, and it will try to clean the new ones. The main\ndrawback of this code is that it misses the resources created in the\nsetUp method and those disposed in the tearDown method.",
    "original_nl": "Decorator. Sample:\n\n@uses_module(UserProcessingServer)\ndef test_foo(self):\n    # Something related with the UserProcessingServer\n\nuses_module will use the '_resource_manager' variable in that module\n(which is an instance of voodoo.resources_manager), will check how many\nresources are used before calling to the test and how many after\ncalling the test, and it will try to clean the new ones. The main\ndrawback of this code is that it misses the resources created in the\nsetUp method and those disposed in the tearDown method."
  },
  {
    "code": "def test_get_stripped_lines():\n    my_string = '\\n\\n             first line\\n\\n       second line\\n\\n    '\n    assert_equals(strings.get_stripped_lines(my_string), ['first line', 'second line'])",
    "nl": "strings.get_stripped_lines strip every line, and jump empty ones",
    "original_nl": "strings.get_stripped_lines strip every line, and jump empty ones"
  },
  {
    "code": "def test_create(file, km_user_factory):\n    models.Entry.objects.create(attachment=file, km_user=km_user_factory(), text='My sample entry text.')",
    "nl": "Test creating a new journal entry.",
    "original_nl": "Test creating a new journal entry."
  },
  {
    "code": "def _len(self, **kwargs):\n    return len(self.points)",
    "nl": "Subclasses may override this method.",
    "original_nl": "Subclasses may override this method."
  },
  {
    "code": "def startHttpServer(self):\n    self.HTTP_SVR = HTTPServer(('0.0.0.0', 80), GetHandler)\n    self.HTTP_SVR.serve_forever()",
    "nl": "Start the http server",
    "original_nl": "Start the http server"
  },
  {
    "code": "@task\n@cmdopts([('tests', 't', 'Package tests with plugin'), ('lessons', 'n', 'Package lessons with plugin')])\ndef package(options):\n    builddocs(options)\n    package_file = (options.plugin.package_dir / ('%s.zip' % options.plugin.name))\n    with zipfile.ZipFile(package_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n        if (not hasattr(options.package, 'lessons')):\n            options.plugin.excludes.extend(options.plugin.lessons)\n        else:\n            lessonsPath = os.path.abspath('./lessons/_lessonstemp')\n            if os.path.exists(lessonsPath):\n                shutil.rmtree(lessonsPath)\n            print('Downloading lessons...')\n            r = requests.get('https://github.com/boundlessgeo/desktop-lessons/archive/master.zip', stream=True)\n            z = zipfile.ZipFile(io.BytesIO(r.content))\n            z.extractall(path=lessonsPath)\n            dstBase = './lessons/_lessons'\n            for f in os.listdir(os.path.join(lessonsPath, 'desktop-lessons-master')):\n                src = os.path.join(lessonsPath, 'desktop-lessons-master', f)\n                if os.path.isdir(src):\n                    dst = os.path.join(dstBase, f)\n                    if os.path.exists(dst):\n                        shutil.rmtree(dst)\n                    shutil.copytree(src, dst)\n            shutil.rmtree(lessonsPath)\n        if (not hasattr(options.package, 'tests')):\n            options.plugin.excludes.extend(options.plugin.tests)\n        _make_zip(zf, options)\n    return package_file",
    "nl": "Create plugin package",
    "original_nl": "Create plugin package"
  },
  {
    "code": "def post_json(url, body, encoding='utf_8', retry=1, headers=None):\n    return _request_json('POST', url, body=body, encoding=encoding, retry=retry, headers=headers)",
    "nl": "Make a HTTP POST request at `url` and return response parsed as JSON.",
    "original_nl": "Make a HTTP POST request at `url` and return response parsed as JSON."
  },
  {
    "code": "def __init__(self, registry, cr):\n    super(product_template, self).__init__(registry, cr)\n    option = ('lpp', 'Last Purchase Price')\n    type_selection = self._columns['cost_method'].selection\n    if (option not in type_selection):\n        type_selection.append(option)",
    "nl": "Add cost method \"Last Purchase Price\"",
    "original_nl": "Add cost method \"Last Purchase Price\""
  },
  {
    "code": "@property\ndef nb_actions(self):\n    return 2",
    "nl": "Continuous actions corresponding to horizontal/vertical thrust.",
    "original_nl": "Continuous actions corresponding to horizontal/vertical thrust."
  },
  {
    "code": "def test_get_contacts_count_too_big(self):\n    range_min = 12345\n    range_max = 98765\n    bucket = Bucket(range_min, range_max)\n    for i in range(10):\n        contact = PeerNode(('%d' % i), ('192.168.0.%d' % i), 9999, 123)\n        bucket.add_contact(contact)\n    result = bucket.get_contacts(count=20)\n    self.assertEqual(10, len(result))",
    "nl": "If the \"count\" argument is bigger than the number of contacts in the\nbucket then all the contacts are returned.",
    "original_nl": "If the \"count\" argument is bigger than the number of contacts in the\nbucket then all the contacts are returned."
  },
  {
    "code": "@property\ndef known_imarr_flat(self):\n    if np.ma.isMA(self.known_imarr):\n        return self.known_imarr.reshape((- 1), self.nbands)\n    else:\n        mask1b = self.known_depth_arr_flat.mask\n        mask = np.repeat(np.atleast_2d(mask1b).T, self.nbands, 1)\n        return np.ma.masked_where(mask, self.imarr_flat)",
    "nl": "The flattend (pix,bands) image array with all pixels of unknown depth\nmasked.",
    "original_nl": "The flattend (pix,bands) image array with all pixels of unknown depth\nmasked."
  },
  {
    "code": "def upload_symbols(env, symbols, config):\n    if env.is_microsoft_platform:\n        index_file = 'symbols_index.txt'\n        with open(index_file, 'w') as symbols_index:\n            for (src, _) in symbols:\n                symbols_index.write((src + '\\n'))\n        store_root = nimp.system.sanitize_path(env.format(env.publish_symbols))\n        transaction_comment = '{0}_{1}_{2}_{3}'.format(env.project, env.platform, config, env.revision)\n        cmd = ['C:/Program Files (x86)/Windows Kits/8.1/Debuggers/x64/symstore.exe', 'add', '/r', '/f', ('@' + index_file), '/s', store_root, '/o', '/t', env.project, '/c', transaction_comment, '/v', env.revision]\n        if (hasattr(env, 'compress') and env.compress):\n            cmd += ['/compress']\n        if (nimp.sys.process.call(cmd) != 0):\n            return False\n        os.remove(index_file)\n    return True",
    "nl": "Uploads build symbols to a symbol server",
    "original_nl": "Uploads build symbols to a symbol server"
  },
  {
    "code": "@tornado.gen.coroutine\ndef resize_term(self, pid, rows, cols):\n    term = self.consoles[pid]['tty']\n    if (not WINDOWS):\n        term.setwinsize(rows, cols)\n    else:\n        term.set_size(cols, rows)",
    "nl": "Resize terminal.",
    "original_nl": "Resize terminal."
  },
  {
    "code": "def p_boolean_value(self, p):\n    p[0] = p[1]",
    "nl": "boolean_value : true\n              | false",
    "original_nl": "boolean_value : true\n              | false"
  },
  {
    "code": "@blueprint.route('/categories/<uuid:category_id>/update')\n@permission_required(TourneyCategoryPermission.update)\n@templated\ndef category_update_form(category_id, erroneous_form=None):\n    category = _get_category_or_404(category_id)\n    form = (erroneous_form if erroneous_form else TourneyCategoryUpdateForm(obj=category))\n    return {\n        'category': category,\n        'form': form,\n    }",
    "nl": "Show form to update a category.",
    "original_nl": "Show form to update a category."
  },
  {
    "code": "def set_image(self, image):\n    with translate_errors():\n        audio = self.MutagenType(self['~filename'])\n    try:\n        data = image.read()\n    except EnvironmentError as e:\n        raise AudioFileError(e)\n    pic = Picture()\n    pic.data = data\n    pic.type = APICType.COVER_FRONT\n    pic.mime = image.mime_type\n    pic.width = image.width\n    pic.height = image.height\n    pic.depth = image.color_depth\n    audio.pop('coverart', None)\n    audio.pop('coverartmime', None)\n    audio['metadata_block_picture'] = base64.b64encode(pic.write()).decode('ascii')\n    with translate_errors():\n        audio.save()\n    self.has_images = True",
    "nl": "Replaces all embedded images by the passed image",
    "original_nl": "Replaces all embedded images by the passed image"
  },
  {
    "code": "def gather_tree(values, parents):\n    res = tf.py_func(func=gather_tree_py, inp=[values, parents], Tout=values.dtype)\n    res.set_shape(values.get_shape().as_list())\n    return res",
    "nl": "Tensor version of gather_tree_py",
    "original_nl": "Tensor version of gather_tree_py"
  },
  {
    "code": "def MkdirFileLock(*args, **kwds):\n    from . import mkdirlockfile\n    return _fl_helper(mkdirlockfile.MkdirLockFile, 'lockfile.mkdirlockfile', *args, **kwds)",
    "nl": "Factory function provided for backwards compatibility.\n\nDo not use in new code.  Instead, import MkdirLockFile from the\nlockfile.mkdirlockfile module.",
    "original_nl": "Factory function provided for backwards compatibility.\n\nDo not use in new code.  Instead, import MkdirLockFile from the\nlockfile.mkdirlockfile module."
  },
  {
    "code": "def _complete_filenames(self, text, index, hp, is_auto):\n    if (is_auto and (text[(index - 1)] not in '\\\\/')):\n        return\n    str_start = (hp.bracketing[hp.indexbracket][0] + 1)\n    pos = (str_start - 1)\n    str_char = text[pos]\n    assert (str_char in ('\"', \"'\"))\n    if (text[(pos + 1):(pos + 3)] == (str_char + str_char)):\n        return\n    is_raw = ((pos > 0) and (text[(pos - 1)].lower() == 'r'))\n    if is_raw:\n        pos -= 1\n    is_unicode = ((pos > 0) and (text[(pos - 1)].lower() == 'u'))\n    if is_unicode:\n        pos -= 1\n    str_prefix = text[pos:str_start]\n    if (is_auto and (text[(index - 1)] == '\\\\') and (not is_raw) and (not self._is_backslash_char(text, (index - 1)))):\n        return\n    sep_ind = max(text.rfind('/', 0, index), text.rfind('\\\\', 0, index))\n    if ((sep_ind == (- 1)) or (sep_ind < str_start)):\n        comp_prefix_index = str_start\n    elif ((text[sep_ind] == '\\\\') and (not is_raw) and (not self._is_backslash_char(text, sep_ind))):\n        return\n    else:\n        comp_prefix_index = (sep_ind + 1)\n    comp_prefix = text[comp_prefix_index:index]\n    add_quote = (not ((len(text) > index) and (text[index] == str_char)))\n    res = self.complete_filenames(str_prefix, text[str_start:comp_prefix_index], str_char, add_quote)\n    if (res is None):\n        return\n    (public, private, is_case_insen) = res\n    return (comp_prefix, public, private, is_case_insen)",
    "nl": "Return (comp_prefix, public, private, is_case_insen) \n(string, list, list, bool).\nIf shouldn't complete - return None.",
    "original_nl": "Return (comp_prefix, public, private, is_case_insen) \n(string, list, list, bool).\nIf shouldn't complete - return None."
  },
  {
    "code": "@pytest.mark.parametrize('value, expectation', ((None, (- 1)), (1, 1), ('1', 1)))\ndef test__none_to_int(value, expectation):\n    result = helpers._none_to_int(value)\n    assert (result == expectation)",
    "nl": "Make sure ``None`` is converted to an integer representation.\n\nAll other values should be returned as unmodified integers.",
    "original_nl": "Make sure ``None`` is converted to an integer representation.\n\nAll other values should be returned as unmodified integers."
  },
  {
    "code": "def test_pydist():\n    import jsonschema\n\n    def open_json(filename):\n        return json.loads(open(filename, 'rb').read().decode('utf-8'))\n    pymeta_schema = open_json(resource_filename('wheel.test', 'pydist-schema.json'))\n    valid = 0\n    for dist in ('simple.dist', 'complex-dist'):\n        basedir = pkg_resources.resource_filename('wheel.test', dist)\n        for (dirname, subdirs, filenames) in os.walk(basedir):\n            for filename in filenames:\n                if filename.endswith('.whl'):\n                    whl = ZipFile(os.path.join(dirname, filename))\n                    for entry in whl.infolist():\n                        if entry.filename.endswith('/metadata.json'):\n                            pymeta = json.loads(whl.read(entry).decode('utf-8'))\n                            jsonschema.validate(pymeta, pymeta_schema)\n                            valid += 1\n    assert (valid > 0), 'No metadata.json found'",
    "nl": "Make sure pydist.json exists and validates against our schema.",
    "original_nl": "Make sure pydist.json exists and validates against our schema."
  },
  {
    "code": "def evaluate(self, repo, spec, args):\n    status = []\n    if (len(spec['files']) == 0):\n        return status\n    with cd(repo.rootdir):\n        rules = None\n        if (('rules-files' in spec) and (len(spec['rules-files']) > 0)):\n            rulesfiles = spec['rules-files']\n            rules = {\n                \n            }\n            for f in rulesfiles:\n                d = json.loads(open(f).read())\n                rules.update(d)\n        elif ('rules' in spec):\n            rules = {\n                'inline': spec['rules'],\n            }\n        if ((rules is None) or (len(rules) == 0)):\n            print('Regression quality validation has been enabled but no rules file has been specified')\n            print(\"Example: { 'min-r2': 0.25 }. Put this either in file or in dgit.json\")\n            raise InvalidParameters('Regression quality checking rules missing')\n        files = dict([(f, open(f).read()) for f in spec['files']])\n        for r in rules:\n            if ('min-r2' not in rules[r]):\n                continue\n            minr2 = float(rules[r]['min-r2'])\n            for f in files:\n                match = re.search('R-squared:\\\\s+(\\\\d.\\\\d+)', files[f])\n                if (match is None):\n                    status.append({\n                        'target': f,\n                        'validator': self.name,\n                        'description': self.description,\n                        'rules': r,\n                        'status': 'ERROR',\n                        'message': 'Invalid model output',\n                    })\n                else:\n                    r2 = match.group(1)\n                    r2 = float(r2)\n                    if (r2 > minr2):\n                        status.append({\n                            'target': f,\n                            'validator': self.name,\n                            'description': self.description,\n                            'rules': r,\n                            'status': 'OK',\n                            'message': 'Acceptable R2',\n                        })\n                    else:\n                        status.append({\n                            'target': f,\n                            'validator': self.name,\n                            'description': self.description,\n                            'rules': r,\n                            'status': 'ERROR',\n                            'message': 'R2 is too low',\n                        })\n    return status",
    "nl": "Evaluate the files identified for checksum.",
    "original_nl": "Evaluate the files identified for checksum."
  },
  {
    "code": "def _install_signal_handlers(self):\n    logger.debug('Installing signal handlers.')\n    for sig in self.signals:\n        signal_manager.push(sig, self._signal_handler)",
    "nl": "Install signal handlers for the signals we know how to handle gracefully.",
    "original_nl": "Install signal handlers for the signals we know how to handle gracefully."
  },
  {
    "code": "def add_source(self, source):\n    assert isinstance(source, Source)\n    source_info = _SourceInfo(self, source)\n    self.source_info[source] = source_info\n    self.sources.append(source)\n    self.current_source_index = (len(self.sources) - 1)\n    self.application.layout.focus(source_info.window)",
    "nl": "Add a new :class:`.Source` instance.",
    "original_nl": "Add a new :class:`.Source` instance."
  },
  {
    "code": "def spearman(x, y):\n    assert (x.shape == y.shape)\n    n = len(x)\n    rank_x = np.argsort(x)\n    rank_y = np.argsort(y)\n    d = (rank_x - rank_y)\n    d2 = (d ** 2)\n    return (1 - ((6 * np.sum(d2)) / ((n ** 3) - n)))",
    "nl": "Calculates the Spearman rank correlation\ncoefficient for the inputs `x` and `y`.\n\nIf there are any Nan values in the data,\nthey will be ignored in the rank for that\nvariable, and therefore may skew the results.\nSee the test_spearman unit-tests for an\nexample.\n\nThe provided inputs must have no ties.\n\nExample\n-------\n\n>>> import numpy as np\n>>> x = np.array([1, 2, 3, 4])\n>>> y = np.array([2, 3, 4, 3])\n>>> spearman(x, y)  # doctest: +ELLIPSIS\n0.800000000...\n\n",
    "original_nl": "Calculates the Spearman rank correlation\ncoefficient for the inputs `x` and `y`.\n\nIf there are any Nan values in the data,\nthey will be ignored in the rank for that\nvariable, and therefore may skew the results.\nSee the test_spearman unit-tests for an\nexample.\n\nThe provided inputs must have no ties.\n\nExample\n-------\n\n>>> import numpy as np\n>>> x = np.array([1, 2, 3, 4])\n>>> y = np.array([2, 3, 4, 3])\n>>> spearman(x, y)  # doctest: +ELLIPSIS\n0.800000000...\n\n:param x: A `numpy.array` of floats or ints\n:param y: A `numpy.array` of floats or ints\n:return: A float representing the correlation"
  },
  {
    "code": "def test_init(self, tmpdir):\n    with tmpdir.as_cwd():\n        t = Leaf('bark')\n        assert (not t.exists)\n        t.make()\n        assert t.exists\n        t2 = Leaf('bark')\n        assert t2.exists\n        t3 = Tree('mark/').make()\n        assert t3.exists\n        with pytest.raises(ValueError):\n            t4 = Leaf('mark')",
    "nl": "Test that leaf works for:\n    1. nonexistent file\n    2. existing file\n\nTest that exception raised for:\n    1. leaf initialized with existing directory",
    "original_nl": "Test that leaf works for:\n    1. nonexistent file\n    2. existing file\n\nTest that exception raised for:\n    1. leaf initialized with existing directory"
  },
  {
    "code": "def _read(self, n, initial=False, _errnos=(errno.EAGAIN, errno.EINTR)):\n    recv = self._quick_recv\n    rbuf = self._read_buffer\n    try:\n        while (len(rbuf) < n):\n            try:\n                s = recv((n - len(rbuf)))\n            except socket.error as exc:\n                if ((not initial) and (exc.errno in _errnos)):\n                    continue\n                raise\n            if (not s):\n                raise IOError('Socket closed')\n            rbuf += s\n    except:\n        self._read_buffer = rbuf\n        raise\n    (result, self._read_buffer) = (rbuf[:n], rbuf[n:])\n    return result",
    "nl": "Read exactly n bytes from the socket",
    "original_nl": "Read exactly n bytes from the socket"
  },
  {
    "code": "def get_ctrl_state():\n    return bool((_gl.glutGetModifiers() & _gl.GLUT_ACTIVE_CTRL))",
    "nl": "Only available in keyboard, special, and mouse callbacks.",
    "original_nl": "Only available in keyboard, special, and mouse callbacks."
  },
  {
    "code": "def _bmpsize(stream):\n    (x, y) = (0, 0)\n    stream.read(18)\n    (x, y) = unpack('<LL', stream.read(8))\n    if ((x == 0) or (y == 0)):\n        raise ValueError('Unable to determine size of BMP data')\n    return ('BMP', x, y)",
    "nl": "size a Windows-ish BitMap image",
    "original_nl": "size a Windows-ish BitMap image"
  },
  {
    "code": "@ensure_csrf_cookie\n@require_POST\ndef remove_user_from_cohort(request, course_key_string, cohort_id):\n    course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string)\n    get_course_with_access(request.user, 'staff', course_key)\n    username = request.POST.get('username')\n    if (username is None):\n        return json_http_response({\n            'success': False,\n            'msg': 'No username specified',\n        })\n    try:\n        user = User.objects.get(username=username)\n    except User.DoesNotExist:\n        log.debug('no user')\n        return json_http_response({\n            'success': False,\n            'msg': \"No user '{0}'\".format(username),\n        })\n    try:\n        membership = CohortMembership.objects.get(user=user, course_id=course_key)\n        membership.delete()\n    except CohortMembership.DoesNotExist:\n        pass\n    return json_http_response({\n        'success': True,\n    })",
    "nl": "Expects 'username': username in POST data.\n",
    "original_nl": "Expects 'username': username in POST data.\n\nReturn json dict of:\n\n{'success': True} or\n{'success': False,\n 'msg': error_msg}"
  },
  {
    "code": "def test_reuse_input(self):\n    original = b'original'\n    tests = [bytearray(original)]\n    try:\n        m = memoryview(bytearray(original))\n    except NameError:\n        pass\n    else:\n        if (bytes(m) == original):\n            tests.append(m)\n    for data in tests:\n        self.buffer.write(data)\n        data[:] = b'reused!!'\n        self.assertEqual(self.buffer.read(), original)",
    "nl": "Objects should be reusable after write()",
    "original_nl": "Objects should be reusable after write()"
  },
  {
    "code": "def documentLoaded(document):\n    addUrl(document.url())",
    "nl": "Called whenever a document loads.",
    "original_nl": "Called whenever a document loads."
  },
  {
    "code": "def ws_connect(self, url, params=None):\n    proto_req = requests.Request('GET', url, params=params)\n    self.apply_authentication(proto_req)\n    preped_req = proto_req.prepare()\n    header = [('%s: %s' % (k, v)) for (k, v) in preped_req.headers.items() if (k == 'Authorization')]\n    url = preped_req.url\n    if params:\n        joined_params = '&'.join([('%s=%s' % (k, v)) for (k, v) in params.items()])\n        url += ('?%s' % joined_params)\n    return websocket.create_connection(url, header=header)",
    "nl": "Websocket-client based implementation.\n\n",
    "original_nl": "Websocket-client based implementation.\n\n:return: WebSocket connection\n:rtype:  websocket.WebSocket"
  },
  {
    "code": "def send(self, topic, message):\n    self.publisher.send(topic, message)",
    "nl": "Wrapper for publisher send method.\n",
    "original_nl": "Wrapper for publisher send method.\n:param topic: topic of the message to send\n:type topic: str \n:param message: dictionary to be sent.  string also work.\n:type message: dict"
  },
  {
    "code": "def _add(self, uri, methods, handler, host=None):\n    if (host is not None):\n        if isinstance(host, str):\n            uri = (host + uri)\n            self.hosts.add(host)\n        else:\n            if (not isinstance(host, Iterable)):\n                raise ValueError('Expected either string or Iterable of host strings, not {!r}'.format(host))\n            for host_ in host:\n                self.add(uri, methods, handler, host_)\n            return\n    if methods:\n        methods = frozenset(methods)\n    parameters = []\n    properties = {\n        'unhashable': None,\n    }\n\n    def add_parameter(match):\n        name = match.group(1)\n        (name, _type, pattern) = self.parse_parameter_string(name)\n        parameter = Parameter(name=name, cast=_type)\n        parameters.append(parameter)\n        if re.search('(^|[^^]){1}/', pattern):\n            properties['unhashable'] = True\n        elif re.search('/', pattern):\n            properties['unhashable'] = True\n        return '({})'.format(pattern)\n    pattern_string = re.sub(self.parameter_pattern, add_parameter, uri)\n    pattern = re.compile('^{}$'.format(pattern_string))\n\n    def merge_route(route, methods, handler):\n        if ((not route.methods) or (not methods)):\n            raise RouteExists('Route already registered: {}'.format(uri))\n        elif route.methods.intersection(methods):\n            duplicated = methods.intersection(route.methods)\n            raise RouteExists('Route already registered: {} [{}]'.format(uri, ','.join(list(duplicated))))\n        if isinstance(route.handler, self._composition_view_class):\n            view = route.handler\n        else:\n            view = self._composition_view_class()\n            view.add(route.methods, route.handler)\n        view.add(methods, handler)\n        route = route._replace(handler=view, methods=methods.union(route.methods))\n        return route\n    if parameters:\n        if properties['unhashable']:\n            routes_to_check = self.routes_always_check\n            (ndx, route) = self.check_dynamic_route_exists(pattern, routes_to_check)\n        else:\n            routes_to_check = self.routes_dynamic[url_hash(uri)]\n            (ndx, route) = self.check_dynamic_route_exists(pattern, routes_to_check)\n        if (ndx != (- 1)):\n            routes_to_check.pop(ndx)\n    else:\n        route = self.routes_all.get(uri)\n    if route:\n        route = merge_route(route, methods, handler)\n    else:\n        if hasattr(handler, '__blueprintname__'):\n            handler_name = '{}.{}'.format(handler.__blueprintname__, handler.__name__)\n        else:\n            handler_name = getattr(handler, '__name__', None)\n        route = Route(handler=handler, methods=methods, pattern=pattern, parameters=parameters, name=handler_name, uri=uri)\n    self.routes_all[uri] = route\n    if properties['unhashable']:\n        self.routes_always_check.append(route)\n    elif parameters:\n        self.routes_dynamic[url_hash(uri)].append(route)\n    else:\n        self.routes_static[uri] = route",
    "nl": "Add a handler to the route list\n\n",
    "original_nl": "Add a handler to the route list\n\n:param uri: path to match\n:param methods: sequence of accepted method names. If none are\n    provided, any method is allowed\n:param handler: request handler function.\n    When executed, it should provide a response object.\n:return: Nothing"
  },
  {
    "code": "@inlineCallbacks\ndef probe_dhcp(self):\n    client = (yield self._tryGetClient())\n    if (client is None):\n        maaslog.error(\"Can't initiate DHCP probe; no RPC connection to region.\")\n        return\n    interfaces = (yield self._get_interfaces())\n    self.log(('Probe for external DHCP servers started on interfaces: %s.' % ', '.join(interfaces)))\n    for interface in interfaces:\n        try:\n            servers = (yield maybeDeferred(probe_interface, interface))\n        except socket.error as e:\n            error = (\"Failed to probe for external DHCP servers on interface '%s'.\" % interface)\n            if is_dev_environment():\n                error += ' (Did you configure authbind per HACKING.rst?)'\n            self.err(e, error)\n            continue\n        else:\n            if (len(servers) > 0):\n                (yield self._inform_region_of_dhcp(client, interface, servers.pop()))\n            else:\n                (yield self._inform_region_of_dhcp(client, interface, None))\n    self.log('External DHCP probe complete.')",
    "nl": "Find all the interfaces on this rack controller and probe for\nDHCP servers.",
    "original_nl": "Find all the interfaces on this rack controller and probe for\nDHCP servers."
  },
  {
    "code": "def testTFSS(self):\n    pass",
    "nl": "Transfer function system with state space feedback block.",
    "original_nl": "Transfer function system with state space feedback block."
  },
  {
    "code": "def test_align():\n    target = 'TAAATAAATATCTGGTGTTTGAGGCAAAAAGGCAGACTTAAATTCTAAATCACACCTGTGCTTCCAGCACTACCTTCAAGCGCAGGTTCGAGCCAGTCAGGCAGGGTACATAAGAGTCCATTGTGCCTGTATTATTTTGAGCAATGGCTAAAGTACCTTCACCCTTGCTCACTGCTCCCCCACTTCCTCAAGTCTCATCGTGTTTTTTTTAGAGCTAGTTTCTTAGTCTCATTAGGCTTCAGTCACCAT'\n    query = 'TCTGGTGTTTGAGGCAAAAAGGCAGACTTAAATTCTAAATCACACCTGTGCTTCCAGCACTACCTTCAAGCGCAGGTTCGAGCCAGTCAGGACTGCTCCCCCACTTCCTCAAGTCTCATCGTGTTTTTTTTAGAGCTAGTTTCTTAGTCTCATTAGGCTTCAGTCACCATCATTTCTTATAGGAATACCA'\n    assert (kevlar.align(target, query) == ('10D91M69D79M20I', 155))",
    "nl": "Smoke test for ksw2 aligner",
    "original_nl": "Smoke test for ksw2 aligner"
  },
  {
    "code": "def walk(self, state, element):\n    old_state = state\n    (state, recurse) = self.entering_element(state, element)\n    if (state != old_state):\n        self.report_transition('entering', element, old_state, state)\n    if (recurse and hasattr(element, 'children')):\n        for child in element.children:\n            state = self.walk(state, child)\n    old_state = state\n    state = self.leaving_element(state, element)\n    if (state != old_state):\n        self.report_transition('leaving', element, old_state, state)\n    return state",
    "nl": "Walk a parsed element.\n\nKeyword Attributes:\nstate - A string specifying the current state\nelement - The element being walked\n",
    "original_nl": "Walk a parsed element.\n\nKeyword Attributes:\nstate - A string specifying the current state\nelement - The element being walked\n\nReturn is the new state"
  },
  {
    "code": "def p_two_base_bound_2(self, args):\n    return TwoBound(args[0], args[2])",
    "nl": "two_base_bound ::= base_position dot base_position",
    "original_nl": "two_base_bound ::= base_position dot base_position"
  },
  {
    "code": "def message_dialog(title='', text='', ok_text='Ok', style=None, async_=False):\n    dialog = Dialog(title=title, body=Label(text=text, dont_extend_height=True), buttons=[Button(text=ok_text, handler=_return_none)], with_background=True)\n    return _run_dialog(dialog, style, async_=async_)",
    "nl": "Display a simple message box and wait until the user presses enter.",
    "original_nl": "Display a simple message box and wait until the user presses enter."
  },
  {
    "code": "def test_default_hexa_conversion(self):\n    rgba = COLOR_CONVERTER.hexes_to_rgbs(['80', '80', '80', '80'])\n    self.assertEqual(rgba, [128, 128, 128, 128])",
    "nl": "4 valid hexes should convert to rgba.",
    "original_nl": "4 valid hexes should convert to rgba."
  },
  {
    "code": "def __call__(self, item):\n    item = sympify(item)\n    if (item in self):\n        return self[item]\n    else:\n        return 0",
    "nl": "Make instance of a class callable.\n\nIf item belongs to current instance of a class, return it.\n\nOtherwise, return 0.",
    "original_nl": "Make instance of a class callable.\n\nIf item belongs to current instance of a class, return it.\n\nOtherwise, return 0."
  },
  {
    "code": "@property\ndef state_dtype(self):\n    return tf.int64",
    "nl": "The state is a game ID, or 0 if the game is over.",
    "original_nl": "The state is a game ID, or 0 if the game is over."
  },
  {
    "code": "def get_part_nodes(self, part):\n    if (time() > self._rtime):\n        self._reload()\n    return [self.devs[r[part]] for r in self._replica2part2dev_id]",
    "nl": "Get the nodes that are responsible for the partition.\n\n",
    "original_nl": "Get the nodes that are responsible for the partition.\n\n:param part: partition to get nodes for\n:returns: list of node dicts\n\nSee :func:`get_nodes` for a description of the node dicts."
  },
  {
    "code": "def composition(data_list, component):\n    if (component == (data_list.shape[1] - 1)):\n        origincomp = (1.0 - data_list[:, :(- 1)].sum(axis=1))\n        return origincomp\n    else:\n        return data_list[:, component]",
    "nl": "Returns one composition column from the given data, which\nwas presumably made using the standard clobber() function.\n\n:data_list: double numpy array\n:component: int specifying which composition to return (2=origin)\n",
    "original_nl": "Returns one composition column from the given data, which\nwas presumably made using the standard clobber() function.\n\n:data_list: double numpy array\n:component: int specifying which composition to return (2=origin)\n:returns: list of first composition values"
  },
  {
    "code": "def to_wiggle(self, wigglefile=sys.stdout, chromosomes=[]):\n    for chrom in sorted(self):\n        if ((not chromosomes) or (chrom in chromosomes)):\n            print('fixedStep\\tchrom={0}\\tstart=1\\tstep=1'.format(chrom), file=wigglefile)\n            for score in self[chrom]:\n                print(str(score), file=wigglefile)\n    pass",
    "nl": "Output mappability data to file in wiggle format",
    "original_nl": "Output mappability data to file in wiggle format"
  },
  {
    "code": "def get_space_or_404(self, specifiers, user, perm):\n    space = self.get_object_by_specifiers_or_raise(specifiers)\n    if user.has_perm(perm, space):\n        return space\n    else:\n        raise PermissionDenied()",
    "nl": "Fetch a `Space` by its id.  Raise exceptions if no `Space` with\nthis id exists or if the provided user has not the required permission\nto access this `Space`.\n\n",
    "original_nl": "Fetch a `Space` by its id.  Raise exceptions if no `Space` with\nthis id exists or if the provided user has not the required permission\nto access this `Space`.\n\n:param specifiers: The space specifiers.\n:type specifiers: string\n:param user: The user that should be used in the permission check.\n:type user: django.contrib.auth.models.User\n:param perm: The permission to assert that the user has on the node.\n:type perm: unicode\n:raises: django.http.Http404_,\n    :class:`maasserver.exceptions.PermissionDenied`.\n\n.. _django.http.Http404: https://\n   docs.djangoproject.com/en/dev/topics/http/views/\n   #the-http404-exception"
  },
  {
    "code": "def genl(matches, reseed=False):\n    prev = None\n    lblist = []\n    cwb = matches\n    cwbo = matches\n    run = 0\n    while (len(cwb) == len(cwbo)):\n        run += 1\n        if (run == 3):\n            break\n        if (cwb != []):\n            cwbo = cwb\n            cwb = []\n        for i in range(0, int((len(cwbo) / 2))):\n            n = Match('L')\n            cwbo[(i * 2)].llink = n\n            cwbo[((i * 2) + 1)].llink = n\n            lblist.append(n)\n        for r in range(0, len(cwbo)):\n            if (not (cwbo[r].wlink in cwb)):\n                cwb.append(cwbo[r].wlink)\n    clb = lblist\n    rever = False\n    while (len(clb) > 1):\n        nlb = []\n        if (len(cwb) == len(clb)):\n            rever = (not rever)\n            for r in range(0, len(cwb)):\n                m = Match('L')\n                m.loserlinked = True\n                if (rever and reseed):\n                    cwb[((len(cwb) - r) - 1)].llink = m\n                else:\n                    cwb[r].llink = m\n                clb[r].wlink = m\n                nlb.append(m)\n            nwb = []\n            for r in range(0, len(cwb)):\n                if (not (cwb[r].wlink in nwb)):\n                    nwb.append(cwb[r].wlink)\n            cwb = nwb\n        else:\n            for r in range(0, int((len(clb) / 2))):\n                m = Match('L')\n                clb[(r * 2)].wlink = m\n                clb[((r * 2) + 1)].wlink = m\n                nlb.append(m)\n        clb = nlb\n    return lblist",
    "nl": "generates a losers bracket given a winners\nbracket.",
    "original_nl": "generates a losers bracket given a winners\nbracket."
  },
  {
    "code": "def argsparseintlist(txt):\n    txt = txt.split(',')\n    listarg = [int(i) for i in txt]\n    return listarg",
    "nl": "Validate the list of int arguments.\n\n",
    "original_nl": "Validate the list of int arguments.\n\n:param txt: argument of comma separated numbers.\n:return: list of integer converted numbers."
  },
  {
    "code": "def __fit_no_precomputed_kernel(self, X, y):\n    positives = np.where((y == 1.0))[0]\n    hold_out_size = np.ceil((len(positives) * self.hold_out_ratio))\n    if (len(positives) <= hold_out_size):\n        raise (('Not enough positive examples to estimate p(s=1|y=1,x). Need at least ' + str((hold_out_size + 1))) + '.')\n    np.random.shuffle(positives)\n    hold_out = positives[:hold_out_size]\n    X_hold_out = X[hold_out]\n    X = np.delete(X, hold_out, 0)\n    y = np.delete(y, hold_out)\n    self.estimator.fit(X, y)\n    hold_out_predictions = self.estimator.predict_proba(X_hold_out)\n    try:\n        hold_out_predictions = hold_out_predictions[:, 1]\n    except:\n        pass\n    c = np.mean(hold_out_predictions)\n    self.c = c\n    self.estimator_fitted = True",
    "nl": "Fits an estimator of p(s=1|x) and estimates the value of p(s=1|y=1,x)\n\nX -- List of feature vectors\ny -- Labels associated to each feature vector in X (Positive label: 1.0, Negative label: -1.0)",
    "original_nl": "Fits an estimator of p(s=1|x) and estimates the value of p(s=1|y=1,x)\n\nX -- List of feature vectors\ny -- Labels associated to each feature vector in X (Positive label: 1.0, Negative label: -1.0)"
  },
  {
    "code": "def send(self, packet):\n    if ((not packet) or (not isinstance(packet, str))):\n        LOGGER.warning('Missing string! No message sent!')\n        return\n    with self.lock:\n        self.serial.write(packet.encode())",
    "nl": "Write a Message to the gateway.",
    "original_nl": "Write a Message to the gateway."
  },
  {
    "code": "def delete(self, *args, **kwargs):\n    try:\n        if (len(args) != 2):\n            raise ValueError('Invalid url')\n        tenant_id = UUID(args[0])\n        addr = EtherAddress(args[1])\n        tenant = RUNTIME.tenants[tenant_id]\n        tenant_pnfdevs = getattr(tenant, self.server.PNFDEV.ALIAS)\n        pnfdev = tenant_pnfdevs[addr]\n        tenant.remove_pnfdev(pnfdev)\n    except ValueError as ex:\n        self.send_error(400, message=ex)\n    except KeyError as ex:\n        self.send_error(404, message=ex)\n    self.set_status(204, None)",
    "nl": "Remove a pnfdev from a Tenant.\n\nArgs:\n    [0]: the network names of the tenant\n    [1]: the address of the pnfdev\n\nExample URLs:\n\n    GET /api/v1/tenants/52313ecb-9d00-4b7d-b873-b55d3d9ada26/\n      <wtps|cpps|vbses>/11:22:33:44:55:66",
    "original_nl": "Remove a pnfdev from a Tenant.\n\nArgs:\n    [0]: the network names of the tenant\n    [1]: the address of the pnfdev\n\nExample URLs:\n\n    GET /api/v1/tenants/52313ecb-9d00-4b7d-b873-b55d3d9ada26/\n      <wtps|cpps|vbses>/11:22:33:44:55:66"
  },
  {
    "code": "def print_error(self, text):\n    print(('Plugin load failed. Reason: %s' % text))",
    "nl": "! Prints error directly on console\n\n",
    "original_nl": "! Prints error directly on console\n\n@param text Error message text message"
  },
  {
    "code": "def _get_value(self, var):\n    return self._problem._p.getVarByName(self._problem._variables[var]).x",
    "nl": "Return value of variable in solution.",
    "original_nl": "Return value of variable in solution."
  },
  {
    "code": "def siphashx24(message, key=b'', encoder=nacl.encoding.HexEncoder):\n    digest = _sip_hashx(message, key)\n    return encoder.encode(digest)",
    "nl": "Computes a keyed MAC of ``message`` using the 128 bit variant of the\nsiphash-2-4 construction.\n\n",
    "original_nl": "Computes a keyed MAC of ``message`` using the 128 bit variant of the\nsiphash-2-4 construction.\n\n:param message: The message to hash.\n:type message: bytes\n:param key: the message authentication key for the siphash MAC construct\n:type key: bytes(:const:`SIPHASHX_KEYBYTES`)\n:param encoder: A class that is able to encode the hashed message.\n:returns: The hashed message.\n:rtype: bytes(:const:`SIPHASHX_BYTES`)\n\n.. versionadded:: 1.2"
  },
  {
    "code": "def squared_distance_matrix(x, y):\n    bs = x.size(0)\n    assert (bs == y.size(0))\n    n = x.size(1)\n    m = y.size(1)\n    d = x.size(2)\n    assert (d == y.size(2))\n    x = x.unsqueeze(2).expand(bs, n, m, d)\n    y = y.unsqueeze(1).expand(bs, n, m, d)\n    dist = torch.pow((x - y), 2).sum(3)\n    return dist",
    "nl": "Calculate the pairwise squared distances between two batches of matrices x and y.\nInput\n    x: a tensor of shape (bs, n, d)\n    y: a tensor of shape (bs, m, d)\nOutput\n    dist: a tensor of shape (bs, n, m) where dist[i,j,k] = || x[i,j] - y[i,k] || ^ 2",
    "original_nl": "Calculate the pairwise squared distances between two batches of matrices x and y.\nInput\n    x: a tensor of shape (bs, n, d)\n    y: a tensor of shape (bs, m, d)\nOutput\n    dist: a tensor of shape (bs, n, m) where dist[i,j,k] = || x[i,j] - y[i,k] || ^ 2"
  },
  {
    "code": "def __getitem__(self, index):\n    return self.values[index]",
    "nl": "The indexing operator\n\nArgs:\n\n:index: the index to acces",
    "original_nl": "The indexing operator\n\nArgs:\n\n:index: the index to acces"
  },
  {
    "code": "def get_file_contents(self):\n    buff = self.readall()\n    filepath = self.f_open.name\n    lines = buff.splitlines()\n    self.output = []\n    self.current_datainfo = DataInfo()\n    self.current_datainfo.filename = filepath\n    self.reset_data_list(len(lines))\n    has_error_dx = None\n    has_error_dy = None\n    is_data = False\n    candidate_lines = 0\n    candidate_lines_previous = 0\n    line_no = 0\n    lentoks = 2\n    for line in lines:\n        toks = self.splitline(line.strip())\n        new_lentoks = len(toks)\n        try:\n            if (new_lentoks == 0):\n                continue\n            elif ((new_lentoks != lentoks) and is_data):\n                break\n            elif ((new_lentoks != lentoks) and (not is_data)):\n                candidate_lines = 0\n                self.reset_data_list((len(lines) - line_no))\n            self.current_dataset.x[candidate_lines] = float(toks[0])\n            if (new_lentoks > 1):\n                self.current_dataset.y[candidate_lines] = float(toks[1])\n            if (new_lentoks > 2):\n                self.current_dataset.dy[candidate_lines] = float(toks[2])\n                has_error_dy = True\n            if (new_lentoks > 3):\n                self.current_dataset.dx[candidate_lines] = float(toks[3])\n                has_error_dx = True\n            candidate_lines += 1\n            if (candidate_lines >= self.min_data_pts):\n                is_data = True\n            if (is_data and (new_lentoks >= 8)):\n                msg = 'This data looks like 2D ASCII data. Use the file '\n                msg += 'converter tool to convert it to NXcanSAS.'\n                raise FileContentsException(msg)\n            lentoks = new_lentoks\n            line_no += 1\n        except ValueError:\n            if is_data:\n                break\n            self.reset_data_list((len(lines) - line_no))\n            lentoks = 2\n            has_error_dx = None\n            has_error_dy = None\n            candidate_lines = 0\n    if (not is_data):\n        self.set_all_to_none()\n        if (self.extension in self.ext):\n            msg = 'ASCII Reader error: Fewer than five Q data points found '\n            msg += 'in {}.'.format(filepath)\n            raise FileContentsException(msg)\n        else:\n            msg = 'ASCII Reader could not load the file {}'.format(filepath)\n            raise DefaultReaderException(msg)\n    if (has_error_dy and (not (len(self.current_dataset.y) == len(self.current_dataset.dy)))):\n        msg = 'ASCII Reader error: Number of I and dI data points are'\n        msg += ' different in {}.'.format(filepath)\n        self.set_all_to_none()\n        raise FileContentsException(msg)\n    if (has_error_dx and (not (len(self.current_dataset.x) == len(self.current_dataset.dx)))):\n        msg = 'ASCII Reader error: Number of Q and dQ data points are'\n        msg += ' different in {}.'.format(filepath)\n        self.set_all_to_none()\n        raise FileContentsException(msg)\n    self.remove_empty_q_values()\n    self.current_dataset.xaxis('\\\\rm{Q}', 'A^{-1}')\n    self.current_dataset.yaxis('\\\\rm{Intensity}', 'cm^{-1}')\n    self.current_datainfo.meta_data['loader'] = self.type_name\n    self.send_to_output()",
    "nl": "Get the contents of the file",
    "original_nl": "Get the contents of the file"
  },
  {
    "code": "@property\ndef prev_num(self):\n    return (self.page - 1)",
    "nl": "Number of the previous page.",
    "original_nl": "Number of the previous page."
  },
  {
    "code": "def test_dynamically_add_fieldsets_inlines_to_tabs(self):\n    added_fieldset = ('Social', {\n        'fields': ('website', 'twitter', 'facebook'),\n    })\n    added_inline = InterviewInline\n\n    class TestBandAdmin(BandAdmin):\n\n        def get_tabs(self, request, obj=None):\n            tabs = self.tabs\n            tab_overview = (self.tab_overview + (added_fieldset,))\n            tab_ressources = (self.tab_ressources + (added_inline,))\n            tabs = [('Overview', tab_overview), ('Ressources', tab_ressources)]\n            self.tabs = tabs\n            return super(TestBandAdmin, self).get_tabs(request, obj)\n    original_admin = BandAdmin(Band, self.site)\n    self.assertNotIn(added_fieldset, original_admin.get_fieldsets(request))\n    self.assertNotIn(added_inline, original_admin.tab_ressources)\n    admin = TestBandAdmin(Band, self.site)\n    inlines_classes = []\n    inlines = admin.get_inline_instances(request)\n    for inline in inlines:\n        inlines_classes.append(inline.__class__)\n    self.assertIn(added_inline, inlines_classes)",
    "nl": "Tests overriding dynamically tabs via get_tabs.",
    "original_nl": "Tests overriding dynamically tabs via get_tabs."
  },
  {
    "code": "def read_file(name):\n    cur_path = os.path.dirname(__file__)\n    exts = ('txt', 'rst', 'md')\n    for ext in exts:\n        path = os.path.join(cur_path, '.'.join((name, ext)))\n        if os.path.exists(path):\n            with open(path, 'rt') as file_obj:\n                return file_obj.read()\n    return ''",
    "nl": "Read file name (without extension) to string.",
    "original_nl": "Read file name (without extension) to string."
  },
  {
    "code": "def description(self):\n    return (ugettext(\"Here's a few of our favorite add-ons to help you get started customizing %s.\") % self.appname)",
    "nl": "Description for the feed",
    "original_nl": "Description for the feed"
  },
  {
    "code": "def getLocalizedString(self, id):\n    return unicode()",
    "nl": "Returns an addon's localized 'unicode string'.\n\n",
    "original_nl": "Returns an addon's localized 'unicode string'.\n\n:param id: integer - id# for string you want to localize.\n\nExample::\n\n    locstr = self.Addon.getLocalizedString(id=6)"
  },
  {
    "code": "@XBlock.handler\ndef transcript(self, request, dispatch):\n    is_bumper = request.GET.get('is_bumper', False)\n    transcripts = self.get_transcripts_info(is_bumper)\n    if dispatch.startswith('translation'):\n        language = dispatch.replace('translation', '').strip('/')\n        if (not language):\n            log.info('Invalid /translation request: no language.')\n            return Response(status=400)\n        if (language not in (['en'] + transcripts['transcripts'].keys())):\n            log.info('Video: transcript facilities are not available for given language.')\n            return Response(status=404)\n        if (language != self.transcript_language):\n            self.transcript_language = language\n        try:\n            transcript = self.translation(request.GET.get('videoId', None), transcripts)\n        except (TypeError, NotFoundError) as ex:\n            log.debug(ex.message)\n            return self.get_static_transcript(request, transcripts)\n        except (TranscriptException, UnicodeDecodeError, TranscriptsGenerationException) as ex:\n            log.info(ex.message)\n            response = Response(status=404)\n        else:\n            response = Response(transcript, headerlist=[('Content-Language', language)])\n            response.content_type = Transcript.mime_types['sjson']\n    elif (dispatch == 'download'):\n        lang = request.GET.get('lang', None)\n        try:\n            (transcript_content, transcript_filename, transcript_mime_type) = self.get_transcript(transcripts, transcript_format=self.transcript_download_format, lang=lang)\n        except (NotFoundError, ValueError, KeyError, UnicodeDecodeError):\n            log.debug('Video@download exception')\n            return Response(status=404)\n        else:\n            response = Response(transcript_content, headerlist=[('Content-Disposition', 'attachment; filename=\"{}\"'.format(transcript_filename.encode('utf8'))), ('Content-Language', self.transcript_language)], charset='utf8')\n            response.content_type = transcript_mime_type\n    elif dispatch.startswith('available_translations'):\n        available_translations = self.available_translations(transcripts)\n        if available_translations:\n            response = Response(json.dumps(available_translations))\n            response.content_type = 'application/json'\n        else:\n            response = Response(status=404)\n    else:\n        log.debug('Dispatch is not allowed')\n        response = Response(status=404)\n    return response",
    "nl": "Entry point for transcript handlers for student_view.\n\nRequest GET contains:\n    (optional) `videoId` for `translation` dispatch.\n    `is_bumper=1` flag for bumper case.\n\nDispatches, (HTTP GET):\n    /translation/[language_id]\n    /download\n    /available_translations/\n\nExplanations:\n    `download`: returns SRT or TXT file.\n    `translation`: depends on HTTP methods:\n            Provide translation for requested language, SJSON format is sent back on success,\n            Proper language_id should be in url.\n    `available_translations`:\n            Returns list of languages, for which transcript files exist.\n            For 'en' check if SJSON exists. For non-`en` check if SRT file exists.",
    "original_nl": "Entry point for transcript handlers for student_view.\n\nRequest GET contains:\n    (optional) `videoId` for `translation` dispatch.\n    `is_bumper=1` flag for bumper case.\n\nDispatches, (HTTP GET):\n    /translation/[language_id]\n    /download\n    /available_translations/\n\nExplanations:\n    `download`: returns SRT or TXT file.\n    `translation`: depends on HTTP methods:\n            Provide translation for requested language, SJSON format is sent back on success,\n            Proper language_id should be in url.\n    `available_translations`:\n            Returns list of languages, for which transcript files exist.\n            For 'en' check if SJSON exists. For non-`en` check if SRT file exists."
  },
  {
    "code": "def infix_to_postfix(nodes, *, recurse_types=None):\n    output = []\n    operators = []\n    for node in nodes:\n        if isinstance(node, OperatorNode):\n            cmp_operator = node.operator\n            while operators:\n                current_operator = operators[(- 1)].operator\n                if ((current_operator.precedence > cmp_operator.precedence) or ((current_operator.precedence == cmp_operator.precedence) and (current_operator.association == Association.left))):\n                    output.append(operators.pop())\n                else:\n                    break\n            operators.append(node)\n        elif ((recurse_types is not None) and (node.node_type in recurse_types)):\n            output.extend(infix_to_postfix(node.children, recurse_types=recurse_types))\n        else:\n            output.append(node)\n    return (output + list(reversed(operators)))",
    "nl": "Convert a list of nodes in infix order to a list of nodes in postfix order.\n\nE.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +",
    "original_nl": "Convert a list of nodes in infix order to a list of nodes in postfix order.\n\nE.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +"
  },
  {
    "code": "@pytest.mark.slow\n@pytest.mark.main\ndef test_entry_point_example_rural_postman_problem_sleeping_giant(script_runner):\n    ret = script_runner.run('rural_postman_sleeping_giant')\n    assert ret.success\n    assert os.path.isfile(OUT_SLEEPING_GIANT_SVG)",
    "nl": "Just testing that Sleeping Giant example runs with pre-parameterized config.\nWill overwrite output in the examples dir... that's OK.",
    "original_nl": "Just testing that Sleeping Giant example runs with pre-parameterized config.\nWill overwrite output in the examples dir... that's OK."
  },
  {
    "code": "def reproducible_permutation(x):\n    return _rs().permutation(x)",
    "nl": "Like NumPy's `random.permutation`, but always returns the same thing\nfor a given argument.",
    "original_nl": "Like NumPy's `random.permutation`, but always returns the same thing\nfor a given argument."
  },
  {
    "code": "def testIncorrectWidths(self):\n    config = copy.deepcopy(networkConfig)\n    config['numCorticalColumns'] = 2\n    config['L4Params']['columnCount'] = 42\n    with self.assertRaises(AssertionError):\n        createNetwork(config)\n    config = copy.deepcopy(networkConfig)\n    config['numCorticalColumns'] = 2\n    config['L6Params']['columnCount'] = 42\n    with self.assertRaises(AssertionError):\n        createNetwork(config)",
    "nl": "We create a network with sensor and coarse sensor widths that don't match\ncolumn counts. We expect assertion errors in this case",
    "original_nl": "We create a network with sensor and coarse sensor widths that don't match\ncolumn counts. We expect assertion errors in this case"
  },
  {
    "code": "def ensure_string_list_with_comma_words(self, option):\n    value = getattr(self, option, None)\n    if (not value):\n        return\n    parts = shlex.split(value)\n    setattr(self, option, parts)",
    "nl": "Ensures that a string with whitespace separated words\nis converted into list of strings.\nNote that commas are allowed in words\n(compared :meth:`ensure_string_list()`).",
    "original_nl": "Ensures that a string with whitespace separated words\nis converted into list of strings.\nNote that commas are allowed in words\n(compared :meth:`ensure_string_list()`)."
  },
  {
    "code": "def pr2_spatial(tslsreg):\n    y = tslsreg.y\n    predy_e = tslsreg.predy_e\n    pr = pearsonr(y, predy_e)[0]\n    pr2_result = float((pr ** 2))\n    return pr2_result",
    "nl": "Calculates the pseudo r^2 for the spatial two stage least squares \nregression.\n",
    "original_nl": "Calculates the pseudo r^2 for the spatial two stage least squares \nregression.\n\nParameters\n----------\nstslsreg            : spatial two stage least squares regression object\n                      output instance from a spatial two stage least \n                      squares regression model\n\nReturns\n-------    \npr2_result          : float\n                      value of the squared pearson correlation between\n                      the y and stsls-predicted y vectors\n\nExamples\n--------\n\nWe first need to import the needed modules. Numpy is needed to convert the\ndata we read into arrays that ``spreg`` understands and ``pysal`` to\nperform all the analysis. The GM_Lag is required to run the model on\nwhich we will perform the tests and the ``pysal.spreg.diagnostics`` module\ncontains the function with the test.\n\n>>> import numpy as np\n>>> import pysal\n>>> import pysal.spreg.diagnostics as D\n>>> from twosls_sp import GM_Lag\n\nOpen data on Columbus neighborhood crime (49 areas) using pysal.open().\nThis is the DBF associated with the Columbus shapefile.  Note that\npysal.open() also reads data in CSV format; since the actual class\nrequires data to be passed in as numpy arrays, the user can read their\ndata in using any method.  \n\n>>> db = pysal.open(pysal.examples.get_path(\"columbus.dbf\"),'r')\n\nExtract the HOVAL column (home value) from the DBF file and make it the\ndependent variable for the regression. Note that PySAL requires this to be\nan numpy array of shape (n, 1) as opposed to the also common shape of (n, )\nthat other packages accept.\n\n>>> y = np.array(db.by_col(\"HOVAL\"))\n>>> y = np.reshape(y, (49,1))\n\nExtract INC (income) vectors from the DBF to be used as\nindependent variables in the regression.  Note that PySAL requires this to\nbe an nxj numpy array, where j is the number of independent variables (not\nincluding a constant). By default this model adds a vector of ones to the\nindependent variables passed in, but this can be overridden by passing\nconstant=False.\n\n>>> X = np.array(db.by_col(\"INC\"))\n>>> X = np.reshape(X, (49,1))\n\nIn this case, we consider CRIME (crime rates) as an endogenous regressor,\nso we acknowledge that by reading it in a different category.\n\n>>> yd = np.array(db.by_col(\"CRIME\"))\n>>> yd = np.reshape(yd, (49,1))\n\nIn order to properly account for the endogeneity, we have to pass in the\ninstruments. Let us consider DISCBD (distance to the CBD) is a good one:\n\n>>> q = np.array(db.by_col(\"DISCBD\"))\n>>> q = np.reshape(q, (49,1))\n\nSince this test has a spatial component, we need to specify the spatial\nweights matrix that includes the spatial configuration of the observations\ninto the error component of the model. To do that, we can open an already\nexisting gal file or create a new one. In this case, we will create one\nfrom ``columbus.shp``.\n\n>>> w = pysal.rook_from_shapefile(pysal.examples.get_path(\"columbus.shp\")) \n\nUnless there is a good reason not to do it, the weights have to be\nrow-standardized so every row of the matrix sums to one. Among other\nthings, this allows to interpret the spatial lag of a variable as the\naverage value of the neighboring observations. In PySAL, this can be\neasily performed in the following way:\n\n>>> w.transform = 'r'\n\nNow we are good to run the spatial lag model. Make sure you pass all the\nparameters correctly and, if desired, pass the names of the variables as\nwell so when you print the summary (reg.summary) they are included:\n\n>>> reg = GM_Lag(y, X, w=w, yend=yd, q=q, w_lags=2, name_x=['inc'], name_y='hoval', name_yend=['crime'], name_q=['discbd'], name_ds='columbus')\n\nOnce we have a regression object, we can perform the spatial version of\nthe pesudo R^2. It is as simple as one line!\n\n>>> result = pr2_spatial(reg)\n>>> print(\"%1.6f\"%result)\n0.299649"
  },
  {
    "code": "def unregister_target(self, fs, target):\n    raise NotImplementedError(NIEXC)",
    "nl": "Set the specified `target', used by `fs', as available in the backend.\n\nThis target could be now reuse, for other targets of the same\nfilesystem or any other one.",
    "original_nl": "Set the specified `target', used by `fs', as available in the backend.\n\nThis target could be now reuse, for other targets of the same\nfilesystem or any other one."
  },
  {
    "code": "def resolve_tracks(self, track_ids):\n    pool = ThreadPool(processes=16)\n    tracks = pool.map(self.get_track, track_ids)\n    pool.close()\n    return self.sanitize_tracks(tracks)",
    "nl": "Resolve tracks concurrently emulating browser\n\n",
    "original_nl": "Resolve tracks concurrently emulating browser\n\n:param track_ids:list of track ids\n:return:list `Track`"
  },
  {
    "code": "def checkdebug(self):\n    opt = self.file_grep('^(DEFAULT_LOGGING|DAEMONOPTIONS)=(.*)', *self.files)\n    for opt1 in opt:\n        for opt2 in opt1.split(' '):\n            if (opt2 in ('--debug', 'debug')):\n                return True\n    return False",
    "nl": "testing if autofs debug has been enabled anywhere",
    "original_nl": "testing if autofs debug has been enabled anywhere"
  },
  {
    "code": "def doImport(self):\n    try:\n        csvr = readcsv.ReadCSV(self.params)\n    except re.error:\n        raise base.ImportingError(_('Invalid date regular expression'))\n    csvr.readData()\n    LF = None\n    if self.params.linked:\n        LF = LinkedFileCSV(self.params)\n    csvr.setData(self.outdatasets, linkedfile=LF)",
    "nl": "Do the data import.",
    "original_nl": "Do the data import."
  },
  {
    "code": "def create_maps(parsed_obs, offering, secondsGranularity, event_time):\n    timestampPattern = '%Y-%m-%dT%H:%M:%S'\n    startTime = event_time.split('+')[0]\n    epochS = int(time.mktime(time.strptime(startTime, timestampPattern)))\n    endTime = event_time.split('+')[1].split('/')[1]\n    epochE = int(time.mktime(time.strptime(endTime, timestampPattern)))\n    for (key, observation) in parsed_obs.iteritems():\n        run_command('g.message', message='Creating vector maps for {}...'.format(key))\n        mapName = '{}_{}_{}'.format(options['output'], offering, key)\n        if (':' in mapName):\n            mapName = '_'.join(mapName.split(':'))\n        if ('-' in mapName):\n            mapName = '_'.join(mapName.split('-'))\n        if ('.' in mapName):\n            mapName = '_'.join(mapName.split('.'))\n        run_command('t.create', output=mapName, type='stvds', title='Dataset for offering {} and observed property {}'.format(offering, key), description='Vector space time dataset')\n        freeCat = 1\n        points = dict()\n        new = VectorTopo(mapName)\n        if (overwrite() is True):\n            try:\n                new.remove()\n            except:\n                pass\n        data = json.loads(observation)\n        cols = [('cat', 'INTEGER PRIMARY KEY'), ('name', 'VARCHAR'), ('value', 'DOUBLE')]\n        intervals = {\n            \n        }\n        for secondsStamp in range(epochS, (epochE + 1), secondsGranularity):\n            intervals.update({\n                secondsStamp: dict(),\n            })\n        timestampPattern = 't%Y%m%dT%H%M%S'\n        for a in data['features']:\n            name = a['properties']['name']\n            if (a['properties']['name'] not in points.keys()):\n                if (new.is_open() is False):\n                    new.open('w')\n                points.update({\n                    a['properties']['name']: freeCat,\n                })\n                new.write(Point(*a['geometry']['coordinates']))\n                freeCat += 1\n            for (timestamp, value) in a['properties'].iteritems():\n                if (timestamp != 'name'):\n                    observationStartTime = timestamp[:(- 4)]\n                    secondsTimestamp = int(time.mktime(time.strptime(observationStartTime, timestampPattern)))\n                    for interval in intervals.keys():\n                        if ((secondsTimestamp >= interval) and (secondsTimestamp < (interval + secondsGranularity))):\n                            if (name in intervals[interval].keys()):\n                                intervals[interval][name].append(float(value))\n                            else:\n                                intervals[interval].update({\n                                    name: [float(value)],\n                                })\n                            break\n        if new.is_open():\n            new.close(build=False)\n            run_command('v.build', map=mapName, quiet=True)\n        i = 1\n        layersTimestamps = list()\n        for interval in intervals.keys():\n            if (len(intervals[interval]) != 0):\n                timestamp = datetime.datetime.fromtimestamp(interval).strftime('t%Y%m%dT%H%M%S')\n                tableName = '{}_{}_{}_{}'.format(options['output'], offering, key, timestamp)\n                if (':' in tableName):\n                    tableName = '_'.join(tableName.split(':'))\n                if ('-' in tableName):\n                    tableName = '_'.join(tableName.split('-'))\n                if ('.' in tableName):\n                    tableName = '_'.join(tableName.split('.'))\n                new.open('rw')\n                db = '$GISDBASE/$LOCATION_NAME/$MAPSET/sqlite/sqlite.db'\n                link = Link(layer=i, name=tableName, table=tableName, key='cat', database=db, driver='sqlite')\n                new.dblinks.add(link)\n                new.table = new.dblinks[(i - 1)].table()\n                new.table.create(cols)\n                i += 1\n                layersTimestamps.append(timestamp)\n                for (name, values) in intervals[interval].iteritems():\n                    if (options['method'] == 'average'):\n                        aggregatedValue = (sum(values) / len(values))\n                    elif (options['method'] == 'sum'):\n                        aggregatedValue = sum(values)\n                    new.table.insert(tuple([points[name], name, aggregatedValue]))\n                    new.table.conn.commit()\n                new.close(build=False)\n                run_command('v.build', map=mapName, quiet=True)\n        create_temporal(mapName, i, layersTimestamps)",
    "nl": "Create vector map representing offerings and observed properties\n",
    "original_nl": "Create vector map representing offerings and observed properties\n:param parsed_obs: Observations for a given offering in geoJSON format\n:param offering: A collection of sensors used to conveniently group them up\n:param secondsGranularity: Granularity in seconds"
  },
  {
    "code": "def get_params_string(self):\n    output = ''\n    for key in self.params:\n        output += (((str(key) + ':') + str(self.params[key])) + '-')\n    return output",
    "nl": "Returns a single string describing all of our param values.\n\n",
    "original_nl": "Returns a single string describing all of our param values.\n\n:return:"
  },
  {
    "code": "def __init__(self):\n    pass",
    "nl": "Create an instance of the MPR121 device.",
    "original_nl": "Create an instance of the MPR121 device."
  },
  {
    "code": "def package(self, pkg):\n    return self.swdb.package(str(pkg))",
    "nl": "Get SwdbPackage from package",
    "original_nl": "Get SwdbPackage from package"
  },
  {
    "code": "@property\ndef return_code(self):\n    return self._return_code",
    "nl": "Return code returned by the executed binary",
    "original_nl": "Return code returned by the executed binary"
  },
  {
    "code": "def WorkerClusterSpecString(num_workers, num_param_servers, port):\n    return ClusterSpecString(num_workers, num_param_servers, port)",
    "nl": "Generates worker cluster spec.",
    "original_nl": "Generates worker cluster spec."
  },
  {
    "code": "def test_503_Kruskal_matched_single_argument(self):\n    np.random.seed(987654321)\n    x_parms = [1.7]\n    x_input_array = st.weibull_min.rvs(*x_parms, size=100)\n    a = 0.05\n    self.assertRaises(NoDataError, (lambda : Kruskal(x_input_array, alpha=a, display=False).p_value))",
    "nl": "Test the Kruskal Wallis class on matched data",
    "original_nl": "Test the Kruskal Wallis class on matched data"
  },
  {
    "code": "@property\ndef maxiter(self):\n    return self._maxiter",
    "nl": "Maximum number of iterations",
    "original_nl": "Maximum number of iterations"
  },
  {
    "code": "def addGcodeFromLoop(self, loop, z):\n    euclidean.addSurroundingLoopBeginning(self, loop, z)\n    self.addPerimeterBlock(loop, z)\n    self.addLine('(</boundaryPerimeter>)')\n    self.addLine('(</surroundingLoop>)')",
    "nl": "Add the gcode loop.",
    "original_nl": "Add the gcode loop."
  },
  {
    "code": "@property\n@abstractmethod\ndef table(self):\n    pass",
    "nl": "Source name.",
    "original_nl": "Source name."
  },
  {
    "code": "def iteritems(self):\n    for key in self:\n        value = self[key]\n        (yield (key, value))",
    "nl": "Return an iterator over the items (``(key, value)`` pairs).\n\nIterating the Mapping while adding or deleting keys may raise a\n`RuntimeError` or fail to iterate over all entries.",
    "original_nl": "Return an iterator over the items (``(key, value)`` pairs).\n\nIterating the Mapping while adding or deleting keys may raise a\n`RuntimeError` or fail to iterate over all entries."
  },
  {
    "code": "def _genargs(self, opts):\n    for (optkey, optval) in self._normalize_options(opts):\n        (yield optkey)\n        if isinstance(optval, (list, tuple)):\n            assert ((len(optval) == 2) and optval[0] and optval[1]), 'Option value can only be either a string or a (tuple, list) of 2 items'\n            (yield optval[0])\n            (yield optval[1])\n        else:\n            (yield optval)",
    "nl": "Generator of args parts based on options specification.\n\nNote: Empty parts will be filtered out at _command generator",
    "original_nl": "Generator of args parts based on options specification.\n\nNote: Empty parts will be filtered out at _command generator"
  },
  {
    "code": "def generateClassificationData(size, nClasses=3):\n    if (nClasses == 3):\n        means = [((- 1), 0), (2, 4), (3, 1)]\n    else:\n        means = [((- 2), 0), (2, 1), (6, 0)]\n    cov = [diag([1, 1]), diag([0.5, 1.2]), diag([1.5, 0.7])]\n    dataset = ClassificationDataSet(2, 1, nb_classes=nClasses)\n    for _ in xrange(size):\n        for c in range(3):\n            input = multivariate_normal(means[c], cov[c])\n            dataset.addSample(input, [(c % nClasses)])\n    dataset.assignClasses()\n    return dataset",
    "nl": "generate a set of points in 2D belonging to two or three different classes",
    "original_nl": "generate a set of points in 2D belonging to two or three different classes"
  },
  {
    "code": "def set_sync_info(self, name, mtime, size):\n    if (not self.is_local()):\n        return self.peer.set_sync_info(name, mtime, size)\n    return self.cur_dir_meta.set_sync_info(name, mtime, size)",
    "nl": "Store mtime/size when this resource was last synchronized with remote.",
    "original_nl": "Store mtime/size when this resource was last synchronized with remote."
  },
  {
    "code": "def at_center(r):\n    if pd.isnull(r.get('ORBIT_LENGTH')):\n        return np.nan\n    if (not pd.isnull(r.get('AT_ENTRY'))):\n        return (r['AT_ENTRY'] + (r['ORBIT_LENGTH'] / 2.0))\n    elif (not pd.isnull(r.get('AT_EXIT'))):\n        return (r['AT_EXIT'] - (r['ORBIT_LENGTH'] / 2.0))\n    else:\n        return np.nan",
    "nl": "Try to compute the element's center 's' position from other data.",
    "original_nl": "Try to compute the element's center 's' position from other data."
  },
  {
    "code": "def getBackTrace():\n    backtrace = cStringIO.StringIO()\n    traceback.print_exc(None, backtrace)\n    ret = backtrace.getvalue()\n    backtrace.close()\n    return ret",
    "nl": "Returns the current backtrace.\n\n",
    "original_nl": "Returns the current backtrace.\n\n@return:\n@rtype:"
  },
  {
    "code": "def main(bratOutput, dams, outputName):\n    if outputName.endswith('.shp'):\n        out_network = os.path.join(os.path.dirname(bratOutput), outputName)\n    else:\n        out_network = os.path.join(os.path.dirname(bratOutput), (outputName + '.shp'))\n    arcpy.Delete_management(out_network)\n    damFields = ['e_DamCt', 'e_DamDens', 'e_DamPcC']\n    otherFields = ['Ex_Categor', 'Pt_Categor', 'mCC_EX_Ct', 'mCC_PT_Ct', 'mCC_EXtoPT']\n    newFields = (damFields + otherFields)\n    inputFields = ['SHAPE@LENGTH', 'oCC_EX', 'oCC_PT']\n    if dams:\n        arcpy.AddMessage('Adding fields that need dam input...')\n        setDamAttributes(bratOutput, out_network, dams, ((damFields + ['Join_Count']) + inputFields), newFields)\n    else:\n        arcpy.CopyFeatures_management(bratOutput, out_network)\n        addFields(out_network, otherFields)\n    arcpy.AddMessage(\"Adding fields that don't need dam input...\")\n    setOtherAttributes(out_network, (otherFields + inputFields))",
    "nl": "The main function\n",
    "original_nl": "The main function\n:param bratOutput: The output of BRAT (a polyline shapefile)\n:param dams: A shapefile containing a point for each dam\n:param outputName: The name of the output shape file\n:return:"
  },
  {
    "code": "@line_magic\ndef cddotfiles(self, line):\n    return self.cd('__DOTFILES', line)",
    "nl": "cddotfiles    -- cd $__DOTFILES/${@}",
    "original_nl": "cddotfiles    -- cd $__DOTFILES/${@}"
  },
  {
    "code": "def GetVersion():\n    return _dll.FreeImage_GetVersion()",
    "nl": "Return a string containing the current version of the library",
    "original_nl": "Return a string containing the current version of the library"
  },
  {
    "code": "def _SetUpKnowledgeBase(self, knowledge_base_values=None):\n    knowledge_base_object = knowledge_base.KnowledgeBase()\n    if knowledge_base_values:\n        for (identifier, value) in iter(knowledge_base_values.items()):\n            if (identifier == 'users'):\n                self._SetUserAccounts(knowledge_base_object, value)\n            else:\n                knowledge_base_object.SetValue(identifier, value)\n    return knowledge_base_object",
    "nl": "Sets up a knowledge base.\n\nArgs:\n  knowledge_base_values (Optional[dict[str, str]]): knowledge base values.\n",
    "original_nl": "Sets up a knowledge base.\n\nArgs:\n  knowledge_base_values (Optional[dict[str, str]]): knowledge base values.\n\nReturns:\n  KnowledgeBase: knowledge base."
  },
  {
    "code": "def host_passes(self, host_state, filter_properties):\n    num_io_ops = host_state.num_io_ops\n    max_io_ops = self._get_max_io_ops_per_host(host_state, filter_properties)\n    passes = (num_io_ops < max_io_ops)\n    if (not passes):\n        LOG.debug('%(host_state)s fails I/O ops check: Max IOs per host is set to %(max_io_ops)s', {\n            'host_state': host_state,\n            'max_io_ops': max_io_ops,\n        })\n    return passes",
    "nl": "Use information about current vm and task states collected from\ncompute node statistics to decide whether to filter.",
    "original_nl": "Use information about current vm and task states collected from\ncompute node statistics to decide whether to filter."
  },
  {
    "code": "def entry(self):\n    api = Api()\n    api.authenticate()\n    return api.fetch_video(self.video_id)",
    "nl": "Connects to Youtube Api and retrieves the video entry object\n",
    "original_nl": "Connects to Youtube Api and retrieves the video entry object\n\nReturn:\n    gdata.youtube.YouTubeVideoEntry"
  },
  {
    "code": "@classmethod\ndef make_request(cls, memory_location, dfi=None):\n    from udsoncan import Request, MemoryLocation\n    dfi = cls.normalize_data_format_identifier(dfi)\n    if (not isinstance(memory_location, MemoryLocation)):\n        raise ValueError('memory_location must be an instance of MemoryLocation')\n    request = Request(service=cls)\n    request.data = b''\n    request.data += dfi.get_byte()\n    request.data += memory_location.alfid.get_byte()\n    request.data += memory_location.get_address_bytes()\n    request.data += memory_location.get_memorysize_bytes()\n    return request",
    "nl": "Generate a request for RequestUpload\n\n",
    "original_nl": "Generate a request for RequestUpload\n\n:param memory_location: The address and the size of the memory block to be read.\n:type memory_location: :ref:`MemoryLocation <MemoryLocation>`\n\n:param dfi: Optional dataFormatIdentifier defining the compression and encryption scheme of the data. \n        If not specified, the default value of 00 will be used, specifying no encryption and no compression\n:type dfi: :ref:`DataFormatIdentifier <DataFormatIdentifier>`           \n\n:raises ValueError: If parameters are out of range, missing or wrong type"
  },
  {
    "code": "def __str__(self):\n    node_str = self.hostname\n    node_str += '\\n\\t{0} = {1}'.format('state', self.state)\n    node_str += '\\n\\t{0} = {1}'.format('np', self.np)\n    properties_str = ','.join(self.properties)\n    node_str += '\\n\\t{0} = {1}'.format('properties', properties_str)\n    node_str += '\\n\\t{0} = {1}'.format('ntype', self.ntype)\n    if self.jobs:\n        jobs_str = ','.join(['{0}={1}'.format(k, v) for (k, v) in self.jobs.items()])\n        node_str += '\\n\\t{0} = {1}'.format('jobs', jobs_str)\n    if self.status:\n        status_str = ','.join(['{0}={1}'.format(k, v) for (k, v) in self.status.items()])\n        node_str += '\\n\\t{0} = {1}'.format('status', status_str)\n    if self.note:\n        node_str += '\\n\\t{0} = {1}'.format('note', self.note)\n    return node_str",
    "nl": "returns string representation for node status",
    "original_nl": "returns string representation for node status"
  },
  {
    "code": "def _(text, disambiguation=None, context='Graph'):\n    return qt4.QCoreApplication.translate(context, text, disambiguation)",
    "nl": "Translate text.",
    "original_nl": "Translate text."
  },
  {
    "code": "def _get_recorder_mic_outputs(recorder, time):\n    connections = DeviceConnection.objects.filter(input__device=recorder, output__device__model__type='Microphone', start_time__lte=time, end_time__gt=time)\n    return dict(((c.input.channel_num, c.output) for c in connections))",
    "nl": "Gets a mapping from recorder input channel numbers to connected\nmicrophone outputs for the specified recorder and time.",
    "original_nl": "Gets a mapping from recorder input channel numbers to connected\nmicrophone outputs for the specified recorder and time."
  },
  {
    "code": "def test__init__(self):\n    session = requests.Session()\n    session.headers.update({\n        'Accept': ','.join(['application/yang.data+json', 'application/yang.errors+json']),\n        'Content-Type': 'application/yang.data+json',\n    })\n    self.assertEqual(self.classObject._session.headers, session.headers)\n    self.assertEqual(self.classObject._host, sm_url)",
    "nl": "Does constructor create a proper object",
    "original_nl": "Does constructor create a proper object"
  },
  {
    "code": "@classmethod\ndef get_translation_table(cls):\n    return cls.__translation_table",
    "nl": "Class method to access the translation table.",
    "original_nl": "Class method to access the translation table."
  },
  {
    "code": "def _transform_op(self, op):\n    if (op in self._info.transformed_ops):\n        return self._info.transformed_ops[op]\n    op_ = self.transform_op_handler(self._info, op)\n    self._info.graph_._record_op_seen_by_control_dependencies(op_)\n    for device_function in reversed(self._info.graph_._device_function_stack):\n        op_._set_device(device_function(op_))\n    if (op is not op_):\n        self.assign_collections_handler(self._info, op, op_)\n    self._info.transformed_ops[op] = op_\n    return op_",
    "nl": "Transform a tf.Operation.\n\nArgs:\n  op: the operation to be transformed.",
    "original_nl": "Transform a tf.Operation.\n\nArgs:\n  op: the operation to be transformed.\nReturns:\n  The transformed operation."
  },
  {
    "code": "def test_magnitude_and_normalize():\n    vector1 = Vector([(- 0.221), 7.437])\n    answer1 = Decimal('7.440')\n    assert (round(vector1.magnitude(), 3) == answer1)\n    vector2 = Vector([8.813, (- 1.331), (- 6.247)])\n    answer2 = Decimal('10.884')\n    assert (round(vector2.magnitude(), 3) == answer2)\n    vector3 = Vector([5.581, (- 2.136)])\n    answer3 = Vector([0.934, (- 0.357)]).round_coords(3)\n    assert (vector3.normalize().round_coords(3) == answer3)\n    vector4 = Vector([1.996, 3.108, (- 4.554)])\n    answer4 = Vector([0.34, 0.53, (- 0.777)]).round_coords(3)\n    assert (vector4.normalize().round_coords(3) == answer4)",
    "nl": "Quiz 2 calculating magnitude and normalization of Vectors",
    "original_nl": "Quiz 2 calculating magnitude and normalization of Vectors"
  },
  {
    "code": "def _update(self, force=False):\n    log.debug('Updating workflow lists...')\n    args = ['/usr/bin/python', self.wf.workflowfile('update_workflows.py')]\n    if force:\n        args.append('--force-update')\n    log.debug('update command : %r', args)\n    retcode = run_in_background('update', args)\n    if retcode:\n        log.debug('Update failed with code %r', retcode)\n        print('Update failed')\n        return 1\n    if force:\n        print('Updating workflow list\u2026'.encode('utf-8'))\n    return 0",
    "nl": "Update cached data",
    "original_nl": "Update cached data"
  },
  {
    "code": "def child(self, path):\n    return ZipPath(self.archive, ZIP_PATH_SEP.join([self.pathInArchive, path]))",
    "nl": "Return a new ZipPath representing a path in C{self.archive} which is\na child of this path.\n\n@note: Requesting the C{\"..\"} (or other special name) child will not\n    cause L{InsecurePath} to be raised since these names do not have\n    any special meaning inside a zip archive.  Be particularly\n    careful with the C{path} attribute (if you absolutely must use\n    it) as this means it may include special names with special\n    meaning outside of the context of a zip archive.",
    "original_nl": "Return a new ZipPath representing a path in C{self.archive} which is\na child of this path.\n\n@note: Requesting the C{\"..\"} (or other special name) child will not\n    cause L{InsecurePath} to be raised since these names do not have\n    any special meaning inside a zip archive.  Be particularly\n    careful with the C{path} attribute (if you absolutely must use\n    it) as this means it may include special names with special\n    meaning outside of the context of a zip archive."
  },
  {
    "code": "def insert_option_group(self, idx, *args, **kwargs):\n    group = self.add_option_group(*args, **kwargs)\n    self.option_groups.pop()\n    self.option_groups.insert(idx, group)\n    return group",
    "nl": "Insert an OptionGroup at a given position.",
    "original_nl": "Insert an OptionGroup at a given position."
  },
  {
    "code": "def p_extractkv_opt_list(p):\n    p[0] = ParseTreeNode('EQ', raw='assign')\n    p[0].add_child(p[1])",
    "nl": "wc_stringlist : wc_string",
    "original_nl": "wc_stringlist : wc_string"
  },
  {
    "code": "def update_route_table(self, context, rt_id, route_table):\n    plugin_rt = copy.deepcopy(route_table)\n    rt_dicts = self._core._update_resource('route_table', context, rt_id, plugin_rt)\n    LOG.debug(('update_route_table(): ' + pformat(rt_dicts)))\n    return rt_dicts",
    "nl": "Updates the attributes of a particular route table.",
    "original_nl": "Updates the attributes of a particular route table."
  },
  {
    "code": "def getParameterList():\n    return [str(p) for p in ERT.ert.ensembleConfig().getKeylistFromVarType(EnkfVarType.PARAMETER)]",
    "nl": "@rtype: list[str]",
    "original_nl": "@rtype: list[str]"
  },
  {
    "code": "def _SuffixName(name, suffix):\n    parts = name.rsplit('#', 1)\n    parts[0] = ('%s_%s' % (parts[0], suffix))\n    return '#'.join(parts)",
    "nl": "Add a suffix to the end of a target.\n\nArguments:\n  name: name of the target (foo#target)\n  suffix: the suffix to be added",
    "original_nl": "Add a suffix to the end of a target.\n\nArguments:\n  name: name of the target (foo#target)\n  suffix: the suffix to be added\nReturns:\n  Target name with suffix added (foo_suffix#target)"
  },
  {
    "code": "@abstractproperty\ndef aside_type(self):\n    raise NotImplementedError()",
    "nl": "Return the type of this aside.",
    "original_nl": "Return the type of this aside."
  },
  {
    "code": "def midpoint(self):\n    return Point2d(((self.start.x + self.end.x) / 2), ((self.start.y + self.end.y) / 2))",
    "nl": "Compute the midpoint of this segment.",
    "original_nl": "Compute the midpoint of this segment."
  },
  {
    "code": "@csrf_protect\ndef login(request):\n    redirect_to = '/surface/'\n    template_name = 'surface_login.html'\n    if (request.method == 'POST'):\n        form = AuthenticationForm(request, data=request.POST)\n        if form.is_valid():\n            auth_login(request, form.get_user())\n            return HttpResponseRedirect(redirect_to)\n    else:\n        form = AuthenticationForm(request)\n    context = {\n        'form': form,\n        'next': redirect_to,\n    }\n    return TemplateResponse(request, template_name, context)",
    "nl": "simple login view, always redirect to surface root",
    "original_nl": "simple login view, always redirect to surface root"
  },
  {
    "code": "def main__test__get_own_log_calls_wrapper():\n    pass",
    "nl": "Class we'll use through this entire set of tests:\n\n    >>> @log_calls(omit='no_deco', mute=log_calls.MUTE.CALLS)\n    ... class B():\n    ...     def __init__(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...     def method(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...     def no_deco(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()         # raises ValueError\n    ...         wrapper.log_message('Hi')\n    ...     @staticmethod\n    ...     def statmethod():\n    ...         wrapper = B.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...\n    ...     @classmethod\n    ...     def clsmethod(cls):\n    ...         wrapper = B.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...\n    ...     @property\n    ...     def prop(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...     @prop.setter\n    ...     @log_calls(name='B.%s.setter')\n    ...     def prop(self, val):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...\n    ...     def setx(self, val):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi from setx alias x.setter')\n    ...     def delx(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi from delx alias x.deleter')\n    ...     x = property(None, setx, delx)\n\n    >>> b = B()\n    B.__init__: Hi\n    >>> b.method()\n    B.method: Hi\n    >>> b.no_deco()     # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    AttributeError: ...\n    >>> b.statmethod()\n    B.statmethod: Hi\n    >>> b.clsmethod()\n    B.clsmethod: Hi\n    >>> b.prop\n    B.prop: Hi\n    >>> b.prop = 17\n    B.prop.setter: Hi\n    >>> b.x = 13\n    B.setx: Hi from setx alias x.setter\n    >>> del b.x\n    B.delx: Hi from delx alias x.deleter\n\nThis won't work/is wrong:\n\n    try:\n        b.method.get_own_log_calls_wrapper()\n    except AttributeError as e:\n        # 'function' object has no attribute 'get_own_log_calls_wrapper'\n        print(e)\n\n    >>> try:\n    ...     b.no_deco()\n    ... except AttributeError as e:\n    ...     print(e)\n    'no_deco' is not decorated [1]\n\n    >>> b.method.log_calls_settings.enabled = 0\n    >>> # no log_* output if enabled <= 0, but method can still call get_own_log_calls_wrapper\n    >>> b.method()\n\n    >>> b.method.log_calls_settings.enabled = -1\n    >>> # \"true bypass\" -- method can't call get_own_log_calls_wrapper\n    >>> try:\n    ...     b.method()\n    ... except AttributeError as e:\n    ...     print(e)\n    'method' is true-bypassed (enabled < 0) or not decorated [2]\n\nInduce more errors\n\n    >>> def _deco_base_f_wrapper_():     # note name -- fake out get_own_log_calls_wrapper for a few clauses\n    ...     # No local named _deco_base__active_call_items__\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is true-bypassed (enabled < 0) or not decorated [2]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper longer\n    ...     _deco_base__active_call_items__ = 17    # exists but isn't a dict\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is not decorated [3]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper still longer\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, no key '_wrapper_deco'\n    ...         'a': 45\n    ...     }\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is not decorated [3]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': 45\n    ...     }\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is not decorated [3]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': log_calls()    # correct type, but not hooked up properly\n    ...     }\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [4]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     lc = log_calls()    # correct type, but still not hooked up properly\n    ...     lc.f = None\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': lc\n    ...     }\n    ...\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [5]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     lc = log_calls()    # correct type, but still not hooked up properly\n    ...     lc.f = None\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': lc\n    ...     }\n    ...\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [5]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     lc = log_calls()    # correct type, lc.f correct, but STILL not hooked up properly\n    ...     lc.f = B.no_deco\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': lc\n    ...     }\n    ...\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [7]",
    "original_nl": "Class we'll use through this entire set of tests:\n\n    >>> @log_calls(omit='no_deco', mute=log_calls.MUTE.CALLS)\n    ... class B():\n    ...     def __init__(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...     def method(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...     def no_deco(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()         # raises ValueError\n    ...         wrapper.log_message('Hi')\n    ...     @staticmethod\n    ...     def statmethod():\n    ...         wrapper = B.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...\n    ...     @classmethod\n    ...     def clsmethod(cls):\n    ...         wrapper = B.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...\n    ...     @property\n    ...     def prop(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...     @prop.setter\n    ...     @log_calls(name='B.%s.setter')\n    ...     def prop(self, val):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi')\n    ...\n    ...     def setx(self, val):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi from setx alias x.setter')\n    ...     def delx(self):\n    ...         wrapper = self.get_own_log_calls_wrapper()\n    ...         wrapper.log_message('Hi from delx alias x.deleter')\n    ...     x = property(None, setx, delx)\n\n    >>> b = B()\n    B.__init__: Hi\n    >>> b.method()\n    B.method: Hi\n    >>> b.no_deco()     # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    AttributeError: ...\n    >>> b.statmethod()\n    B.statmethod: Hi\n    >>> b.clsmethod()\n    B.clsmethod: Hi\n    >>> b.prop\n    B.prop: Hi\n    >>> b.prop = 17\n    B.prop.setter: Hi\n    >>> b.x = 13\n    B.setx: Hi from setx alias x.setter\n    >>> del b.x\n    B.delx: Hi from delx alias x.deleter\n\nThis won't work/is wrong:\n\n    try:\n        b.method.get_own_log_calls_wrapper()\n    except AttributeError as e:\n        # 'function' object has no attribute 'get_own_log_calls_wrapper'\n        print(e)\n\n    >>> try:\n    ...     b.no_deco()\n    ... except AttributeError as e:\n    ...     print(e)\n    'no_deco' is not decorated [1]\n\n    >>> b.method.log_calls_settings.enabled = 0\n    >>> # no log_* output if enabled <= 0, but method can still call get_own_log_calls_wrapper\n    >>> b.method()\n\n    >>> b.method.log_calls_settings.enabled = -1\n    >>> # \"true bypass\" -- method can't call get_own_log_calls_wrapper\n    >>> try:\n    ...     b.method()\n    ... except AttributeError as e:\n    ...     print(e)\n    'method' is true-bypassed (enabled < 0) or not decorated [2]\n\nInduce more errors\n\n    >>> def _deco_base_f_wrapper_():     # note name -- fake out get_own_log_calls_wrapper for a few clauses\n    ...     # No local named _deco_base__active_call_items__\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is true-bypassed (enabled < 0) or not decorated [2]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper longer\n    ...     _deco_base__active_call_items__ = 17    # exists but isn't a dict\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is not decorated [3]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper still longer\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, no key '_wrapper_deco'\n    ...         'a': 45\n    ...     }\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is not decorated [3]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': 45\n    ...     }\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    'no_deco' is not decorated [3]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': log_calls()    # correct type, but not hooked up properly\n    ...     }\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [4]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     lc = log_calls()    # correct type, but still not hooked up properly\n    ...     lc.f = None\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': lc\n    ...     }\n    ...\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [5]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     lc = log_calls()    # correct type, but still not hooked up properly\n    ...     lc.f = None\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': lc\n    ...     }\n    ...\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [5]\n\n    >>> def _deco_base_f_wrapper_():         # note name -- fake out get_own_log_calls_wrapper even longer\n    ...     lc = log_calls()    # correct type, lc.f correct, but STILL not hooked up properly\n    ...     lc.f = B.no_deco\n    ...     _deco_base__active_call_items__ = {    # exists, is a dict, has key '_wrapper_deco', but type != log_calls\n    ...         '_wrapper_deco': lc\n    ...     }\n    ...\n    ...     try:\n    ...         b.no_deco()\n    ...     except AttributeError as e:\n    ...         print(e)\n    >>> _deco_base_f_wrapper_()\n    inconsistent log_calls decorator object for 'no_deco' [7]"
  },
  {
    "code": "def reshape(self, *newshape):\n    new_total_size = functools.reduce((lambda x, y: (x * y)), newshape)\n    if (new_total_size != self._loop_size):\n        raise ValueError(('Invalid reshape parameters ' + newshape))\n    return type(self)(self._array, newshape)",
    "nl": "Returns MutableDenseNDimArray instance with new shape. Elements number\nmust be        suitable to new shape. The only argument of method sets\nnew shape.\n\nExamples\n========\n\n>>> from sympy import MutableDenseNDimArray\n>>> a = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n>>> a.shape\n(2, 3)\n>>> a\n[[1, 2, 3], [4, 5, 6]]\n>>> b = a.reshape(3, 2)\n>>> b.shape\n(3, 2)\n>>> b\n[[1, 2], [3, 4], [5, 6]]",
    "original_nl": "Returns MutableDenseNDimArray instance with new shape. Elements number\nmust be        suitable to new shape. The only argument of method sets\nnew shape.\n\nExamples\n========\n\n>>> from sympy import MutableDenseNDimArray\n>>> a = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n>>> a.shape\n(2, 3)\n>>> a\n[[1, 2, 3], [4, 5, 6]]\n>>> b = a.reshape(3, 2)\n>>> b.shape\n(3, 2)\n>>> b\n[[1, 2], [3, 4], [5, 6]]"
  },
  {
    "code": "def one_hot_to_string(self, matrix):\n    return ''.join((self.one_hot_to_char(vector) for vector in matrix))",
    "nl": "Given a matrix of single one-hot encoded char vectors, returns the\nencoded string.",
    "original_nl": "Given a matrix of single one-hot encoded char vectors, returns the\nencoded string."
  },
  {
    "code": "def chanpress(ch, value, start=(- 1)):\n    if (start < 0):\n        return (alsaseq.SND_SEQ_EVENT_CHANPRESS, alsaseq.SND_SEQ_TIME_STAMP_REAL, 0, SND_SEQ_QUEUE_DIRECT, ((start / 1000), ((start % 1000) * 1000000)), (0, 0), (0, 0), (ch, 0, value))\n    else:\n        return (alsaseq.SND_SEQ_EVENT_CHANPRESS, alsaseq.SND_SEQ_TIME_STAMP_REAL, 0, queue, ((start / 1000), ((start % 1000) * 1000000)), (0, 0), (0, 0), (ch, 0, value))",
    "nl": "Return an ALSA event tuple to be sent by alsaseq.output().\n\nIf start is not used, the event will be sent directly.\nIf start is provided, the event will be scheduled in a queue.",
    "original_nl": "Return an ALSA event tuple to be sent by alsaseq.output().\n\nIf start is not used, the event will be sent directly.\nIf start is provided, the event will be scheduled in a queue."
  },
  {
    "code": "def get_qos_policy_id(self, policy):\n    return self.find_resourceid_by_name_or_id('policy', policy)",
    "nl": "Returns the id of QoS policy.\n\nArgs:\npolicy: ID or name of the policy.",
    "original_nl": "Returns the id of QoS policy.\n\nArgs:\npolicy: ID or name of the policy."
  },
  {
    "code": "def handle_trunks(self, context, resource_type, trunk, event_type):\n    raise NotImplementedError()",
    "nl": "This method is not required by the OVS Agent driver.\n\nTrunk notifications are handled via local OVSDB events.",
    "original_nl": "This method is not required by the OVS Agent driver.\n\nTrunk notifications are handled via local OVSDB events."
  },
  {
    "code": "@patch('django.utils.translation.activate')\ndef test_setup_lang_no_user(self, mock_translation_activate):\n    setup_lang('bademail@gitcoin.co')\n    assert (mock_translation_activate.call_count == 0)",
    "nl": "Test the marketing mails setup_lang method.",
    "original_nl": "Test the marketing mails setup_lang method."
  },
  {
    "code": "def _ParseDefinitions(self, tagging_file_path):\n    queries = None\n    label_name = None\n    with io.open(tagging_file_path, 'r', encoding='utf-8') as tagging_file:\n        for line in tagging_file.readlines():\n            label_match = self._TAG_LABEL_LINE.match(line)\n            if label_match:\n                if (label_name and queries):\n                    (yield (label_name, queries))\n                queries = []\n                label_name = label_match.group(1)\n                continue\n            event_tagging_expression = self._TAG_RULE_LINE.match(line)\n            if (not event_tagging_expression):\n                continue\n            tagging_rule = self._ParseEventTaggingRule(event_tagging_expression.group(1))\n            queries.append(tagging_rule)\n        if (label_name and queries):\n            (yield (label_name, queries))",
    "nl": "Parses the tag file and yields tuples of label name, list of rule ASTs.\n\nArgs:\n  tagging_file_path (str): path to the tagging file.\n\nYields:\n  tuple: contains:\n\n    str: label name.\n    list[efilter.query.Query]: efilter queries.",
    "original_nl": "Parses the tag file and yields tuples of label name, list of rule ASTs.\n\nArgs:\n  tagging_file_path (str): path to the tagging file.\n\nYields:\n  tuple: contains:\n\n    str: label name.\n    list[efilter.query.Query]: efilter queries."
  },
  {
    "code": "def set_size(self, possible_size, offset, global_offset, flags):\n    self.base_global_offset = global_offset\n    self.offset = offset\n    size = Vector2(0, 0)\n    if (flags & FILLX):\n        if ((self.requested_size.x == 0) or (self.requested_size.x == (- 1))):\n            size.x = possible_size.x\n        else:\n            raise Exception('can not FILLX, as widget.style.x is already set')\n    elif (self.requested_size.x == 0):\n        raise Exception('widget.style.x is equal to 0')\n    elif (self.requested_size.x == (- 1)):\n        size.x = possible_size.x\n    else:\n        size.x = self.requested_size.x\n    if (flags & FILLY):\n        if ((self.requested_size.y == 0) or (self.requested_size.y == (- 1))):\n            size.y = possible_size.y\n        else:\n            raise Exception('can not FILLY, as widget.style.y is already set')\n    elif (self.requested_size.y == 0):\n        raise Exception('widget.style.y is equal to 0')\n    elif (self.requested_size.y == (- 1)):\n        size.y = possible_size.y\n    else:\n        size.y = self.requested_size.y\n    size -= self.margin.widthheight\n    self.size = size\n    if (flags & CENTERX):\n        space = (possible_size.x - size.x)\n        self.margin.left = (space / 2.0)\n        self.margin.right = (space / 2.0)\n    if (flags & CENTERY):\n        space = (possible_size.y - size.y)\n        self.margin.top = (space / 2.0)\n        self.margin.bottom = (space / 2.0)\n    self.rect = Rect(self.total_offset, self.size)\n    self.global_rect = Rect((self.base_global_offset + self.total_offset), self.size)",
    "nl": "Automatically calculate the widgets size and position.  Size will never exceed possible_size, but may be\nsmaller depending on flags.  The calculation is also influenced by the widgets self.requested_size\n\nFILLX\nFILLY\nCENTERX\nCENTERY\n\n",
    "original_nl": "Automatically calculate the widgets size and position.  Size will never exceed possible_size, but may be\nsmaller depending on flags.  The calculation is also influenced by the widgets self.requested_size\n\nFILLX\nFILLY\nCENTERX\nCENTERY\n\n:param possible_size: Vector2 of the maximum size the parent panel(widget) has allotted this widget.\n:param offset:  Vector2 top left corner of where this widget will draw n the parent's surface\n:param global_offset: total screen global offset of where offset is actually located. This is needed for\n                      to maintain screen space rect's of this widget. useful for mouse clicks..\n:param flags: Positioning flags to influence the automatic fitting.\n:return: None"
  },
  {
    "code": "def elapsed_time(self):\n    return format((self._stop_time - self._start_time), '.3f')",
    "nl": "Returns the time the context took to run between the calls to\n`begin()` and `end()`, in seconds.",
    "original_nl": "Returns the time the context took to run between the calls to\n`begin()` and `end()`, in seconds."
  },
  {
    "code": "def parse(self, start_date, end_date):\n    start_date = datetime.strptime(start_date, '%Y%m%d')\n    end_date = datetime.strptime(end_date, '%Y%m%d')\n    date = start_date\n    delta_time = timedelta(days=1)\n    while (date <= end_date):\n        file_name = os.path.join(self._config['log_dir'], date.strftime('%Y%m%d'))\n        if os.path.exists(file_name):\n            self.parse_file(file_name)\n        else:\n            sys.stderr.write(\"no log file '{0}'\\n\".format(file_name))\n        date += delta_time",
    "nl": "Parse log information from the given start date to the end\ndate, inclusive",
    "original_nl": "Parse log information from the given start date to the end\ndate, inclusive"
  },
  {
    "code": "def get_message_details(self):\n    dws_group = frappe.get_doc('Daily Work Summary Group', self.daily_work_summary_group)\n    replies = frappe.get_all('Communication', fields=['content', 'text_content', 'sender'], filters=dict(reference_doctype=self.doctype, reference_name=self.name, communication_type='Communication', sent_or_received='Received'), order_by='creation asc')\n    did_not_reply = self.email_sent_to.split()\n    for d in replies:\n        user = frappe.db.get_values('User', {\n            'email': d.sender,\n        }, ['full_name', 'user_image'], as_dict=True)\n        d.sender_name = (user[0].full_name if user else d.sender)\n        d.image = (user[0].image if (user and user[0].image) else None)\n        original_image = d.image\n        try:\n            if original_image:\n                file_name = frappe.get_list('File', {\n                    'file_url': original_image,\n                })\n                if file_name:\n                    file_name = file_name[0].name\n                    file_doc = frappe.get_doc('File', file_name)\n                    thumbnail_image = file_doc.make_thumbnail(set_as_thumbnail=False, width=100, height=100, crop=True)\n                    d.image = thumbnail_image\n        except:\n            d.image = original_image\n        if (d.sender in did_not_reply):\n            did_not_reply.remove(d.sender)\n        if d.text_content:\n            d.content = markdown(EmailReplyParser.parse_reply(d.text_content))\n    did_not_reply = [(frappe.db.get_value('User', {\n        'email': email,\n    }, 'full_name') or email) for email in did_not_reply]\n    return dict(replies=replies, original_message=dws_group.message, title=_('Work Summary for {0}'.format(global_date_format(self.creation))), did_not_reply=(', '.join(did_not_reply) or ''), did_not_reply_title=_('No replies from'))",
    "nl": "Return args for template",
    "original_nl": "Return args for template"
  },
  {
    "code": "def _get_results(self):\n    statepoint = glob.glob(os.path.join(os.getcwd(), self._sp_name))[0]\n    with StatePoint(statepoint) as sp:\n        outstr = 'k-combined:\\n'\n        outstr += '{:12.6E} {:12.6E}\\n'.format(sp.k_combined.n, sp.k_combined.s)\n        outstr += 'entropy:\\n'\n        results = ['{:12.6E}'.format(x) for x in sp.entropy]\n        outstr += ('\\n'.join(results) + '\\n')\n    return outstr",
    "nl": "Digest info in the statepoint and return as a string.",
    "original_nl": "Digest info in the statepoint and return as a string."
  },
  {
    "code": "def to_python(self, value):\n    try:\n        value = super(DecOrHexIntegerField, self).to_python(value)\n    except core.exceptions.ValidationError:\n        try:\n            value = int(str(value), 0)\n        except (ValueError, TypeError):\n            raise core.exceptions.ValidationError(self.error_messages['invalid'])\n    return value",
    "nl": "Validates that int() can be called on the input. Returns the result\nof int(). Returns None for empty values.",
    "original_nl": "Validates that int() can be called on the input. Returns the result\nof int(). Returns None for empty values."
  },
  {
    "code": "@classmethod\ndef all(cls, path=''):\n    url = urljoin(cls._meta.base_url, path)\n    pq_items = cls._get_items(url=url, **cls._meta._pyquery_kwargs)\n    return [cls(item=i) for i in pq_items.items()]",
    "nl": "Return all ocurrences of the item.",
    "original_nl": "Return all ocurrences of the item."
  },
  {
    "code": "def __len__(self):\n    return len(self.row)",
    "nl": "Return how many columns are in this row",
    "original_nl": "Return how many columns are in this row"
  },
  {
    "code": "def exists(self, path):\n    return (self._getReturnCodeCmd([self._hadoop_cmd, 'fs', '-test', '-e', path]) == 0)",
    "nl": "Return True if <src> exists, False if doesn't",
    "original_nl": "Return True if <src> exists, False if doesn't"
  },
  {
    "code": "def on_change_user(self, cr, uid, ids, user_id, section_id, context=None):\n    if user_id:\n        if section_id:\n            user_in_section = self.pool.get('crm.case.section').search(cr, uid, [('id', '=', section_id), '|', ('user_id', '=', user_id), ('member_ids', '=', user_id)], context=context, count=True)\n        else:\n            user_in_section = False\n        if (not user_in_section):\n            result = self.pool['crm.lead'].on_change_user(cr, uid, ids, user_id, context=context)\n            section_id = ((result.get('value') and result['value'].get('section_id') and result['value']['section_id']) or False)\n    return {\n        'value': {\n            'section_id': section_id,\n        },\n    }",
    "nl": "When changing the user, also set a section_id or restrict section id\nto the ones user_id is member of.",
    "original_nl": "When changing the user, also set a section_id or restrict section id\nto the ones user_id is member of."
  },
  {
    "code": "@abstractmethod\ndef upload_multipart(self, stream, cloud_name, metadata, permissions, buffersize, verbose):\n    pass",
    "nl": "Multi-part upload for large stream objects\n",
    "original_nl": "Multi-part upload for large stream objects\n\nParameters\n----------\nstream : stream\n    streaming object\ncloud_name : str\n    name to use on cloud\nmetadata : dict\n    custom metadata\npermissions : str?\n    permissions for this file\nbuffersize : int\n    s3 uploading buffersize\nverbose : bool\n    s3 verbosity\n\nReturns\n-------\nbool, upload success"
  },
  {
    "code": "def filename_task(task, session):\n    items = (task.items if task.is_album else [task.item])\n    missing_titles = sum((bad_title(i.title) for i in items))\n    if missing_titles:\n        names = {\n            \n        }\n        for item in items:\n            path = displayable_path(item.path)\n            (name, _) = os.path.splitext(os.path.basename(path))\n            names[item] = name\n        for pattern in PATTERNS:\n            d = all_matches(names, pattern)\n            if d:\n                apply_matches(d)",
    "nl": "Examine each item in the task to see if we can extract a title\nfrom the filename. Try to match all filenames to a number of\nregexps, starting with the most complex patterns and successively\ntrying less complex patterns. As soon as all filenames match the\nsame regex we can make an educated guess of which part of the\nregex that contains the title.",
    "original_nl": "Examine each item in the task to see if we can extract a title\nfrom the filename. Try to match all filenames to a number of\nregexps, starting with the most complex patterns and successively\ntrying less complex patterns. As soon as all filenames match the\nsame regex we can make an educated guess of which part of the\nregex that contains the title."
  },
  {
    "code": "def serveThread(self):\n    while True:\n        try:\n            client = self.clients.get()\n            self.serveClient(client)\n        except Exception as x:\n            logger.exception(x)",
    "nl": "Loop around getting clients from the shared queue and process them.",
    "original_nl": "Loop around getting clients from the shared queue and process them."
  },
  {
    "code": "def strip_color(text):\n    if isinstance(text, bytes):\n        return _ANSI_ESC_RE_B.sub(b'', text)\n    return _ANSI_ESC_RE.sub('', text)",
    "nl": "Strip ansi escape codes from the passed text",
    "original_nl": "Strip ansi escape codes from the passed text"
  },
  {
    "code": "def __init__(self, filename, screen=(lambda cf: True)):\n    self._filename = filename\n    datadump = open(filename).read()\n    data = json.loads(datadump)\n    self._jsoneci_full = data\n    self._jsoneci = {\n        'cluster_functions': [],\n    }\n    screened = [cf for cf in data['cluster_functions'] if screen(cf)]\n    self._jsoneci['cluster_functions'] = screened\n    self._pdeci = squashed_eci(json_eci_to_pandas(self._jsoneci))\n    self._pdeci = self._pdeci.reset_index()",
    "nl": "Constructs the object, saving all the data from the json\nfile, making a pandas representation, and setting different",
    "original_nl": "Constructs the object, saving all the data from the json\nfile, making a pandas representation, and setting different\nparameters for plotting, etc.\nA screen function can be passed that takes a cluster function\ndict from the eci.json file and returns False it should be\nscreened out, or True if it should be kept.\n\n:filename: eci.json path\n:screen: function that returns True or False for a given cluster function"
  },
  {
    "code": "def list_all_sgs(db_session):\n    sg_type = sql_types['ServerGroup']\n    sg_names = db_session.query(sg_type.name).all()\n    sg_names = [x[0] for x in sg_names]\n    return sg_names",
    "nl": "Return a list of all SG names",
    "original_nl": "Return a list of all SG names"
  },
  {
    "code": "def client_part2(seckey_x, msg, node_id, pubkey_B, keyBytes=72):\n    assert (len(msg) == (G_LENGTH + H_LENGTH))\n    pubkey_Y = curve25519mod.Public(msg[:G_LENGTH])\n    their_auth = msg[G_LENGTH:]\n    pubkey_X = seckey_x.get_public()\n    yx = seckey_x.get_shared_key(pubkey_Y, hash_nil)\n    bx = seckey_x.get_shared_key(pubkey_B, hash_nil)\n    secret_input = ((((((yx + bx) + node_id) + pubkey_B.serialize()) + pubkey_X.serialize()) + pubkey_Y.serialize()) + PROTOID)\n    verify = H_verify(secret_input)\n    auth_input = ((((((verify + node_id) + pubkey_B.serialize()) + pubkey_Y.serialize()) + pubkey_X.serialize()) + PROTOID) + b'Server')\n    my_auth = H_mac(auth_input)\n    badness = (my_auth != their_auth)\n    badness = (bad_result(yx) + bad_result(bx))\n    if badness:\n        return None\n    return kdf_ntor(secret_input, keyBytes)",
    "nl": "Handshake step 3: client side again.\n\nFrom the spec:\n\n<<\n  The client then checks Y is in G^* [see NOTE below], and computes\n\n  secret_input = EXP(Y,x) | EXP(B,x) | ID | B | X | Y | PROTOID\n  KEY_SEED = H(secret_input, t_key)\n  verify = H(secret_input, t_verify)\n  auth_input = verify | ID | B | Y | X | PROTOID | \"Server\"\n\n  The client verifies that AUTH == H(auth_input, t_mac).\n>>\n\nTakes seckey_x -- the secret key we generated in step 1.\n      msg -- the message from the server.\n      node_id -- the node_id we used in step 1.\n      server_key -- the same public key we used in step 1.\n      keyBytes -- the number of bytes we want to generate",
    "original_nl": "Handshake step 3: client side again.\n\nFrom the spec:\n\n<<\n  The client then checks Y is in G^* [see NOTE below], and computes\n\n  secret_input = EXP(Y,x) | EXP(B,x) | ID | B | X | Y | PROTOID\n  KEY_SEED = H(secret_input, t_key)\n  verify = H(secret_input, t_verify)\n  auth_input = verify | ID | B | Y | X | PROTOID | \"Server\"\n\n  The client verifies that AUTH == H(auth_input, t_mac).\n>>\n\nTakes seckey_x -- the secret key we generated in step 1.\n      msg -- the message from the server.\n      node_id -- the node_id we used in step 1.\n      server_key -- the same public key we used in step 1.\n      keyBytes -- the number of bytes we want to generate\nReturns key material, or None on error"
  },
  {
    "code": "def inverse(self):\n    new_g = DiGraph(multiple_edges=self.multiple_edges)\n    for edge in self.edges:\n        new_edge = copy.deepcopy(edge)\n        new_edge.inverse()\n        new_g.add_edge(new_edge)\n    new_g.freeze()\n    return new_g",
    "nl": "Returns a frozen copy of this graph where all edges have been inverted.",
    "original_nl": "Returns a frozen copy of this graph where all edges have been inverted."
  },
  {
    "code": "def test_get_api_key():\n    umi = email_auth.EmailAuth()\n    umi.user_key_table = TEST_TABLE\n    request = DummyRequest()\n    test_username = 'test@test.com'\n    test_api_key = 'TestAPIKEY'\n    form = DummyForm({\n        'username': test_username,\n    })\n    request.add_form(form)\n    with open('temp_file.txt', 'w') as user_table:\n        user_table.write(('%s,%s\\n' % (test_username, test_api_key)))\n    assert (umi.get_api_key_for_user(request) == test_api_key)",
    "nl": "Verify that when the user logs in with an existing username, the \ncorresponding API key is returned.",
    "original_nl": "Verify that when the user logs in with an existing username, the \ncorresponding API key is returned."
  },
  {
    "code": "def corr(label, pred):\n    numerator1 = (label - np.mean(label, axis=0))\n    numerator2 = (pred - np.mean(pred, axis=0))\n    numerator = np.mean((numerator1 * numerator2), axis=0)\n    denominator = (np.std(label, axis=0) * np.std(pred, axis=0))\n    return np.mean((numerator / denominator))",
    "nl": "computes the empirical correlation coefficient",
    "original_nl": "computes the empirical correlation coefficient"
  },
  {
    "code": "def requestAvatarId(self, c):\n    up = credentials.IUsernamePassword(c, None)\n    username = up.username\n    password = up.password\n    player = PlayerDB.objects.get_player_from_name(username)\n    res = (None, self.factory)\n    if (player and player.check_password(password)):\n        res = (player, self.factory)\n    return defer.succeed(res)",
    "nl": "Generic credentials.",
    "original_nl": "Generic credentials."
  },
  {
    "code": "@classmethod\ndef write(cls, fname=None, outpath=None, fpath=None, structure=None, atoms=None, comment_line=None, **kwargs):\n    if ((structure is None) and (atoms is None)):\n        raise ValueError('Expected either `structure` or `atoms` object.')\n    if ((structure is not None) and (atoms is None)):\n        atoms = structure.atoms\n    if (fpath is None):\n        fpath = get_fpath(fname=fname, ext='xyz', outpath=outpath, overwrite=True, add_fnum=False)\n    if (comment_line is None):\n        comment_line = default_comment_line\n    atoms.rezero_coords()\n    with zopen(fpath, 'wt') as f:\n        f.write('{:d}\\n'.format(atoms.Natoms))\n        f.write('{}\\n'.format(comment_line))\n        for atom in atoms:\n            f.write('{:>3s}{:15.8f}{:15.8f}{:15.8f}\\n'.format(atom.symbol, atom.x, atom.y, atom.z))",
    "nl": "Write structure data to file.\n",
    "original_nl": "Write structure data to file.\n\nParameters\n----------\nfname : str, optional\n    Output file name.\noutpath : str, optional\n    Output file path.\nfpath : str, optional\n    Full path (directory path + file name) to output data file.\natoms : :class:`~sknano.core.atoms.Atoms`\n    An :class:`~sknano.core.atoms.Atoms` instance.\ncomment_line : str, optional\n    A string written to the first line of `xyz` file. If `None`,\n    then it is set to the full path of the output `xyz` file."
  },
  {
    "code": "@command.skill_level('administrator')\ndef do_providers(self, context, ra_type, ra_class='ocf'):\n    print(' '.join(ra.ra_providers(ra_type, ra_class)))",
    "nl": "usage: providers <ra> [<class>]",
    "original_nl": "usage: providers <ra> [<class>]"
  },
  {
    "code": "def dump_part(part, total_segments=None):\n    try:\n        connection = Connection(host=config['host'], region=config['region'])\n        filename = '.'.join([config['table_name'], str(part), 'dump'])\n        if config['compress']:\n            opener = gzip.GzipFile\n            filename += '.gz'\n        else:\n            opener = open\n        dumper = BatchDumper(connection, config['table_name'], config['capacity'], part, total_segments)\n        with opener(filename, 'w') as output:\n            while dumper.has_items:\n                items = dumper.get_items()\n                for item in items:\n                    output.write(json.dumps(item))\n                    output.write('\\n')\n                output.flush()\n                config['queue'].put(len(items))\n        config['queue'].put('complete')\n    except Exception as e:\n        print('Unhandled exception: {0}'.format(e))",
    "nl": "'part' may be the hash_key if we are dumping just a few hash_keys - else\nit will be the segment number",
    "original_nl": "'part' may be the hash_key if we are dumping just a few hash_keys - else\nit will be the segment number"
  },
  {
    "code": "def launch(*args, **options):\n    action = args[0]\n    if options['reload']:\n        logReload(options)\n    else:\n        assignDeploymentInstance(action, options)",
    "nl": "launch acts on the user specified action and options by executing\nHedrix.run",
    "original_nl": "launch acts on the user specified action and options by executing\nHedrix.run"
  },
  {
    "code": "def __eq__(self, other):\n    return bool(capi.feature_equal(self.ptr, other._ptr))",
    "nl": "Does equivalence testing on the features.",
    "original_nl": "Does equivalence testing on the features."
  },
  {
    "code": "def test_day_tweets(self):\n    tweets = ditto_twitter.day_tweets(datetime.date(2015, 3, 18))\n    self.assertEqual(2, len(tweets))\n    self.assertEqual(tweets[0].pk, self.tweets_2[1].pk)\n    self.assertEqual(tweets[1].pk, self.tweets_1[0].pk)",
    "nl": "Returns only public Tweets from the date",
    "original_nl": "Returns only public Tweets from the date"
  },
  {
    "code": "def spev(t_int, C, deg, x, cov_C=None, M_spline=False, I_spline=False, n=0):\n    C = scipy.asarray(C, dtype=float)\n    t_int = scipy.asarray(t_int, dtype=float)\n    if (t_int != scipy.sort(t_int)).any():\n        raise ValueError('Knots must be in increasing order!')\n    if (n > deg):\n        return scipy.zeros_like(x, dtype=float)\n    if I_spline:\n        if (cov_C is not None):\n            cov_C = scipy.asarray(cov_C)\n            if (cov_C.ndim == 1):\n                cov_C = cov_C[1:]\n            elif (cov_C.ndim == 2):\n                cov_C = cov_C[1:, 1:]\n        if (n > 0):\n            return spev(t_int, C[1:], (deg - 1), x, cov_C=cov_C, M_spline=True, I_spline=False, n=(n - 1))\n        M_spline = True\n    if (n > 0):\n        if M_spline:\n            t = scipy.concatenate((([t_int[0]] * deg), t_int, ([t_int[(- 1)]] * deg)))\n            C = ((deg + 1.0) * ((C[1:] / (t[(deg + 2):(len(t_int) + (2 * deg))] - t[1:((len(t_int) + deg) - 1)])) - (C[:(- 1)] / (t[(deg + 1):((len(t_int) + (2 * deg)) - 1)] - t[:((len(t_int) + deg) - 2)]))))\n        else:\n            C = (C[1:] - C[:(- 1)])\n        return spev(t_int, C, (deg - 1), x, cov_C=cov_C, M_spline=True, I_spline=False, n=(n - 1))\n    if (len(C) != ((len(t_int) + deg) - 1)):\n        raise ValueError('Length of C must be equal to M + deg - 1!')\n    t = scipy.concatenate((([t_int[0]] * deg), t_int, ([t_int[(- 1)]] * deg)))\n    B = scipy.zeros(((deg + 1), (len(t) - 1), len(x)))\n    d = 0\n    for i in xrange(deg, (((deg + len(t_int)) - 2) + 1)):\n        mask = ((t[i] <= x) & ((x < t[(i + 1)]) | ((i == ((deg + len(t_int)) - 2)) & (x == t[(- 1)]))))\n        B[(d, i, mask)] = ((1.0 / (t[(i + 1)] - t[i])) if M_spline else 1.0)\n    for d in xrange(1, (deg + 1)):\n        for i in xrange((deg - d), (((deg + len(t_int)) - 2) + 1)):\n            if (t[(i + d)] != t[i]):\n                v = ((x - t[i]) * B[(d - 1), i, :])\n                if (not M_spline):\n                    v /= (t[(i + d)] - t[i])\n                B[d, i, :] += v\n            if (t[((i + d) + 1)] != t[(i + 1)]):\n                v = ((t[((i + d) + 1)] - x) * B[(d - 1), (i + 1), :])\n                if (not M_spline):\n                    v /= (t[((i + d) + 1)] - t[(i + 1)])\n                B[d, i, :] += v\n            if (M_spline and ((t[(i + d)] != t[i]) or (t[((i + d) + 1)] != t[(i + 1)]))):\n                B[d, i, :] *= ((d + 1) / (d * (t[((i + d) + 1)] - t[i])))\n    B = B[deg, 0:len(C), :].T\n    if I_spline:\n        I = scipy.zeros_like(B)\n        for i in xrange(0, len(C)):\n            for m in xrange(i, len(C)):\n                I[:, i] += (((t[((m + deg) + 1)] - t[m]) * B[:, m]) / (deg + 1.0))\n        B = I\n    y = B.dot(C)\n    if (cov_C is not None):\n        cov_C = scipy.asarray(cov_C)\n        if (cov_C.ndim == 1):\n            cov_C = scipy.diag(cov_C)\n        cov_y = B.dot(cov_C).dot(B.T)\n        return (y, cov_y)\n    else:\n        return y",
    "nl": "Evaluate a B-, M- or I-spline with the specified internal knots, order and coefficients.\n\n`deg` boundary knots are appended at both sides of the domain.\n\nThe zeroth order basis functions are modified to ensure continuity at the\nright-hand boundary.\n\nNote that the I-splines include the :math:`i=0` case in order to have a \"DC\noffset\". This way your functions do not have to start at zero. If you want\nto not include this, simply set the first coefficient in `C` to zero.\n",
    "original_nl": "Evaluate a B-, M- or I-spline with the specified internal knots, order and coefficients.\n\n`deg` boundary knots are appended at both sides of the domain.\n\nThe zeroth order basis functions are modified to ensure continuity at the\nright-hand boundary.\n\nNote that the I-splines include the :math:`i=0` case in order to have a \"DC\noffset\". This way your functions do not have to start at zero. If you want\nto not include this, simply set the first coefficient in `C` to zero.\n\nParameters\n----------\nt_int : array of float, (`M`,)\n    The internal knot locations. Must be monotonic (this is NOT checked).\nC : array of float, (`M + deg - 1`,)\n    The coefficients applied to the basis functions.\ndeg : nonnegative int\n    The polynomial degree to use.\nx : array of float, (`N`,)\n    The locations to evaluate the spline at.\ncov_C : array of float, (`M + deg - 1`,) or (`M + deg - 1`, `M + deg - 1`), optional\n    The covariance matrix of the coefficients. If a 1d array is passed, this\n    is treated as the variance. If None, then the uncertainty is not\n    computed.\nM_spline : bool, optional\n    If True, compute the M-spline instead of the B-spline. M-splines are\n    normalized to integrate to unity, as opposed to B-splines which sum to\n    unity at all points. Default is False (compute B-spline).\nI_spline : bool, optional\n    If True, compute the I-spline instead of the B-spline. Note that this\n    will override `M_spline`. I-splines are the integrals of the M-splines,\n    and hence ensure curves are monotonic if all coefficients are of the\n    same sign. Note that the I-splines returned will be of polynomial degree\n    `deg` (i.e., the integral of what is returned from calling the function\n    with `deg=deg-1` and `M_spline=True`. Default is False (compute B-spline\n    or M-spline).\nn : int, optional\n    The derivative order to compute. Default is 0. If `n>d`, all zeros are\n    returned (i.e., the discontinuities are not included).\n\nReturns\n-------\n`y` or (`y`, `cov_y`): The values (and possibly uncertainties) of the spline\nat the specified locations."
  },
  {
    "code": "@classmethod\ndef obj_to_doc(cls, obj):\n    return {\n        'name': obj.name,\n    }",
    "nl": "Translate an object to an index document.",
    "original_nl": "Translate an object to an index document."
  },
  {
    "code": "def get_original_string(self, name):\n    words = []\n    if self.graph.has_id(name):\n        words.append(self.graph.original_string(name))\n    elif (name[0] == 'N'):\n        words.append(name[1:])\n    elif (name[0] == 'D'):\n        (year, month, day) = name[1:].split('-')\n        if (year[0] != 'x'):\n            words.append(year)\n        if (month[0] != 'x'):\n            words.append(MONTHS[int(month)].title())\n        if (day[0] != 'x'):\n            words.append(day)\n    return ' '.join(words)",
    "nl": "Get the original string. Also works for numbers and dates.\n\nArgs:\n    name (unicode): predicate name",
    "original_nl": "Get the original string. Also works for numbers and dates.\n\nArgs:\n    name (unicode): predicate name\nReturns:\n    unicode"
  },
  {
    "code": "def build_fingerprint_regression_model(fp_length=50, fp_depth=4, conv_width=20, predictor_MLP_layers=[200, 200, 200], L2_reg=0.0004, num_input_atom_features=62, num_bond_features=6, batch_normalization=False):\n    inputs = {\n        \n    }\n    inputs['input_atom_features'] = layers.Input(name='input_atom_features', shape=(num_input_atom_features,))\n    for degree in degrees:\n        inputs[('bond_features_degree_' + str(degree))] = layers.Input(name=('bond_features_degree_' + str(degree)), shape=(num_bond_features,))\n        inputs[('atom_features_selector_matrix_degree_' + str(degree))] = layers.Input(name=('atom_features_selector_matrix_degree_' + str(degree)), shape=(None,))\n        inputs[('atom_batch_matching_matrix_degree_' + str(degree))] = layers.Input(name=('atom_batch_matching_matrix_degree_' + str(degree)), shape=(None,))\n    if 1:\n        atom_features = inputs['input_atom_features']\n        all_outputs_to_fingerprint = []\n        num_atom_features = num_input_atom_features\n        for i in range(fp_depth):\n            (atom_features, output_to_fingerprint) = neural_fingerprint_layer(inputs, atom_features_of_previous_layer=atom_features, num_atom_features=num_atom_features, conv_width=conv_width, fp_length=fp_length, L2_reg=L2_reg, num_bond_features=num_bond_features, batch_normalization=batch_normalization, layer_index=i)\n            num_atom_features = conv_width\n            all_outputs_to_fingerprint.append(output_to_fingerprint)\n        neural_fingerprint = (layers.merge(all_outputs_to_fingerprint, mode='sum') if (len(all_outputs_to_fingerprint) > 1) else all_outputs_to_fingerprint)\n    Prediction_MLP_layer = neural_fingerprint\n    for (i, hidden) in enumerate(predictor_MLP_layers):\n        Prediction_MLP_layer = layers.Dense(hidden, activation='relu', W_regularizer=regularizers.l2(L2_reg), name=('MLP_hidden_' + str(i)))(Prediction_MLP_layer)\n    main_prediction = layers.Dense(1, activation='linear', name='main_prediction')(Prediction_MLP_layer)\n    model = models.Model(input=inputs.values(), output=[main_prediction])\n    model.compile(optimizer=optimizers.Adam(), loss={\n        'main_prediction': 'mse',\n    })\n    return model",
    "nl": "fp_length   # Usually neural fps need far fewer dimensions than morgan.\nfp_depth     # The depth of the network equals the fingerprint radius.\nconv_width   # Only the neural fps need this parameter.\nh1_size     # Size of hidden layer of network on top of fps.",
    "original_nl": "fp_length   # Usually neural fps need far fewer dimensions than morgan.\nfp_depth     # The depth of the network equals the fingerprint radius.\nconv_width   # Only the neural fps need this parameter.\nh1_size     # Size of hidden layer of network on top of fps."
  },
  {
    "code": "def get(self):\n    return self.request().get()",
    "nl": "Sends the GET request\n",
    "original_nl": "Sends the GET request\n\nReturns:\n    :class:`ItemsCollectionResponse<onedrivesdk.request.items_collection.ItemsCollectionResponse>`:\n    The resulting ItemsCollectionResponse from the operation"
  },
  {
    "code": "def has_changed(self):\n    return (getmtime(self.pickle_gz_path) != self._mtime)",
    "nl": "Check to see if the ring on disk is different than the current one in\nmemory.\n\n",
    "original_nl": "Check to see if the ring on disk is different than the current one in\nmemory.\n\n:returns: True if the ring on disk has changed, False otherwise"
  },
  {
    "code": "def error_handler(app):\n    mail_handler = SMTPHandler(app.config['ERROR_MAIL']['smtp'], app.config['ERROR_MAIL']['to'], app.config['ERROR_MAIL']['from'], app.config['ERROR_MAIL']['subject'])\n    mail_handler.setLevel(logging.ERROR)\n    app.logger.addHandler(mail_handler)",
    "nl": "Seperated to keep the `create_app()` clean.\nOnely pass the app context and set the rights variables::\n\nERROR_MAIL = {\n    'smtp': '<SMTP SERVER',\n    'to': ['<TO>'],\n    'from': '<FROM>',\n    'subject': '<SUBJECT>'\n}",
    "original_nl": "Seperated to keep the `create_app()` clean.\nOnely pass the app context and set the rights variables::\n\nERROR_MAIL = {\n    'smtp': '<SMTP SERVER',\n    'to': ['<TO>'],\n    'from': '<FROM>',\n    'subject': '<SUBJECT>'\n}"
  },
  {
    "code": "def set_comments(self, section, comments=[]):\n    self.settings.comments[section] = comments",
    "nl": "Set multi-line comments for a section\n\n@type section:      string\n",
    "original_nl": "Set multi-line comments for a section\n\n@type section:      string\n@param section:     A settings section.\n\n@type comments:     list\n@param comments:    A list of strings."
  },
  {
    "code": "def train(epochs, ctx):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n    opt_options = {\n        'learning_rate': opt.lr,\n        'wd': opt.wd,\n    }\n    if (opt.optimizer == 'sgd'):\n        opt_options['momentum'] = 0.9\n    if (opt.optimizer == 'adam'):\n        opt_options['epsilon'] = 1e-07\n    trainer = gluon.Trainer(net.collect_params(), opt.optimizer, opt_options, kvstore=opt.kvstore)\n    if (opt.lr_beta > 0.0):\n        beta.initialize(mx.init.Constant(opt.beta), ctx=ctx)\n        trainer_beta = gluon.Trainer([beta], 'sgd', {\n            'learning_rate': opt.lr_beta,\n            'momentum': 0.9,\n        }, kvstore=opt.kvstore)\n    loss = MarginLoss(margin=opt.margin, nu=opt.nu)\n    best_val = 0.0\n    for epoch in range(epochs):\n        tic = time.time()\n        (prev_loss, cumulative_loss) = (0.0, 0.0)\n        trainer.set_learning_rate(get_lr(opt.lr, epoch, steps, opt.factor))\n        logging.info('Epoch %d learning rate=%f', epoch, trainer.learning_rate)\n        if (opt.lr_beta > 0.0):\n            trainer_beta.set_learning_rate(get_lr(opt.lr_beta, epoch, steps, opt.factor))\n            logging.info('Epoch %d beta learning rate=%f', epoch, trainer_beta.learning_rate)\n        for i in range(200):\n            batch = train_data.next()\n            data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n            Ls = []\n            with ag.record():\n                for (x, y) in zip(data, label):\n                    (a_indices, anchors, positives, negatives, _) = net(x)\n                    if (opt.lr_beta > 0.0):\n                        L = loss(anchors, positives, negatives, beta, y[a_indices])\n                    else:\n                        L = loss(anchors, positives, negatives, opt.beta, None)\n                    Ls.append(L)\n                    cumulative_loss += nd.mean(L).asscalar()\n                for L in Ls:\n                    L.backward()\n            trainer.step(batch.data[0].shape[0])\n            if (opt.lr_beta > 0.0):\n                trainer_beta.step(batch.data[0].shape[0])\n            if (((i + 1) % opt.log_interval) == 0):\n                logging.info(('[Epoch %d, Iter %d] training loss=%f' % (epoch, (i + 1), (cumulative_loss - prev_loss))))\n                prev_loss = cumulative_loss\n        logging.info(('[Epoch %d] training loss=%f' % (epoch, cumulative_loss)))\n        logging.info(('[Epoch %d] time cost: %f' % (epoch, (time.time() - tic))))\n        (names, val_accs) = test(ctx)\n        for (name, val_acc) in zip(names, val_accs):\n            logging.info(('[Epoch %d] validation: %s=%f' % (epoch, name, val_acc)))\n        if (val_accs[0] > best_val):\n            best_val = val_accs[0]\n            logging.info(('Saving %s.' % opt.save_model_prefix))\n            net.save_params(('%s.params' % opt.save_model_prefix))\n    return best_val",
    "nl": "Training function.",
    "original_nl": "Training function."
  },
  {
    "code": "def lte(value, arg):\n    return (value <= int(arg))",
    "nl": "Returns a boolean of whether the value is less than or equal to the argument",
    "original_nl": "Returns a boolean of whether the value is less than or equal to the argument"
  },
  {
    "code": "def test_456_EqualVariance_Bartlett_unmatched_w_value(self):\n    np.random.seed(987654321)\n    x_parms = [4, 1.35]\n    y_parms = [4, 1.35]\n    z_parms = [4, 0.75]\n    x_input_array = st.norm.rvs(*x_parms, size=100)\n    y_input_array = st.norm.rvs(*y_parms, size=100)\n    z_input_array = st.norm.rvs(*z_parms, size=100)\n    a = 0.05\n    self.assertRaises(KeyError, (lambda : EqualVariance(x_input_array, y_input_array, z_input_array, alpha=a, display=False).w_value))",
    "nl": "Test the EqualVariance class for normally distributed unmatched variances",
    "original_nl": "Test the EqualVariance class for normally distributed unmatched variances"
  },
  {
    "code": "def test_splits_url_parts(self):\n    test_value = 'http://google.com/drives-autonomous_cars'\n    self.assertEqual(set(['cars', 'autonomous']), suggest_tags(test_value))",
    "nl": "- and _ should be good split points",
    "original_nl": "- and _ should be good split points"
  },
  {
    "code": "@call\ndef delete(self, groupId: BlogCollaboratorGroup.Id) -> bool:\n    pass",
    "nl": "Deletes the blog collaborator group and all associated members",
    "original_nl": "Deletes the blog collaborator group and all associated members"
  }
]
