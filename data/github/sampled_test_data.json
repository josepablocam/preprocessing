[
  {
    "code": "def netgroup(base_dn, cn, triples=(), members=()):\n    attr_list = [('objectClass', [b'top', b'nisNetgroup'])]\n    if triples:\n        triples = [triple.encode('utf-8') for triple in triples]\n        attr_list.append(('nisNetgroupTriple', triples))\n    if members:\n        members = [member.encode('utf-8') for member in members]\n        attr_list.append(('memberNisNetgroup', members))\n    return (((('cn=' + cn) + ',ou=Netgroups,') + base_dn), attr_list)",
    "nl": "Generate an RFC2307bis netgroup add-modlist for passing to ldap.add*.",
    "original_nl": "Generate an RFC2307bis netgroup add-modlist for passing to ldap.add*."
  },
  {
    "code": "def testWells(self):\n    return self.test_case.wells()",
    "nl": "Will return a list of wells keys in the test case.",
    "original_nl": "Will return a list of wells keys in the test case."
  },
  {
    "code": "def do_action(self, action: Action):\n    action.do()\n    self._logging(action, 'Last action: ')\n    self._undo.append(action)\n    self._redo.clear()\n    self.action_done.emit(action)",
    "nl": "Execute the action, and add it the `undo` stack.\n\nWhen an action is executed:\n * is logged\n * is appended to the `undo` stack\n * the `redo` stack is cleared to maintain consistency\n * the `action_done` signal is emitted",
    "original_nl": "Execute the action, and add it the `undo` stack.\n\nWhen an action is executed:\n * is logged\n * is appended to the `undo` stack\n * the `redo` stack is cleared to maintain consistency\n * the `action_done` signal is emitted"
  },
  {
    "code": "def cleanup(self):\n    pass",
    "nl": "Backend cleanup. Is run by\n:class:`celery.task.DeleteExpiredTaskMetaTask`.",
    "original_nl": "Backend cleanup. Is run by\n:class:`celery.task.DeleteExpiredTaskMetaTask`."
  },
  {
    "code": "@staticmethod\ndef _search_cases(test_case_path, case_filter=None):\n    with open(test_case_path, 'r') as f:\n        raw_data = yaml.load(f)\n    test_cases = raw_data['test cases']\n    if case_filter:\n        for key in case_filter:\n            filtered_cases = []\n            for case in test_cases:\n                try:\n                    if isinstance(case[key], str):\n                        _value = case[key].lower()\n                    else:\n                        _value = case[key]\n                    if (_value in case_filter[key]):\n                        filtered_cases.append(case)\n                except KeyError:\n                    filtered_cases.append(case)\n            test_cases = filtered_cases\n    return test_cases",
    "nl": "For unit test case, we don't search for test functions.\nThe unit test cases is stored in a yaml file which is created in job build-idf-test.",
    "original_nl": "For unit test case, we don't search for test functions.\nThe unit test cases is stored in a yaml file which is created in job build-idf-test."
  },
  {
    "code": "def init_process(self):\n    if self.cfg.reload:\n\n        def changed(fname):\n            self.log.info('Worker reloading: %s modified', fname)\n            os.kill(self.pid, signal.SIGQUIT)\n            raise SystemExit()\n        Reloader(callback=changed).start()\n    if self.cfg.env:\n        for (k, v) in self.cfg.env.items():\n            os.environ[k] = v\n    util.set_owner_process(self.cfg.uid, self.cfg.gid)\n    util.seed()\n    self.PIPE = os.pipe()\n    for p in self.PIPE:\n        util.set_non_blocking(p)\n        util.close_on_exec(p)\n    [util.close_on_exec(s) for s in self.sockets]\n    util.close_on_exec(self.tmp.fileno())\n    self.log.close_on_exec()\n    self.init_signals()\n    self.wsgi = self.app.wsgi()\n    self.cfg.post_worker_init(self)\n    self.booted = True\n    self.run()",
    "nl": "If you override this method in a subclass, the last statement\nin the function should be to call this method with\nsuper(MyWorkerClass, self).init_process() so that the ``run()``\nloop is initiated.",
    "original_nl": "If you override this method in a subclass, the last statement\nin the function should be to call this method with\nsuper(MyWorkerClass, self).init_process() so that the ``run()``\nloop is initiated."
  },
  {
    "code": "def txp_bin_counter_callback(self, counter):\n    self.log.info('Mcast address %s packets %u', counter.mcast, counter.tx_packets[0])",
    "nl": "Counters callback.",
    "original_nl": "Counters callback."
  },
  {
    "code": "def predict(self, premise: str, hypothesis: str) -> JsonDict:\n    return self.predict_json({\n        'premise': premise,\n        'hypothesis': hypothesis,\n    })",
    "nl": "Predicts whether the hypothesis is entailed by the premise text.\n",
    "original_nl": "Predicts whether the hypothesis is entailed by the premise text.\n\nParameters\n----------\npremise : ``str``\n    A passage representing what is assumed to be true.\n\nhypothesis : ``str``\n    A sentence that may be entailed by the premise.\n\nReturns\n-------\nA dictionary where the key \"label_probs\" determines the probabilities of each of\n[entailment, contradiction, neutral]."
  },
  {
    "code": "def isSameObject(self, obj1, obj2):\n    if (obj1 == obj2):\n        return True\n    elif ((not obj1) or (not obj2)):\n        return False\n    elif ((obj1.name != obj2.name) or (obj1.childCount != obj2.childCount)):\n        return False\n    if (obj1.getRole() == obj2.getRole() == pyatspi.ROLE_LABEL):\n        try:\n            ext1 = obj1.queryComponent().getExtents(0)\n            ext2 = obj2.queryComponent().getExtents(0)\n        except:\n            pass\n        else:\n            if ((ext1.x == ext2.x) and (ext1.y == ext2.y) and (ext1.width == ext2.width) and (ext1.height == ext2.height)):\n                return True\n    try:\n        parent1 = obj1\n        parent2 = obj2\n        while (parent1 and parent2 and (parent1.getRole() == pyatspi.ROLE_LABEL) and (parent2.getRole() == pyatspi.ROLE_LABEL)):\n            if (parent1.getIndexInParent() != parent2.getIndexInParent()):\n                return False\n            parent1 = parent1.parent\n            parent2 = parent2.parent\n        if (parent1 and parent2 and (parent1 == parent2)):\n            return True\n    except:\n        pass\n    return script_utilities.Utilities.isSameObject(self, obj1, obj2)",
    "nl": "Compares two objects to determine if they are functionally\nthe same object. This is needed because some applications and\ntoolkits kill and replace accessibles.",
    "original_nl": "Compares two objects to determine if they are functionally\nthe same object. This is needed because some applications and\ntoolkits kill and replace accessibles."
  },
  {
    "code": "def _GenerateColLoadAggregateStore(emitter, registers, lanes_count, elements_count, aggregators, input_address, stride, output):\n    emitter.EmitNewline()\n    emitter.EmitComment(('Load Aggregate Store - column major %dx%d' % (lanes_count, elements_count)))\n    block = [registers.DoubleRegister() for unused_i in range(lanes_count)]\n    if (elements_count is not 8):\n        _GenerateClear(emitter, 'i8', block)\n    block = emitter.EmitLoadColBlock(registers, 8, lanes_count, elements_count, block, input_address, stride)\n    for (aggregator, row) in zip(aggregators, block):\n        emitter.EmitVAddw('u8', aggregator, aggregator, row)\n    emitter.EmitVStoreAE(8, (8 * lanes_count), block, output, _AlignForLanes(lanes_count))\n    registers.FreeRegisters(block)",
    "nl": "Emit inner loop code for reading N col lanes and interweaving them.",
    "original_nl": "Emit inner loop code for reading N col lanes and interweaving them."
  },
  {
    "code": "def check(**kwargs):\n    jdata = kwargs['jdata']\n    logger = kwargs['logger']\n    env.gateway = jdata['data']['gateway']\n    env.host_string = jdata['data']['host_string']\n    env.user = jdata['data']['username']\n    env.key = jdata['data']['sshkey']\n    env.shell = '/bin/sh -c'\n    env.disable_known_hosts = True\n    env.warn_only = True\n    env.abort_on_prompts = True\n    results = run_cmd('uname -a')\n    if results.succeeded:\n        if ('FreeBSD' in results):\n            cmd = 'vmstat 2 2'\n            results = run_cmd(cmd)\n            if results.succeeded:\n                lines = results.splitlines()\n                vmstat_info = lines[(- 1)].split()\n                cpu_idle = float(vmstat_info[(- 1)])\n            else:\n                return None\n        else:\n            cmd = 'vmstat 2 2'\n            results = run_cmd(cmd)\n            if results.succeeded:\n                lines = results.splitlines()\n                vmstat_info = lines[(- 1)].split()\n                cpu_idle = float(vmstat_info[(- 3)])\n            else:\n                return None\n    else:\n        return None\n    threshold = float(jdata['data']['threshold'])\n    logger.debug('cpu-idle: Idle {0} Threshold {1}'.format(cpu_idle, threshold))\n    if (cpu_idle > threshold):\n        return True\n    else:\n        return False",
    "nl": "Login over SSH and execute shell command",
    "original_nl": "Login over SSH and execute shell command"
  },
  {
    "code": "@skipUnless((_ssh.ssh_version >= (6, 5)), 'ED25519 not available in OpenSSH < 6.5')\ndef test_18(self):\n    with tempfile.NamedTemporaryFile() as tmp:\n        filename = tmp.name\n    obj = CbSSHKeygen()\n    obj.algorithm = 'ed25519'\n    obj.keylength = 256\n    obj.keygen(filename=filename, passphrase='secret')\n    self.assertTrue((os.path.isfile(filename) and os.path.isfile('.'.join((filename, 'pub')))))\n    self.assertEqual(obj.return_code, os.EX_OK)\n    os.unlink(filename)\n    os.unlink('.'.join((filename, 'pub')))",
    "nl": "Test Case 18:\nTry creating a public/private key pair using the ED25519 algorithm.\n\nTest is passed if two files, ``filename`` and ``filename.pub`` are created during the test and the",
    "original_nl": "Test Case 18:\nTry creating a public/private key pair using the ED25519 algorithm.\n\nTest is passed if two files, ``filename`` and ``filename.pub`` are created during the test and the\nreturn value of ssh-keygen is zero."
  },
  {
    "code": "def find_videos_by_userid(self, user_id, orderby='published', page=1, count=20):\n    url = 'https://openapi.youku.com/v2/videos/by_user.json'\n    params = {\n        'client_id': self.client_id,\n        'user_id': user_id,\n        'orderby': orderby,\n        'page': page,\n        'count': count,\n    }\n    r = requests.get(url, params=params)\n    check_error(r)\n    return r.json()",
    "nl": "doc: http://open.youku.com/docs/doc?id=49",
    "original_nl": "doc: http://open.youku.com/docs/doc?id=49"
  },
  {
    "code": "def report_test(cond, filename, test, captured_output, error_contents, delta):\n    time_str = ('%.2f' % (delta,))\n    pydev_runfiles_xml_rpc.notifyTest(cond, captured_output, error_contents, filename, test, time_str)",
    "nl": "@param filename: 'D:\\src\\mod1\\hello.py'\n@param test: 'TestCase.testMet1'\n@param cond: fail, error, ok",
    "original_nl": "@param filename: 'D:\\src\\mod1\\hello.py'\n@param test: 'TestCase.testMet1'\n@param cond: fail, error, ok"
  },
  {
    "code": "def parse(self, html):\n    html = self.__fix_html(html)\n    self.reset()\n    try:\n        self.feed(html)\n        self.close()\n    finally:\n        for tag in self.__tag_stack[1:]:\n            self.__close_tag(tag, True)",
    "nl": "Parses the specified HTML page.",
    "original_nl": "Parses the specified HTML page."
  },
  {
    "code": "def __init__(self, client, db_name):\n    self.client = client\n    self.db_name = db_name",
    "nl": "Creates a new instance of CloudantOnlineStore.\n\n",
    "original_nl": "Creates a new instance of CloudantOnlineStore.\n\n:param Object client: instance of Cloudant client to connect to\n:param str db_name: name of the database to use"
  },
  {
    "code": "def testGetFormatStringAttributeNames(self):\n    event_formatter = windows.WindowsVolumeCreationEventFormatter()\n    expected_attribute_names = ['device_path', 'serial_number', 'origin']\n    self._TestGetFormatStringAttributeNames(event_formatter, expected_attribute_names)",
    "nl": "Tests the GetFormatStringAttributeNames function.",
    "original_nl": "Tests the GetFormatStringAttributeNames function."
  },
  {
    "code": "def op(name, audio, sample_rate, labels=None, max_outputs=3, encoding=None, display_name=None, description=None, collections=None):\n    if (display_name is None):\n        display_name = name\n    if (encoding is None):\n        encoding = 'wav'\n    if (encoding == 'wav'):\n        encoding = metadata.Encoding.Value('WAV')\n        encoder = functools.partial(tf.contrib.ffmpeg.encode_audio, samples_per_second=sample_rate, file_format='wav')\n    else:\n        raise ValueError(('Unknown encoding: %r' % encoding))\n    with tf.name_scope(name), tf.control_dependencies([tf.assert_rank(audio, 3)]):\n        limited_audio = audio[:max_outputs]\n        encoded_audio = tf.map_fn(encoder, limited_audio, dtype=tf.string, name='encode_each_audio')\n        if (labels is None):\n            limited_labels = tf.tile([''], tf.shape(limited_audio)[:1])\n        else:\n            limited_labels = labels[:max_outputs]\n        tensor = tf.transpose(tf.stack([encoded_audio, limited_labels]))\n        summary_metadata = metadata.create_summary_metadata(display_name=display_name, description=description, encoding=encoding)\n        return tf.summary.tensor_summary(name='audio_summary', tensor=tensor, collections=collections, summary_metadata=summary_metadata)",
    "nl": "Create an audio summary op for use in a TensorFlow graph.\n\nArguments:\n  name: A unique name for the generated summary node.\n  audio: A `Tensor` representing audio data with shape `[k, t, c]`,\n    where `k` is the number of audio clips, `t` is the number of\n    frames, and `c` is the number of channels. Elements should be\n    floating-point values in `[-1.0, 1.0]`. Any of the dimensions may\n    be statically unknown (i.e., `None`).\n  sample_rate: An `int` or rank-0 `int32` `Tensor` that represents the\n    sample rate, in Hz. Must be positive.\n  labels: Optional `string` `Tensor`, a vector whose length is the\n    first dimension of `audio`, where `labels[i]` contains arbitrary\n    textual information about `audio[i]`. (For instance, this could be\n    some text that a TTS system was supposed to produce.) Markdown is\n    supported. Contents should be UTF-8.\n  max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n    many audio clips will be emitted at each step. When more than\n    `max_outputs` many clips are provided, the first `max_outputs`\n    many clips will be used and the rest silently discarded.\n  encoding: A constant `str` (not string tensor) indicating the\n    desired encoding. You can choose any format you like, as long as\n    it's \"wav\". Please see the \"API compatibility note\" below.\n  display_name: Optional name for this summary in TensorBoard, as a\n    constant `str`. Defaults to `name`.\n  description: Optional long-form description for this summary, as a\n    constant `str`. Markdown is supported. Defaults to empty.\n  collections: Optional list of graph collections keys. The new\n    summary op is added to these collections. Defaults to\n    `[Graph Keys.SUMMARIES]`.\n",
    "original_nl": "Create an audio summary op for use in a TensorFlow graph.\n\nArguments:\n  name: A unique name for the generated summary node.\n  audio: A `Tensor` representing audio data with shape `[k, t, c]`,\n    where `k` is the number of audio clips, `t` is the number of\n    frames, and `c` is the number of channels. Elements should be\n    floating-point values in `[-1.0, 1.0]`. Any of the dimensions may\n    be statically unknown (i.e., `None`).\n  sample_rate: An `int` or rank-0 `int32` `Tensor` that represents the\n    sample rate, in Hz. Must be positive.\n  labels: Optional `string` `Tensor`, a vector whose length is the\n    first dimension of `audio`, where `labels[i]` contains arbitrary\n    textual information about `audio[i]`. (For instance, this could be\n    some text that a TTS system was supposed to produce.) Markdown is\n    supported. Contents should be UTF-8.\n  max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n    many audio clips will be emitted at each step. When more than\n    `max_outputs` many clips are provided, the first `max_outputs`\n    many clips will be used and the rest silently discarded.\n  encoding: A constant `str` (not string tensor) indicating the\n    desired encoding. You can choose any format you like, as long as\n    it's \"wav\". Please see the \"API compatibility note\" below.\n  display_name: Optional name for this summary in TensorBoard, as a\n    constant `str`. Defaults to `name`.\n  description: Optional long-form description for this summary, as a\n    constant `str`. Markdown is supported. Defaults to empty.\n  collections: Optional list of graph collections keys. The new\n    summary op is added to these collections. Defaults to\n    `[Graph Keys.SUMMARIES]`.\n\nReturns:\n  A TensorFlow summary op.\n\nAPI compatibility note: The default value of the `encoding`\nargument is _not_ guaranteed to remain unchanged across TensorBoard\nversions. In the future, we will by default encode as FLAC instead of\nas WAV. If the specific format is important to you, please provide a\nfile format explicitly."
  },
  {
    "code": "def visit_For(self, node):\n    newnode = node\n    if (node in self.analysis.loopsToBeConverted):\n        newbody = ([ast.Assign(targets=[node.target], value=ast.Yield(value=None))] + node.body)\n        whileNode = ast.While(test=ast.Name(id=self.moreValuesAvailableId, ctx=ast.Load()), body=newbody, orelse=[])\n        newnode = self._tryExceptGeneratorExit([whileNode], [self._moreValuesAvailableAssignmentNode('False')])\n    self.generic_visit(newnode)\n    return ast.copy_location(newnode, node)",
    "nl": "Change iteration into while-yield statements",
    "original_nl": "Change iteration into while-yield statements"
  },
  {
    "code": "def CompileFilter(self, filter_expression):\n    if (not os.path.isfile(filter_expression)):\n        raise errors.WrongPlugin('ObjectFilterList requires an YAML file to be passed on, this filter string is not a file.')\n    yaml.add_constructor('!include', self._IncludeKeyword, Loader=yaml.loader.SafeLoader)\n    results = None\n    with open(filter_expression, 'rb') as file_object:\n        try:\n            results = yaml.safe_load(file_object)\n        except (yaml.scanner.ScannerError, IOError) as exception:\n            raise errors.WrongPlugin('Unable to parse YAML file with error: {0!s}.'.format(exception))\n    self.filters = []\n    results_type = type(results)\n    if (results_type is dict):\n        self._ParseEntry(results)\n    elif (results_type is list):\n        for result in results:\n            if (not isinstance(result, dict)):\n                raise errors.WrongPlugin('Wrong format of YAML file, entry not a dict ({0:s})'.format(results_type))\n            self._ParseEntry(result)\n    else:\n        raise errors.WrongPlugin('Wrong format of YAML file, entry not a dict ({0:s})'.format(results_type))\n    self._filter_expression = filter_expression",
    "nl": "Compiles the filter expression.\n\nThe filter expression contains the name of a YAML file.\n\nArgs:\n  filter_expression: string that contains the filter expression.\n\nRaises:\n  WrongPlugin: if the filter could not be compiled.",
    "original_nl": "Compiles the filter expression.\n\nThe filter expression contains the name of a YAML file.\n\nArgs:\n  filter_expression: string that contains the filter expression.\n\nRaises:\n  WrongPlugin: if the filter could not be compiled."
  },
  {
    "code": "@login_required\ndef delete(request, obj_id=None):\n    data = (request.DELETE or json.loads(request.body))\n    guids = data.get('guids').split(',')\n    objects = getObjectsFromGuids(guids)\n    gallery = Gallery.objects.get(pk=obj_id)\n    LOGGER.info('{} removed {} from {}'.format(request.user.email, guids, gallery))\n    for o in objects:\n        if isinstance(o, Image):\n            gallery.images.remove(o)\n        elif isinstance(o, Video):\n            gallery.videos.remove(o)\n    res = Result()\n    return JsonResponse(res.asDict())",
    "nl": "Removes ImageVideo objects from Gallery",
    "original_nl": "Removes ImageVideo objects from Gallery"
  },
  {
    "code": "def count(self):\n    (resp, page) = self.request('GET', self.uri)\n    return page['total']",
    "nl": "Return the number of instance resources contained in this list resource",
    "original_nl": "Return the number of instance resources contained in this list resource"
  },
  {
    "code": "def test_get_logout_screen(self):\n    url = (reverse('django-pam:logout') + '?next=home-page')\n    response = self.client.get(url)\n    msg = 'response status: {}, should be 302'.format(response.status_code)\n    self.assertEqual(response.status_code, 302, msg)\n    self._login_form()\n    url = (reverse('django-pam:logout') + '?next=home-page')\n    response = self.client.get(url)\n    msg = 'response status: {}, should be 200'.format(response.status_code)\n    self.assertEqual(response.status_code, 200, msg)\n    content = response.content.decode('utf-8')\n    msg = 'content: {}'.format(content)\n    self.assertTrue(('csrfmiddlewaretoken' in content), msg)\n    self.assertTrue(('next' in content), msg)",
    "nl": "Test that the logout screen returns properly.",
    "original_nl": "Test that the logout screen returns properly."
  },
  {
    "code": "def getDistDir(packageDir):\n    if isfile(join(packageDir, 'setup.cfg')):\n        p = configparser.ConfigParser()\n        p.read(join(packageDir, 'setup.cfg'))\n        return normpath(join(packageDir, p.get('bdist_egg', 'dist_dir')))\n    else:\n        return dirname(script)",
    "nl": "Return the distribution directory corresponding to the given package.",
    "original_nl": "Return the distribution directory corresponding to the given package."
  },
  {
    "code": "def print_formatted(query_result):\n    for (cluster, cluster_usage) in query_result['clusters'].items():\n        if ('usage' in cluster_usage):\n            usage_map = cluster_usage['usage']\n            share_map = cluster_usage['share']\n            print_info(colors.bold(cluster))\n            print_info(format_share(share_map))\n            print_info(format_usage(usage_map))\n            applications = cluster_usage['applications']\n            if applications:\n                print_info('Applications:')\n            else:\n                print_info(colors.waiting('Nothing Running'))\n            for (application, application_usage) in applications.items():\n                usage_map = application_usage['usage']\n                print_info(f\"- {colors.running((application if application else '[no application defined]'))}\")\n                print_info(f'  {format_usage(usage_map)}')\n                print_info('  Job Groups:')\n                for (group, group_usage) in application_usage['groups'].items():\n                    usage_map = group_usage['usage']\n                    jobs = group_usage['jobs']\n                    print_info(f\"\t- {colors.bold((group if group else '[ungrouped]'))}\")\n                    print_info(f'\t  {format_usage(usage_map)}')\n                    print_info(f'\t  Jobs: {len(jobs)}')\n                    print_info('')\n            print_info('')",
    "nl": "Prints the query result as a hierarchical set of bullets",
    "original_nl": "Prints the query result as a hierarchical set of bullets"
  },
  {
    "code": "@abstractmethod\ndef create_margin(self, window_render_info, width, height):\n    return []",
    "nl": "Creates a margin.\nThis should return a list of (style_str, text) tuples.\n\n",
    "original_nl": "Creates a margin.\nThis should return a list of (style_str, text) tuples.\n\n:param window_render_info:\n    :class:`~prompt_toolkit.layout.containers.WindowRenderInfo`\n    instance, generated after rendering and copying the visible part of\n    the :class:`~prompt_toolkit.layout.controls.UIControl` into the\n    :class:`~prompt_toolkit.layout.containers.Window`.\n:param width: The width that's available for this margin. (As reported\n    by :meth:`.get_width`.)\n:param height: The height that's available for this margin. (The height\n    of the :class:`~prompt_toolkit.layout.containers.Window`.)"
  },
  {
    "code": "def post_save_update_cache(sender, instance, created, raw, **kwargs):\n    if raw:\n        return\n    name = sender.__name__\n    if ((name == 'User') and created):\n        return\n    delay_cache = getattr(instance, '_delay_cache', False)\n    if (not delay_cache):\n        update_cache_for_instance(name, instance.pk, instance)",
    "nl": "Invalidate the cache when an instance is created or updated.",
    "original_nl": "Invalidate the cache when an instance is created or updated."
  },
  {
    "code": "def get_addresses(self):\n    return self.http_get('/wallet/addresses').get('addresses')",
    "nl": "Returns a list of addresses from the wallet.",
    "original_nl": "Returns a list of addresses from the wallet."
  },
  {
    "code": "def test_filter_assign():\n\n    class TestFilter(Filter):\n        pass\n\n    def _assert(list, length):\n        'Confirm that everything in the list is a filter instance, and\\n        that the list as the required length.'\n        assert (len(list) == length)\n        assert bool([f for f in list if isinstance(f, Filter)])\n    b = Bundle(filters='jsmin,cssutils')\n    _assert(b.filters, 2)\n    b = Bundle(filters=['jsmin', 'cssutils'])\n    _assert(b.filters, 2)\n    assert_raises(ValueError, Bundle, filters=['jsmin,cssutils'])\n    b = Bundle(filters=TestFilter)\n    _assert(b.filters, 1)\n    b = Bundle(filters=[TestFilter, TestFilter, TestFilter])\n    _assert(b.filters, 3)\n    b = Bundle(filters=TestFilter())\n    _assert(b.filters, 1)\n    b = Bundle(filters=[TestFilter(), TestFilter(), TestFilter()])\n    _assert(b.filters, 3)\n    b = Bundle(filters=[TestFilter, TestFilter()])\n    _assert(b.filters, 2)\n    assert_raises(ValueError, Bundle, filters='notreallyafilter')\n    assert_raises(ValueError, Bundle, filters=object())\n    Bundle().filters = None\n    b = Bundle()\n    assert (b.filters is None)\n    b.filters = TestFilter\n    _assert(b.filters, 1)\n    old_filters = b.filters\n    b.filters = b.filters\n    assert (b.filters == old_filters)",
    "nl": "Test the different ways we can assign filters to the bundle.",
    "original_nl": "Test the different ways we can assign filters to the bundle."
  },
  {
    "code": "def wait_until(pred, timeout=30, interval=5):\n    if timeout:\n        finish = (datetime.now() + timedelta(seconds=timeout))\n    else:\n        finish = None\n    while True:\n        result = pred()\n        if (result or quit_running):\n            break\n        if (finish and (datetime.now() >= finish)):\n            break\n        time.sleep(interval)\n    return result",
    "nl": "Wait, retrying a predicate until it is True, or the \ntimeout value has been exceeded.",
    "original_nl": "Wait, retrying a predicate until it is True, or the \ntimeout value has been exceeded."
  },
  {
    "code": "def martinis_tree(domain, b):\n    clouds = b['pBLUE'].gte(0.27)\n    temp1 = b['EVI'].lte(0.3).And(b['DVEL'].lte(0.05))\n    temp2 = b['EVI'].lte(0.05).And(b['LSWI'].lte(0.0))\n    nonFlood1 = b['EVI'].gt(0.3).And(temp1.Not())\n    waterRelated1 = temp1.Or(temp2)\n    nonFlood2 = temp1.And(temp2.Not())\n    demSensor = domain.get_dem()\n    dem = demSensor.image\n    demSlopeDegrees = compute_dem_slope_degrees(demSensor.image, demSensor.band_resolutions[demSensor.band_names[0]])\n    highSlope = demSlopeDegrees.gt(10).Or(demSlopeDegrees.gt(8).And(dem.gt(2000)))\n    waterRelated2 = waterRelated1.And(highSlope.Not())\n    nonFlood3 = waterRelated1.And(highSlope).Or(nonFlood2)\n    mixture1 = waterRelated2.And(b['EVI'].gt(0.1))\n    flood1 = waterRelated2.And(b['EVI'].lte(0.1))\n    REGION_GROW_SIZE = 35\n    expansionKernel = ee.Kernel.circle(REGION_GROW_SIZE, 'pixels', False)\n    temp1 = b['EVI'].lte(0.31).And(b['DVEL'].lte(0.07))\n    temp2 = b['EVI'].lte(0.06).And(b['LSWI'].lte(0.01))\n    relaxedConditions = temp1.Or(temp2)\n    potentialExpansionArea = waterRelated2.convolve(expansionKernel).And(nonFlood3)\n    waterRelated3 = potentialExpansionArea.And(relaxedConditions)\n    mixture3 = waterRelated3.And(b['EVI'].gt(0.1))\n    flood3 = waterRelated3.And(b['EVI'].lte(0.1))\n    potentialExpansionArea = flood1.convolve(expansionKernel).And(mixture1)\n    mixture2 = potentialExpansionArea.And(b['LSWI'].lt(0.08))\n    flood2 = potentialExpansionArea.And(b['LSWI'].gte(0.08))\n    waterMask = ee.Image('MODIS/MOD44W/MOD44W_005_2000_02_24').select(['water_mask'])\n    recedingWater = nonFlood3.And(waterMask)\n    mergedFlood = flood1.Or(flood2).Or(flood3)\n    standingWater = mergedFlood.And(waterMask)\n    flood4 = mergedFlood.And(waterMask.Not())\n    badModisPixels = getModisBadPixelMask(domain.modis.image)\n    flood5 = flood4.And(badModisPixels.Not())\n    fullMixture = mixture2.Or(mixture3)\n    outputFlood = flood5.Or(standingWater)\n    return outputFlood.select(['sur_refl_b02'], ['b1'])",
    "nl": "Based on Figure 3 from \"A Multi-Scale Flood Monitoring System Based on Fully\n Automatic MODIS and TerraSAR-X Processing Chains\" by \n Sandro Martinis, Andre Twele, Christian Strobl, Jens Kersten and Enrico Stein\n \nSome steps had to be approximated such as the cloud filtering.  The main pixel\nclassification steps are implemented as accurately as can be determined from \nthe figure.",
    "original_nl": "Based on Figure 3 from \"A Multi-Scale Flood Monitoring System Based on Fully\n Automatic MODIS and TerraSAR-X Processing Chains\" by \n Sandro Martinis, Andre Twele, Christian Strobl, Jens Kersten and Enrico Stein\n \nSome steps had to be approximated such as the cloud filtering.  The main pixel\nclassification steps are implemented as accurately as can be determined from \nthe figure."
  },
  {
    "code": "def North_East_vectors_target(ra_xallarap, dec_xallarap):\n    target_angles_in_the_sky = [((ra_xallarap * np.pi) / 180), ((dec_xallarap * np.pi) / 180)]\n    source_center_of_mass = np.array([(np.cos(target_angles_in_the_sky[1]) * np.cos(target_angles_in_the_sky[0])), (np.cos(target_angles_in_the_sky[1]) * np.sin(target_angles_in_the_sky[0])), np.sin(target_angles_in_the_sky[1])])\n    East = np.array([(- np.sin(target_angles_in_the_sky[0])), np.cos(target_angles_in_the_sky[0]), 0.0])\n    North = np.cross(source_center_of_mass, East)\n    return (North, East)",
    "nl": "This function define the North and East vectors projected on the sky plane\nperpendicular to the line\nof sight (i.e the line define by ra,dec of the center of mass of the source binary).\n\n",
    "original_nl": "This function define the North and East vectors projected on the sky plane\nperpendicular to the line\nof sight (i.e the line define by ra,dec of the center of mass of the source binary).\n\n:param float ra_xallarap : the right acsension of the source binary center of mass in degree.\n:param float dec_xallarap : the declinaision of the source binary center of mass in degree.\n:return: the North and East vectors projected in the sky plan for this trajectory\n:rtype: array_like, array_like"
  },
  {
    "code": "def settings_add_bool(*args, **kwargs):\n    get_script().settings_add_bool(*args, **kwargs)",
    "nl": "see Script.settings_add_bool()",
    "original_nl": "see Script.settings_add_bool()"
  },
  {
    "code": "def test_get_licenses():\n    expected_matches = [('mit', 'mit'), ('mti', 'mit'), ('MIT', 'mit'), ('bit', 'mit'), ('nit', 'mit'), ('ics', 'isc'), ('isc', 'isc'), ('allrightsreserved', 'all-rights-reserved'), ('gpl2', 'gpl-2.0'), ('gpl-2', 'gpl-2.0'), ('apache', 'apache-2.0'), ('apache-2', 'apache-2.0'), ('mpl-2', 'mpl-2.0'), ('bsd2clause', 'bsd-2-clause'), ('agpl3', 'agpl-3.0'), ('agpl', 'agpl-3.0')]\n    for pair in expected_matches:\n        match = licenser.get_license(pair[0])\n        assert (match == pair[1]), '{a} != {b}, test: {pair}'.format(a=match, b=pair[1], pair=pair)",
    "nl": "calls get_license to test matching functionality",
    "original_nl": "calls get_license to test matching functionality"
  },
  {
    "code": "def resident(since=0):\n    return (_VmB('VmRSS:') - since)",
    "nl": "Return resident memory usage in kilobytes.",
    "original_nl": "Return resident memory usage in kilobytes."
  },
  {
    "code": "def reset_status(self, cluster):\n    body = {\n        'reset-status': {\n            \n        },\n    }\n    self._action(cluster, body)",
    "nl": "Reset the status of a cluster\n\n",
    "original_nl": "Reset the status of a cluster\n\n:param cluster: The cluster to reset"
  },
  {
    "code": "def readSubjectivityClues(postag, datafile=None):\n    A = {\n        \n    }\n    if (datafile == None):\n        return None\n    if (postag == 'a'):\n        posname = 'adj'\n    elif (postag == 'v'):\n        posname = 'verb'\n    elif (postag == 'n'):\n        posname = 'noun'\n    else:\n        posname = 'anypos'\n    Wfile = open(datafile, 'r', 1024000)\n    for line in Wfile:\n        entry = line.split()\n        posval = 0\n        negval = 0\n        pos_type = entry[3].split('=')[1]\n        polarity = entry[5].split('=')[1]\n        term = entry[2].split('=')[1]\n        if (polarity == 'negative'):\n            posval = 0\n            negval = 1\n        elif (polarity == 'positive'):\n            posval = 1\n            negval = 0\n        if (((pos_type == posname) or (pos_type == 'anypos')) and (polarity != 'neutral')):\n            if (term not in A):\n                A[term] = []\n            A[term].append((term, posval, negval))\n    return A",
    "nl": "Reads Wiebe's subjectivity clues into a dictionary\n\nTypical line read from file:\ntype=weaksubj len=1 word1=wrestle pos1=verb stemmed1=y priorpolarity=negative",
    "original_nl": "Reads Wiebe's subjectivity clues into a dictionary\n\nTypical line read from file:\ntype=weaksubj len=1 word1=wrestle pos1=verb stemmed1=y priorpolarity=negative"
  },
  {
    "code": "def cross_validation(model, horizon, period=None, initial=None):\n    te = model.history['ds'].max()\n    ts = model.history['ds'].min()\n    horizon = pd.Timedelta(horizon)\n    period = ((0.5 * horizon) if (period is None) else pd.Timedelta(period))\n    initial = ((3 * horizon) if (initial is None) else pd.Timedelta(initial))\n    k = int(np.ceil((((te - horizon) - (ts + initial)) / period)))\n    if (k < 1):\n        raise ValueError('Not enough data for specified horizon, period, and initial.')\n    return simulated_historical_forecasts(model, horizon, k, period)",
    "nl": "Cross-Validation for time series.\n\nComputes forecasts from historical cutoff points. Beginning from initial,\nmakes cutoffs with a spacing of period up to (end - horizon).\n\nWhen period is equal to the time interval of the data, this is the\ntechnique described in https://robjhyndman.com/hyndsight/tscv/ .\n",
    "original_nl": "Cross-Validation for time series.\n\nComputes forecasts from historical cutoff points. Beginning from initial,\nmakes cutoffs with a spacing of period up to (end - horizon).\n\nWhen period is equal to the time interval of the data, this is the\ntechnique described in https://robjhyndman.com/hyndsight/tscv/ .\n\nParameters\n----------\nmodel: Prophet class object. Fitted Prophet model\nhorizon: string with pd.Timedelta compatible style, e.g., '5 days',\n    '3 hours', '10 seconds'.\nperiod: string with pd.Timedelta compatible style. Simulated forecast will\n    be done at every this period. If not provided, 0.5 * horizon is used.\ninitial: string with pd.Timedelta compatible style. The first training\n    period will begin here. If not provided, 3 * horizon is used.\n\nReturns\n-------\nA pd.DataFrame with the forecast, actual value and cutoff."
  },
  {
    "code": "def _testXDailyDownload(self):\n    self.data_dir = 'c:/temp/_daily_test_data/'\n    catalog = IceCat.IceCatCatalog(log=self.log, data_dir=self.data_dir, auth=self.auth)\n    detail_keys = ['ProductDescription[@LongDesc]', 'ShortSummaryDescription', 'LongSummaryDescription', 'ProductDescription[@ShortDesc]']\n    catalog.add_product_details_parallel(keys=detail_keys, connections=100)\n    file = 'test.large.json'\n    if os.path.exists(file):\n        os.remove(file)\n    catalog.dump_to_file(file)\n    self.assertEqual(os.path.isfile(file), True)",
    "nl": "load all data files from Ice Cat. \nthis test check that data is parsed correctly to the dictionary\nproduct details are tested. parallel download tested\nit's normal for this test to run long, several minutes",
    "original_nl": "load all data files from Ice Cat. \nthis test check that data is parsed correctly to the dictionary\nproduct details are tested. parallel download tested\nit's normal for this test to run long, several minutes"
  },
  {
    "code": "def Main():\n    argument_parser = argparse.ArgumentParser(description='Extracts information from timezone information files.')\n    argument_parser.add_argument('-d', '--debug', dest='debug', action='store_true', default=False, help='enable debug output.')\n    argument_parser.add_argument('source', nargs='?', action='store', metavar='PATH', default=None, help='path of the timezone information file.')\n    options = argument_parser.parse_args()\n    if (not options.source):\n        print('Source file missing.')\n        print('')\n        argument_parser.print_help()\n        print('')\n        return False\n    logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n    output_writer = output_writers.StdoutWriter()\n    try:\n        output_writer.Open()\n    except IOError as exception:\n        print('Unable to open output writer with error: {0!s}'.format(exception))\n        print('')\n        return False\n    tzif_file = tzif.TimeZoneInformationFile(debug=options.debug, output_writer=output_writer)\n    tzif_file.Open(options.source)\n    output_writer.WriteText('Timezone information:\\n')\n    tzif_file.Close()\n    output_writer.Close()\n    return True",
    "nl": "The main program function.\n",
    "original_nl": "The main program function.\n\nReturns:\n  bool: True if successful or False if not."
  },
  {
    "code": "def init(param_test):\n    default_args = ['-i t2/t2_seg_manual.nii.gz -d t2/t2_seg_manual.nii.gz']\n    param_test.contrast = 't2'\n    if (not param_test.args):\n        param_test.args = default_args\n    return param_test",
    "nl": "Initialize class: param_test",
    "original_nl": "Initialize class: param_test"
  },
  {
    "code": "def defuse_xml_libs():\n    from defusedxml import defuse_stdlib\n    defuse_stdlib()\n    import lxml\n    import lxml.etree\n    from . import etree as safe_etree\n    lxml.etree = safe_etree",
    "nl": "Monkey patch and defuse all stdlib xml packages and lxml.",
    "original_nl": "Monkey patch and defuse all stdlib xml packages and lxml."
  },
  {
    "code": "def getTempF(self):\n    tempC = self.getTemp()\n    return (((tempC * 9.0) / 5) + 32)",
    "nl": "Reads and returns the current ambient temperature in fahrenheit\n\nReceived value is checked against a CRC and an AssertionError is thrown if \nit is invalid.",
    "original_nl": "Reads and returns the current ambient temperature in fahrenheit\n\nReceived value is checked against a CRC and an AssertionError is thrown if \nit is invalid."
  },
  {
    "code": "def cmd_status(args):\n    opts = {\n        'bynode': '-n',\n        'inactive': '-r',\n        'ops': '-o',\n        'timing': '-t',\n        'failcounts': '-f',\n        'verbose': '-V',\n        'quiet': '-Q',\n        'html': '--as-html',\n        'xml': '--as-xml',\n        'simple': '-s',\n        'tickets': '-c',\n        'noheaders': '-D',\n        'detail': '-R',\n        'brief': '-b',\n        'full': '-ncrft',\n    }\n    extra = ' '.join((opts.get(arg, arg) for arg in args))\n    if (not args):\n        extra = '-r'\n    (rc, s) = crm_mon(extra)\n    if (rc != 0):\n        raise IOError(('crm_mon (rc=%d): %s' % (rc, s)))\n    utils.page_string(CrmMonFilter()(s))\n    return True",
    "nl": "Calls crm_mon -1, passing optional extra arguments.\nDisplays the output, paging if necessary.\nRaises IOError if crm_mon fails.",
    "original_nl": "Calls crm_mon -1, passing optional extra arguments.\nDisplays the output, paging if necessary.\nRaises IOError if crm_mon fails."
  },
  {
    "code": "def handle_sigint(self, signum, frame):\n    self.finish()\n    self.original_handler(signum, frame)",
    "nl": "Call self.finish() before delegating to the original SIGINT handler.\n\nThis handler should only be in place while the progress display is\nactive.",
    "original_nl": "Call self.finish() before delegating to the original SIGINT handler.\n\nThis handler should only be in place while the progress display is\nactive."
  },
  {
    "code": "def test_optionsInvalidArguments(self):\n    self.patchExit()\n    Twist.options(['twist', '--bogus-bagels'])\n    self.assertIdentical(self.exit.status, ExitStatus.EX_USAGE)\n    self.assertTrue(self.exit.message.startswith('Error: '))\n    self.assertTrue(self.exit.message.endswith('\\n\\n{}'.format(TwistOptions())))",
    "nl": "L{Twist.options} given invalid arguments exits with\nL{ExitStatus.EX_USAGE} and an error/usage message.",
    "original_nl": "L{Twist.options} given invalid arguments exits with\nL{ExitStatus.EX_USAGE} and an error/usage message."
  },
  {
    "code": "def test_success_test_method(self):\n    self._run_test_class(testcases.SuccessfulTestCase, 'successful_test_case')",
    "nl": "Check the XML output of a test class with a successful test method.",
    "original_nl": "Check the XML output of a test class with a successful test method."
  },
  {
    "code": "@sigint.setter\ndef sigint(self, handler):\n    self._sigint = _normalize_handler(handler, self._default_handler)",
    "nl": "Normalizes and sets sigint.",
    "original_nl": "Normalizes and sets sigint."
  },
  {
    "code": "def test_init_seed_bad_genome():\n    np.random.seed(4303423)\n    seed_genome = np.random.randint(0, 256, 10000)\n    seed_genome[0:2] = np.array([42, 213])\n    seed_genome[(- 10):(- 8)] = np.array([42, 213])\n    test_mn = MarkovNetwork(num_input_states=4, num_memory_states=5, num_output_states=6, probabilistic=False, genome=seed_genome)\n    assert np.all((test_mn.genome == seed_genome))\n    assert (len(test_mn.markov_gates) == 1)",
    "nl": "MarkovNetwork initializer with bad seeded genome",
    "original_nl": "MarkovNetwork initializer with bad seeded genome"
  },
  {
    "code": "def check_status_code(response, verbose):\n    if ((response.status_code == 410) and response.url.startswith(('https://pypi.python.org', 'https://testpypi.python.org'))):\n        print(\"It appears you're uploading to pypi.python.org (or testpypi.python.org). You've received a 410 error response. Uploading to those sites is deprecated. The new sites are pypi.org and test.pypi.org. Try using https://upload.pypi.org/legacy/ (or https://test.pypi.org/legacy/) to upload your packages instead. These are the default URLs for Twine now. More at https://packaging.python.org/guides/migrating-to-pypi-org/ \")\n    try:\n        response.raise_for_status()\n    except HTTPError as err:\n        if response.text:\n            if verbose:\n                print('Content received from server:\\n{}'.format(response.text))\n            else:\n                print('NOTE: Try --verbose to see response content.')\n        raise err",
    "nl": "Shouldn't happen, thanks to the UploadToDeprecatedPyPIDetected\nexception, but this is in case that breaks and it does.",
    "original_nl": "Shouldn't happen, thanks to the UploadToDeprecatedPyPIDetected\nexception, but this is in case that breaks and it does."
  },
  {
    "code": "def get_nicklist(self):\n    if (not self.nicklist):\n        self.sendLine(('NAMES %s' % self.channel))",
    "nl": "Retrieve name list from the channel. The return\nis handled by the catch methods below.",
    "original_nl": "Retrieve name list from the channel. The return\nis handled by the catch methods below."
  },
  {
    "code": "def tags(self):\n    for item in self._prop_list:\n        (yield Tag(item))",
    "nl": "Get a generator of Tag within the TagsCollectionPage\n\nYields:\n    :class:`Tag<onedrivesdk.model.tag.Tag>`:\n        The next Tag in the collection",
    "original_nl": "Get a generator of Tag within the TagsCollectionPage\n\nYields:\n    :class:`Tag<onedrivesdk.model.tag.Tag>`:\n        The next Tag in the collection"
  },
  {
    "code": "def test_discover_depth_treantdepth(tmpdir):\n    with tmpdir.as_cwd():\n        ghosts = ('inky', 'inky/blinky', 'pinky', 'inky/blinky/nothing/clyde')\n        for name in ghosts:\n            dtr.Treant(name)\n        assert (len(discover('.', treantdepth=0, depth=0)) == 0)\n        assert (len(discover('.', treantdepth=0, depth=1)) == 2)\n        assert (len(discover('pinky', treantdepth=0, depth=0)) == 1)\n        assert (len(discover('inky', treantdepth=0, depth=2)) == 1)\n        assert (len(discover('.', treantdepth=1, depth=1)) == 2)\n        assert (len(discover('inky', treantdepth=1, depth=1)) == 2)\n        assert (len(discover('inky', treantdepth=1, depth=0)) == 1)\n        assert (len(discover('inky', treantdepth=2)) == 3)\n        assert (len(discover('inky', treantdepth=2, depth=2)) == 2)\n        assert (len(discover('inky', treantdepth=2, depth=3)) == 3)",
    "nl": "Check that using `treantdepth` and `depth` parameters together gives\nexpected result.",
    "original_nl": "Check that using `treantdepth` and `depth` parameters together gives\nexpected result."
  },
  {
    "code": "def useNotifyByWriteFile(fileObj: TextIO=None, prefix: str=None, publisher: Publisher=None, all: bool=True, **kwargs):\n    notifHandler = NotifyByWriteFile(fileObj, prefix)\n    if (publisher is None):\n        from .. import pub\n        publisher = pub.getDefaultPublisher()\n    publisher.addNotificationHandler(notifHandler)\n    publisher.setNotificationFlags(all=all, **kwargs)",
    "nl": "Will cause all pubsub notifications of pubsub \"actions\" (such as new topic created, message sent, listener died\netc) to be written to specified file (or stdout if none given). The fileObj need only provide a 'write(string)'\nmethod.\n\nThe first two arguments are the same as those of NotifyByWriteFile constructor. The 'all' and kwargs arguments\nare those of pubsub's setNotificationFlags(), except that 'all' defaults to True.  See useNotifyByPubsubMessage()\nfor an explanation of pubModule (typically only if pubsub inside wxPython's wx.lib)",
    "original_nl": "Will cause all pubsub notifications of pubsub \"actions\" (such as new topic created, message sent, listener died\netc) to be written to specified file (or stdout if none given). The fileObj need only provide a 'write(string)'\nmethod.\n\nThe first two arguments are the same as those of NotifyByWriteFile constructor. The 'all' and kwargs arguments\nare those of pubsub's setNotificationFlags(), except that 'all' defaults to True.  See useNotifyByPubsubMessage()\nfor an explanation of pubModule (typically only if pubsub inside wxPython's wx.lib)"
  },
  {
    "code": "def bin_search(binary):\n    result = None\n    mode = (os.R_OK | os.X_OK)\n    for p in bin_search_path:\n        path = os.path.join(p, binary)\n        if (os.access(path, mode) == 1):\n            result = path\n            break\n    else:\n        raise MissingBinary(('Unable to find binary \"%s\"' % binary))\n    return result",
    "nl": "search the bin_search_path  for a given binary",
    "original_nl": "search the bin_search_path  for a given binary\nreturning its fullname or None"
  },
  {
    "code": "def load_blood_data(train=True, SEED=97, scale=False, minmax=False, norm=False, nointercept=False, engineering=False):\n    from sklearn.utils import shuffle\n    from patsy import dmatrices, dmatrix\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import Normalizer\n    import numpy as np\n    import pandas as pd\n    import re\n    global scaler\n    global minmaxer\n    global normalizer\n    if (scale and minmax):\n        raise ValueError('cannot specify both scale and minmax')\n    if (scale and norm):\n        raise ValueError('cannot specify both scale and norm')\n    if (norm and minmax):\n        raise ValueError('cannot specify both norm and minmax')\n    if (type(train) is not bool):\n        raise ValueError('train must be boolean')\n    if (type(SEED) is not int):\n        raise ValueError('SEED must be int')\n    if (type(scale) is not bool):\n        raise ValueError('scale must be boolean')\n    if (type(norm) is not bool):\n        raise ValueError('norm must be boolean')\n    if (type(nointercept) is not bool):\n        raise ValueError('nointercept must be boolean')\n    if (type(engineering) is not bool):\n        raise ValueError('engineering must be boolean')\n    file_name = ('../input/train.csv' if train else '../input/test.csv')\n    data = pd.read_csv(file_name)\n    column_names = ['ID', 'moSinceLast', 'numDonations', 'volume', 'moSinceFirst', 'donated']\n    data.columns = (column_names if train else column_names[:(- 1)])\n    if engineering:\n        data['moRatio'] = pd.Series((data.moSinceLast / data.moSinceFirst), index=data.index)\n        data['avgDonation'] = pd.Series(((data.volume / data.numDonations) / data.moSinceFirst), index=data.index)\n        data['avgWait'] = pd.Series((data.moSinceFirst / data.numDonations), index=data.index)\n    if scale:\n        if train:\n            scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n            exclude = ['ID', 'donated']\n            data.ix[:, data.columns.difference(exclude)] = scaler.fit_transform(data.ix[:, data.columns.difference(exclude)].values.astype(np.float32))\n        else:\n            exclude = ['ID', 'donated']\n            data.ix[:, data.columns.difference(exclude)] = scaler.transform(data.ix[:, data.columns.difference(exclude)].values.astype(np.float32))\n    if minmax:\n        if (len(minmax) != 2):\n            raise ValueError('minmax must be a 2-tuple')\n        if train:\n            minmaxer = MinMaxScaler(feature_range=minmax)\n            exclude = ['ID', 'donated']\n            data.ix[:, data.columns.difference(exclude)] = minmaxer.fit_transform(data.ix[:, data.columns.difference(exclude)].values.astype(np.float32))\n        else:\n            exclude = ['ID', 'donated']\n            data.ix[:, data.columns.difference(exclude)] = minmaxer.transform(data.ix[:, data.columns.difference(exclude)].values.astype(np.float32))\n    if norm:\n        if train:\n            normalizer = Normalizer(norm='l2', copy=True)\n            exclude = ['ID', 'donated']\n            data.ix[:, data.columns.difference(exclude)] = normalizer.fit_transform(data.ix[:, data.columns.difference(exclude)].values.astype(np.float32))\n        else:\n            exclude = ['ID', 'donated']\n            data.ix[:, data.columns.difference(exclude)] = normalizer.transform(data.ix[:, data.columns.difference(exclude)].values.astype(np.float32))\n    formula = 'donated ~ moSinceLast * moSinceFirst +  numDonations + volume'\n    if engineering:\n        formula = (formula + ' + moRatio + avgDonation + avgWait')\n    if nointercept:\n        formula = (formula + ' -1')\n    if (not train):\n        match = re.search('~\\\\s??(.*)', formula)\n        if match:\n            formula = match.group(1)\n        else:\n            raise ValueError('Patsy formula {} does not match the expected format'.format(formula))\n    if train:\n        (y_train, X_train) = dmatrices(formula, data=data, return_type='dataframe')\n        y_train = np.ravel(y_train).astype(np.int32)\n        (X_train, y_train) = shuffle(X_train, y_train, random_state=SEED)\n        return (y_train, X_train)\n    else:\n        X_test = dmatrix(formula, data=data, return_type='dataframe')\n        IDs = data.ID.values\n        return (X_test, IDs)",
    "nl": "Load training and test datasets\nfor DrivenData's Predict Blood Donations warmup contest\n\nThe training data is shuffled before it's returned; test data is not\n\nNote: patsy returns float64 data; Theano requires float32 so conversion\n      will be required; the y values are converted to int32, so they're OK\n\nArguments\n---------\n    train (bool) if True\n                     y_train, X_train = load_blood_data(train=True, ...\n                 if False\n                     X_test, IDs = load_blood_data(train=False, ...\n                     \n    SEED (int)   random seed\n    \n    scale (bool) if True, scale the data to mean zero, var 1; standard normal\n    \n    minmax (2-tuple) to scale the data to a specified range, provide a\n                     2-tuple (min, max)\n                     \n    norm (bool)  if True, L2 normalize for distance and similarity measures\n    \n    nointercept (bool) if True, patsy will not create an intercept\n                     \n                     \nUsage\n-----\nfrom load_blood_data import load_blood_data",
    "original_nl": "Load training and test datasets\nfor DrivenData's Predict Blood Donations warmup contest\n\nThe training data is shuffled before it's returned; test data is not\n\nNote: patsy returns float64 data; Theano requires float32 so conversion\n      will be required; the y values are converted to int32, so they're OK\n\nArguments\n---------\n    train (bool) if True\n                     y_train, X_train = load_blood_data(train=True, ...\n                 if False\n                     X_test, IDs = load_blood_data(train=False, ...\n                     \n    SEED (int)   random seed\n    \n    scale (bool) if True, scale the data to mean zero, var 1; standard normal\n    \n    minmax (2-tuple) to scale the data to a specified range, provide a\n                     2-tuple (min, max)\n                     \n    norm (bool)  if True, L2 normalize for distance and similarity measures\n    \n    nointercept (bool) if True, patsy will not create an intercept\n                     \n                     \nUsage\n-----\nfrom load_blood_data import load_blood_data"
  },
  {
    "code": "@shared_test_lib.skipUnlessHasTestFile(['\u00edmynd.dd'])\ndef testMatches(self):\n    test_path = self._GetTestFilePath(['\u00edmynd.dd'])\n    os_path_spec = path_spec_factory.Factory.NewPathSpec(dfvfs_definitions.TYPE_INDICATOR_OS, location=test_path)\n    test_filter = file_entry_filters.NamesFileEntryFilter(['passwords.txt'])\n    tsk_path_spec = path_spec_factory.Factory.NewPathSpec(dfvfs_definitions.TYPE_INDICATOR_TSK, inode=16, location='/a_directory/another_file', parent=os_path_spec)\n    file_entry = path_spec_resolver.Resolver.OpenFileEntry(tsk_path_spec)\n    self.assertFalse(test_filter.Matches(file_entry))\n    tsk_path_spec = path_spec_factory.Factory.NewPathSpec(dfvfs_definitions.TYPE_INDICATOR_TSK, inode=12, location='/a_directory', parent=os_path_spec)\n    file_entry = path_spec_resolver.Resolver.OpenFileEntry(tsk_path_spec)\n    self.assertFalse(test_filter.Matches(file_entry))\n    tsk_path_spec = path_spec_factory.Factory.NewPathSpec(dfvfs_definitions.TYPE_INDICATOR_TSK, inode=15, location='/passwords.txt', parent=os_path_spec)\n    file_entry = path_spec_resolver.Resolver.OpenFileEntry(tsk_path_spec)\n    self.assertTrue(test_filter.Matches(file_entry))\n    test_filter = file_entry_filters.NamesFileEntryFilter([])\n    self.assertFalse(test_filter.Matches(file_entry))",
    "nl": "Tests the Matches function.",
    "original_nl": "Tests the Matches function."
  },
  {
    "code": "def test_get_by_region_name(self):\n    for reg in list(RegionCode.Region):\n        reg_str = reg.name.replace('_', '-')\n        self.assertEquals(self.REGIONS[reg], RegionCode[reg_str])",
    "nl": "Code can be correctly retrieved from Region string with dashes using RegionCode.",
    "original_nl": "Code can be correctly retrieved from Region string with dashes using RegionCode."
  },
  {
    "code": "def read_history(self, num=10, segment=0):\n    if (num < 0):\n        num = 0\n    return self._builder.read_history(num, segment)",
    "nl": "Outputs the last `num` rows that were appended either by `append` or\n`append_multiple`.\n",
    "original_nl": "Outputs the last `num` rows that were appended either by `append` or\n`append_multiple`.\n\nReturns\n-------\nout : list[list]"
  },
  {
    "code": "def run_command(self, command, timeout=(- 1)):\n    cmd_lines = [l for l in command.splitlines() if (l and (not l.startswith('//')))]\n    cmd = re.sub('\\\\s{2,}', ' ', ' '.join(cmd_lines))\n    logger.debug('Command length: {} chars'.format(len(cmd)))\n    logger.debug('Command: {}'.format(cmd))\n    if (len(cmd) > 1024):\n        error = 'Code too long. Please commands with less than 1024 effective chracters.\\nIndentation spaces/tabs don\\'t count towards \"effective\" characters.'\n        logger.error(error)\n        raise ValueError(error.replace('\\n', ' '))\n    self._send_line(cmd)\n    match = self._expect_prompt(timeout=timeout)\n    logger.debug('Prompt type: {}'.format(match))\n    logger.debug('Iterating over message')\n    response = []\n    while (not self._isbufferempty()):\n        response.append(self.child.before)\n        logger.debug('Buffer not empty, sending blank line')\n        match = self._expect_prompt(timeout=timeout)\n        if (match == 1):\n            error = 'Code incomplete. Please enter valid and complete code.\\nContinuation prompt functionality not implemented yet.'\n            logger.error(error.replace('\\n', ' '))\n            raise ValueError(error)\n        self._send_line('')\n    response.append(self.child.before)\n    response = self._filter_response(''.join(response))\n    logger.debug('Response: {}'.format(response))\n    return response",
    "nl": "Send a command to the REPL, wait for and return output.\n\n",
    "original_nl": "Send a command to the REPL, wait for and return output.\n\n:param str command: The command to send. Trailing newlines are not needed.\n  This should be a complete block of input that will trigger execution;\n  if a continuation prompt is found after sending input, :exc:`ValueError`\n  will be raised.\n:param int timeout: How long to wait for the next prompt. -1 means the\n  default from the :class:`pexpect.spawn` object (default 30 seconds).\n  None means to wait indefinitely."
  },
  {
    "code": "@classmethod\ndef is_collected_outdated(cls, block_structure):\n    outdated_transformers = []\n    for transformer in TransformerRegistry.get_registered_transformers():\n        version_in_block_structure = block_structure._get_transformer_data_version(transformer)\n        if (transformer.VERSION != version_in_block_structure):\n            outdated_transformers.append(transformer)\n    if outdated_transformers:\n        logger.debug(\"Collected Block Structure data for the following transformers is outdated: '%s'.\", [(transformer.name(), transformer.VERSION) for transformer in outdated_transformers])\n    return bool(outdated_transformers)",
    "nl": "Returns whether the collected data in the block structure is outdated.",
    "original_nl": "Returns whether the collected data in the block structure is outdated."
  },
  {
    "code": "def fetch_as_element(self, **kw):\n    clone = self.copy()\n    clone.format.field_format('id')\n    for custom_field in ['field_ids', 'field_names']:\n        clone.format.data.pop(custom_field, None)\n    for list_of_results in clone.fetch_raw(**kw):\n        for entry in list_of_results:\n            (yield Connection(**entry))",
    "nl": "Fetch the results and return as a Connection element. The original\nquery is not modified.\n\n",
    "original_nl": "Fetch the results and return as a Connection element. The original\nquery is not modified.\n\n:return: generator of elements\n:rtype: :class:`.Connection`"
  },
  {
    "code": "def rapor_turu_sec(self):\n    form = RaporTurForm(self.object, current=self.current)\n    self.form_out(form)",
    "nl": "Se\u00e7ili olan projenin rapor t\u00fcr\u00fcn\u00fcn se\u00e7ilmesi i\u015flemlerini ger\u00e7ekle\u015ftirir.",
    "original_nl": "Se\u00e7ili olan projenin rapor t\u00fcr\u00fcn\u00fcn se\u00e7ilmesi i\u015flemlerini ger\u00e7ekle\u015ftirir."
  },
  {
    "code": "def ci_width(self, alpha=0.05):\n    half_width = self.std_dev.copy()\n    n = self.num_realizations\n    if (n > 1):\n        half_width *= scipy.stats.t.ppf((1 - (alpha / 2)), (n - 1))\n    return half_width",
    "nl": "Confidence interval half-width based on a Student t distribution\n",
    "original_nl": "Confidence interval half-width based on a Student t distribution\n\nParameters\n----------\nalpha : float\n    Significance level (one minus the confidence level!)\n\nReturns\n-------\nfloat\n    Half-width of a two-sided (1 - :math:`alpha`) confidence interval"
  },
  {
    "code": "def read(self, request, id):\n    resource = get_object_or_404(BootResource, id=id)\n    stream = json_object(boot_resource_to_dict(resource, with_sets=True), request)\n    return HttpResponse(stream, content_type='application/json; charset=utf-8', status=int(http.client.OK))",
    "nl": "Read a boot resource.",
    "original_nl": "Read a boot resource."
  },
  {
    "code": "def url_fix(url, charset='UTF-8'):\n    if isinstance(url, unicode):\n        url = url.encode(charset)\n    return quote(url, safe=\"%/:=&?~#+!$,;'@()*[]\")",
    "nl": "Normalize the URL if it contains Non-ASCII chars",
    "original_nl": "Normalize the URL if it contains Non-ASCII chars"
  },
  {
    "code": "def usernames_in(message):\n    if _command_re.match(message.split(' ', 1)[0]):\n        message = message.split(' ', 1)[1]\n    message = strip_urls(message)\n    results = []\n    for result in _username_re.finditer(message):\n        if result.group(1):\n            results.append(result.group(1))\n    return results",
    "nl": "Return all the matched usernames in the message",
    "original_nl": "Return all the matched usernames in the message"
  },
  {
    "code": "def test_cleared(self):\n    message = 'the message'\n    category = RuntimeWarning\n    warnings.warn(message=message, category=category)\n    self.assertDictSubsets(self.flushWarnings(), [{\n        'category': category,\n        'message': message,\n    }])\n    self.assertEqual(self.flushWarnings(), [])",
    "nl": "After a particular warning event has been returned by\nL{TestCase.flushWarnings}, it is not returned by subsequent calls.",
    "original_nl": "After a particular warning event has been returned by\nL{TestCase.flushWarnings}, it is not returned by subsequent calls."
  },
  {
    "code": "@classmethod\ndef deserialize(cls, uid, trusted=True, registry=None, **kwargs):\n    if (registry is None):\n        raise ValueError('no registry provided')\n    if (uid not in registry):\n        raise ValueError('uid not found: {}'.format(uid))\n    if (not isinstance(registry[uid], UidModel)):\n        date_created = registry[uid]['date_created']\n        date_modified = registry[uid]['date_modified']\n        kwargs.update({\n            'verbose': False,\n        })\n        new_model = super(UidModel, cls).deserialize(value=registry[uid], registry=registry, trusted=trusted, **kwargs)\n        new_model._backend.update({\n            'uid': properties.Uuid.from_json(uid),\n            'date_created': properties.DateTime.from_json(date_created),\n            'date_modified': properties.DateTime.from_json(date_modified),\n        })\n        registry.update({\n            uid: new_model,\n        })\n    return registry[uid]",
    "nl": "Deserialize nested UidModels from flat pointer dictionary",
    "original_nl": "Deserialize nested UidModels from flat pointer dictionary"
  },
  {
    "code": "def _masterClosed(self):\n    try:\n        self.master.item.sigImageChanged.disconnect(self.updateView)\n    except TypeError:\n        pass\n    self.sigRegionChanged.disconnect(self.updateView)\n    self.tool.display.stack.sigValuesChanged.disconnect(self.updateView)",
    "nl": "roi still exists but doesnt listen to changes in the master display",
    "original_nl": "roi still exists but doesnt listen to changes in the master display"
  },
  {
    "code": "def _calc_xp(self, txt, gid):\n    spc = self.plugin_config[gid]\n    if (len(txt) < spc['low_cutoff']):\n        return 0\n    t_percent = ((len(txt) - spc['low_cutoff']) / (2000 - spc['low_cutoff']))\n    t_xp = (spc['xp_min'] + ((spc['xp_max'] - spc['xp_min']) * t_percent))\n    return int(t_xp)",
    "nl": ":type txt:str\n",
    "original_nl": ":type txt:str\n:param txt:message to calculate the xp for\n:return:"
  },
  {
    "code": "@home_position.setter\ndef home_position(self, value: np.ndarray) -> None:\n    self._home_position = value",
    "nl": "Set home position.",
    "original_nl": "Set home position."
  },
  {
    "code": "def test_svg_seaborn():\n    draw_format_method('svg', 'seaborn')",
    "nl": "Write .svg graphics with seaborn",
    "original_nl": "Write .svg graphics with seaborn"
  },
  {
    "code": "def GetMessages(self, unused_formatter_mediator, event):\n    if (self.DATA_TYPE != event.data_type):\n        raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(event.data_type))\n    event_values = event.CopyToDict()\n    file_reference = event_values.get('file_reference', None)\n    if file_reference:\n        event_values['file_reference'] = '{0:d}-{1:d}'.format((file_reference & 281474976710655), (file_reference >> 48))\n    parent_file_reference = event_values.get('parent_file_reference', None)\n    if parent_file_reference:\n        event_values['parent_file_reference'] = '{0:d}-{1:d}'.format((parent_file_reference & 281474976710655), (parent_file_reference >> 48))\n    update_reason_flags = event_values.get('update_reason_flags', 0)\n    update_reasons = []\n    for (bitmask, description) in sorted(self._USN_REASON_FLAGS.items()):\n        if (bitmask & update_reason_flags):\n            update_reasons.append(description)\n    event_values['update_reason'] = ', '.join(update_reasons)\n    update_source_flags = event_values.get('update_source_flags', 0)\n    update_sources = []\n    for (bitmask, description) in sorted(self._USN_SOURCE_FLAGS.items()):\n        if (bitmask & update_source_flags):\n            update_sources.append(description)\n    event_values['update_source'] = ', '.join(update_sources)\n    return self._ConditionalFormatMessages(event_values)",
    "nl": "Determines the formatted message strings for an event object.\n\nArgs:\n  formatter_mediator (FormatterMediator): mediates the interactions between\n      formatters and other components, such as storage and Windows EventLog\n      resources.\n  event (EventObject): event.\n",
    "original_nl": "Determines the formatted message strings for an event object.\n\nArgs:\n  formatter_mediator (FormatterMediator): mediates the interactions between\n      formatters and other components, such as storage and Windows EventLog\n      resources.\n  event (EventObject): event.\n\nReturns:\n  tuple(str, str): formatted message string and short message string.\n\nRaises:\n  WrongFormatter: if the event object cannot be formatted by the formatter."
  },
  {
    "code": "def cartesian_to_spherical_azimuthal(x, y):\n    y = (float(y) if isinstance(y, int) else y)\n    phi = numpy.arctan2(y, x)\n    return (phi % (2 * numpy.pi))",
    "nl": "Calculates the azimuthal angle in spherical coordinates from Cartesian\ncoordinates. The azimuthal angle is in [0,2*pi].\n",
    "original_nl": "Calculates the azimuthal angle in spherical coordinates from Cartesian\ncoordinates. The azimuthal angle is in [0,2*pi].\n\nParameters\n----------\nx : {numpy.array, float}\n    X-coordinate.\ny : {numpy.array, float}\n    Y-coordinate.\n\nReturns\n-------\nphi : {numpy.array, float}\n    The azimuthal angle."
  },
  {
    "code": "def settingsDeploy(self, action='start', options={\n    \n}):\n    os.environ['DJANGO_SETTINGS_MODULE'] = TEST_SETTINGS\n    options.update({\n        'settings': TEST_SETTINGS,\n    })\n    return self.deploy(action, options)",
    "nl": "Use the hendrix test project to test the bash deployment flow path",
    "original_nl": "Use the hendrix test project to test the bash deployment flow path"
  },
  {
    "code": "def test_update_airspeed_estimate(mpstate):\n    loadedModule = cuav_check.init(mpstate)\n    m = common.MAVLink_global_position_int_message(2000, 230100000, (- 344560000), 110000, 100000, 3000, 200, 1, 9000)\n    loadedModule.cuav_settings.wind_speed = 10\n    loadedModule.cuav_settings.wind_direction = 88\n    loadedModule.update_airspeed_estimate(m)\n    loadedModule.unload()",
    "nl": "Test the airspeed updater",
    "original_nl": "Test the airspeed updater"
  },
  {
    "code": "def _find_event(self, clz):\n    return ((k, v) for (k, v) in enumerate(self.bot.cron.events) if isinstance(v, clz)).next()",
    "nl": "Find an event of a given class in cron.",
    "original_nl": "Find an event of a given class in cron."
  },
  {
    "code": "def json_deep_search(iterable, field):\n    fields_found = []\n    if isinstance(iterable, dict):\n        for (key, value) in iterable.items():\n            if (key == field):\n                fields_found.append(value)\n            elif isinstance(value, (dict, list)):\n                fields_found.extend(json_deep_search(value, field))\n    elif isinstance(iterable, list):\n        for item in iterable:\n            fields_found.extend(json_deep_search(item, field))\n    return fields_found",
    "nl": "Takes a JSON like data-structure and search for the occurrences\nof the given field/key.",
    "original_nl": "Takes a JSON like data-structure and search for the occurrences\nof the given field/key."
  },
  {
    "code": "def test_update_user_nonexistent(mailchimp_user_factory, mock_mc_client):\n    mailchimp_user = mailchimp_user_factory()\n    user = mailchimp_user.user\n    mock_mc_client.lists.members.update.side_effect = MailChimpError\n    mock_mc_client.lists.members.create.return_value = {\n        'id': 'hash',\n    }\n    with mock.patch('mailing_list.mailchimp_utils._get_client', return_value=mock_mc_client):\n        mailchimp_utils.sync_mailchimp_data('list', user)\n    expected_data = mailchimp_utils.get_member_info(user)\n    expected_data.update({\n        'status': 'subscribed',\n    })\n    assert (mock_mc_client.lists.members.create.call_count == 1)\n    assert (mock_mc_client.lists.members.create.call_args[1] == {\n        'data': expected_data,\n        'list_id': 'list',\n    })\n    assert (models.MailchimpUser.objects.count() == 1)\n    mailchimp_user.refresh_from_db()\n    assert (mailchimp_user.subscriber_hash == mock_mc_client.lists.members.create.return_value['id'])",
    "nl": "If we have a record of a mailing list member but that member doesn't\nexist, the member should be created and the record updated.",
    "original_nl": "If we have a record of a mailing list member but that member doesn't\nexist, the member should be created and the record updated."
  },
  {
    "code": "def make_call_side_effect(text):\n\n    def side_effect(*args, **kwargs):\n        log = kwargs['stdout']\n        log.write(text)\n    return side_effect",
    "nl": "Intended for mocking the subprocess.call() in testing\n_release_tools.up_to_date(). Return a side effect that\nprints text to mocked function's stdout argument.",
    "original_nl": "Intended for mocking the subprocess.call() in testing\n_release_tools.up_to_date(). Return a side effect that\nprints text to mocked function's stdout argument."
  },
  {
    "code": "def sampling(orig, dest, rand, format, conf):\n    return ShellCommand('\\n                        count=$({tool} view -Sc {input[sam]})\\n                        ## judge mapped reads number less than sampling number\\n                        if [ $count -le {param[random_number]} ]\\n                        then\\n                            ln -f {input[sam]} {input[sam]}.{param[random_number]}\\n                            {tool} view -bS {input[sam]}.{param[random_number]} > {output[samp]}\\n                        else\\n                            sampling_pe_sam.py {input[sam]} {param[random_number]}\\n                            {tool} view -bS {input[sam]}.{param[random_number]} > {output[samp]}\\n                        fi\\n                        ', tool='samtools', input={\n        'sam': orig,\n    }, output={\n        'samp': dest,\n    }, param={\n        'random_number': rand,\n    }, name='sampling bam')",
    "nl": "prepare sampling fastq files for library contamination and fastqc\nrand: the number of random selected fastq reads\nuse lh3's https://github.com/lh3/seqtk/ to sample fastq and fastq.gz",
    "original_nl": "prepare sampling fastq files for library contamination and fastqc\nrand: the number of random selected fastq reads\nuse lh3's https://github.com/lh3/seqtk/ to sample fastq and fastq.gz"
  },
  {
    "code": "def targets(self, node):\n    node = self.node(node)\n    nodes = [conn.target for conn in self.connections if (conn.target == node)]\n    return nodes",
    "nl": "Return nodes that `node` passes data into.",
    "original_nl": "Return nodes that `node` passes data into."
  },
  {
    "code": "def linkQualityCB(self, percentage):\n    q = self.linkQuality.addMeasurementCount(percentage)\n    if (q is not None):\n        self.sig_flieLink.emit(percentage)",
    "nl": "Called when the link driver updates the link quality measurement",
    "original_nl": "Called when the link driver updates the link quality measurement"
  },
  {
    "code": "def test_eval_running_mode3(self):\n    self.root.database.prepare_to_run()\n    self.root.database.set_value('root', 'val1', 10.0)\n    test = 'cm.sqrt({val1}/{val2})'\n    formatted = self.root.format_and_eval_string(test)\n    assert (formatted == (1 + 0j))\n    assert self.root._eval_cache\n    assert (test in self.root._eval_cache)\n    self.root.database.set_value('root', 'val1', 40.0)\n    formatted = self.root.format_and_eval_string(test)\n    assert (formatted == (2 + 0j))",
    "nl": "Test eval expression containing a cmath function.",
    "original_nl": "Test eval expression containing a cmath function."
  },
  {
    "code": "def test_layermap_strict(self):\n    with self.assertRaises(InvalidDecimal):\n        lm = LayerMapping(Interstate, inter_shp, inter_mapping)\n        lm.save(silent=True, strict=True)\n    Interstate.objects.all().delete()\n    lm = LayerMapping(Interstate, inter_shp, inter_mapping)\n    lm.save(silent=True)\n    self.assertEqual(2, Interstate.objects.count())\n    ds = DataSource(inter_shp)\n    valid_feats = ds[0][:2]\n    for feat in valid_feats:\n        istate = Interstate.objects.get(name=feat['Name'].value)\n        if (feat.fid == 0):\n            self.assertEqual(Decimal(str(feat['Length'])), istate.length)\n        elif (feat.fid == 1):\n            self.assertAlmostEqual(feat.get('Length'), float(istate.length), 2)\n        for (p1, p2) in zip(feat.geom, istate.path):\n            self.assertAlmostEqual(p1[0], p2[0], 6)\n            self.assertAlmostEqual(p1[1], p2[1], 6)",
    "nl": "Testing the `strict` keyword, and import of a LineString shapefile.",
    "original_nl": "Testing the `strict` keyword, and import of a LineString shapefile."
  },
  {
    "code": "def calculate_base_skill_roll(pc, skill):\n    trait = skill.trait\n    trait_value = api.character.modified_trait_rank(trait)\n    skill_value = api.character.skills.get_skill_rank(skill.id)\n    return DicePool().from_values(roll=(skill_value + trait_value), keep=trait_value)",
    "nl": "calculate the base skill roll for a given skill",
    "original_nl": "calculate the base skill roll for a given skill"
  },
  {
    "code": "def check_switch_vendor(old_vendor, name, urls, _depth=0):\n    if (_depth > 3):\n        return ''\n    new_name = check_for_launchpad(old_vendor, name, urls)\n    if new_name:\n        return ('launchpad', new_name)\n    return ('', '')",
    "nl": "Check if the project should switch vendors. E.g\nproject pushed on pypi, but changelog on launchpad.\n\n",
    "original_nl": "Check if the project should switch vendors. E.g\nproject pushed on pypi, but changelog on launchpad.\n\n:param name: str, name of the project\n:param urls: set, urls to check.\n:return: tuple, (str(new vendor name), str(new project name))"
  },
  {
    "code": "def total_number_of_data_points(self):\n    n_data = 0.0\n    for telescope in self.telescopes:\n        n_data = (n_data + telescope.n_data('flux'))\n    return n_data",
    "nl": "Compute the parallax displacement for all the telescopes, if this is desired in\nthe second order parameter.\n",
    "original_nl": "Compute the parallax displacement for all the telescopes, if this is desired in\nthe second order parameter.\n:return: n_data, the total number of points\n:rtype: float"
  },
  {
    "code": "def request_headers(request):\n    return CaseInsensitiveDict(((key[5:].lower().replace('_', '-'), value) for (key, value) in request.META.items() if key.startswith('HTTP_')))",
    "nl": "Return a dict with headers from a request.\n\nHeader keys are case insensitive.",
    "original_nl": "Return a dict with headers from a request.\n\nHeader keys are case insensitive."
  },
  {
    "code": "def test_load_geetest_js():\n    jsfile = os.path.join(os.getcwd(), 'gsxt', 'geetest.5.10.10.js')\n    print(jsfile)\n    js_context = JSRUNTIME.compile(load_filetext(jsfile))\n    print(js_context)",
    "nl": "load javascript text from file, compile, return context object",
    "original_nl": "load javascript text from file, compile, return context object"
  },
  {
    "code": "def copy(self):\n    return ContinuousFactor(self.scope(), self.distribution.copy())",
    "nl": "Return a copy of the distribution.\n",
    "original_nl": "Return a copy of the distribution.\n\nReturns\n-------\nContinuousFactor object: copy of the distribution\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.special import beta\n>>> from pgmpy.factors.continuous import ContinuousFactor\n# Two variable dirichlet distribution with alpha = (1,2)\n>>> def dirichlet_pdf(x, y):\n...     return (np.power(x, 1) * np.power(y, 2)) / beta(x, y)\n>>> dirichlet_factor = ContinuousFactor(['x', 'y'], dirichlet_pdf)\n>>> dirichlet_factor.variables\n['x', 'y']\n>>> copy_factor = dirichlet_factor.copy()\n>>> copy_factor.variables\n['x', 'y']"
  },
  {
    "code": "def _tidy(self, work, file_path):\n    output_file = os.path.join(self._output_dir, work)\n    self._logger.info('Tidying file {} into {}'.format(file_path, output_file))\n    try:\n        tei_doc = etree.parse(file_path)\n    except etree.XMLSyntaxError as err:\n        self._logger.error('XML file \"{}\" is invalid: {}'.format(file_path, err))\n        raise\n    return self.transform(tei_doc).getroot()",
    "nl": "Transforms the file at `file_path` into simpler XML and returns\nthat.",
    "original_nl": "Transforms the file at `file_path` into simpler XML and returns\nthat."
  },
  {
    "code": "def GetScript(self, source, line, col, filename):\n    return jedi.Script(source, line, col, filename)",
    "nl": "Return Jedi script object suitable to AutoComplete and ShowCallTip",
    "original_nl": "Return Jedi script object suitable to AutoComplete and ShowCallTip"
  },
  {
    "code": "def file_convert(mib_txt_path, mib_py_path):\n    mod_logger_snmp = loggers.module_logger(name=__name__)\n    smidump = Popen(['smidump', '-k', '-f', 'python', mib_txt_path], stdout=PIPE)\n    list_stdout = smidump.communicate()[0]\n    if (len(list_stdout) == 0):\n        return 'Fail'\n    mib_path_tmp = os.path.join(mib_py_path, 'tmp')\n    if (not os.path.exists(mib_path_tmp)):\n        os.makedirs(mib_path_tmp)\n    sys.path.append(mib_path_tmp)\n    file_name = os.path.splitext(os.path.basename(mib_txt_path))[0]\n    temp_file_name = '{0}.py'.format(file_name)\n    temp_file_path = os.path.join(mib_path_tmp, temp_file_name)\n    with open(temp_file_path, 'ab') as a:\n        a.write(list_stdout)\n    temp_module = __import__(os.path.splitext(os.path.basename(mib_txt_path))[0])\n    if (('moduleName' in list(temp_module.MIB.keys())) and ('nodes' in list(temp_module.MIB.keys()))):\n        helpers.MIBS_DICT.update({\n            temp_module.MIB['moduleName']: list(temp_module.MIB['nodes'].keys()),\n        })\n    sys.path.remove(mib_path_tmp)\n    os.remove(temp_file_path)\n    pipe = Popen(['libsmi2pysnmp', '--no-text'], stdout=PIPE, stdin=PIPE)\n    stdout = pipe.communicate(input=list_stdout)\n    mib_name = '{0}.py'.format(temp_module.MIB['moduleName'])\n    mib_py_path = os.path.join(mib_py_path, mib_name)\n    mod_logger_snmp.debug(('Convert %s to %s' % (file_name, temp_file_name)))\n    with open(mib_py_path, 'a') as py_file:\n        for string in stdout:\n            if (string is not None):\n                str_dict = string.decode('utf-8').split('\\n')\n                for each_str in str_dict:\n                    if ('ModuleCompliance' in each_str):\n                        if ('ObjectGroup' in each_str):\n                            py_file.write((each_str + '\\n'))\n                    elif ('Compliance)' in each_str):\n                        pass\n                    else:\n                        py_file.write((each_str + '\\n'))\n    return mib_name",
    "nl": "Convert .txt MIB to .py.\n\nArgs:\n    mib_txt_path(str):  Full path to .txt MIB.\n    mib_py_path(str):  Full path to .py MIB\n\nExamples::\n\n    file_convert(mib_txt_path, mib_py_path)",
    "original_nl": "Convert .txt MIB to .py.\n\nArgs:\n    mib_txt_path(str):  Full path to .txt MIB.\n    mib_py_path(str):  Full path to .py MIB\n\nExamples::\n\n    file_convert(mib_txt_path, mib_py_path)"
  },
  {
    "code": "def _modify_command(command):\n    if isinstance(command, list):\n        for i in xrange(len(command)):\n            if (command[i] == 'configtest'):\n                command[i] = '-t'\n    else:\n        command = command.replace('configtest', '-t')\n    return command",
    "nl": "Modifies command so configtest works inside the docker image",
    "original_nl": "Modifies command so configtest works inside the docker image"
  },
  {
    "code": "def do_info_threads(self):\n    return self.execute_command(self.GS_INFO_THREADS)",
    "nl": "Perform 'info threads' command. Returns output.",
    "original_nl": "Perform 'info threads' command. Returns output."
  },
  {
    "code": "def replace_nodes(self, node1, node2):\n    (node1[PARENT][DIRECTION1], node2[PARENT][DIRECTION2]) = (node2[PARENT][DIRECTION2], node1[PARENT][DIRECTION1])\n    (node1[PARENT], node2[PARENT]) = (node2[PARENT], node1[PARENT])\n    for direction in [LEFT, RIGHT]:\n        (node1[direction][PARENT], node2[direction][PARENT]) = (node2[direction][PARENT], node1[direction][PARENT])\n        (node1[direction], node2[direction]) = (node2[direction], node1[direction])",
    "nl": "Replaces node1 and node2 by rewiring the pointers (parent, left\nand right) of the two nodes.\n\nArgs:\n    node1: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]\n    node2: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]",
    "original_nl": "Replaces node1 and node2 by rewiring the pointers (parent, left\nand right) of the two nodes.\n\nArgs:\n    node1: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]\n    node2: list, structure contains a node with format:\n        [PARENT, KEY, LEFT, RIGHT, SIZE]"
  },
  {
    "code": "def getfield(self, pkt, s):\n    if s.startswith('\\r\\n'):\n        s = s.lstrip('\\r\\n')\n        if (s == ''):\n            return ('', '')\n    self.myresult = ''\n    for c in s:\n        self.myresult = (self.myresult + base64.standard_b64encode(c))\n    return ('', self.myresult)",
    "nl": "this method will get the packet, takes what does need to be\ntaken and let the remaining go, so it returns two values.\nfirst value which belongs to this field and the second is\nthe remaining which does need to be dissected with\nother \"field classes\".\n@param pkt: holds the whole packet\n@param s: holds only the remaining data which is not dissected yet.",
    "original_nl": "this method will get the packet, takes what does need to be\ntaken and let the remaining go, so it returns two values.\nfirst value which belongs to this field and the second is\nthe remaining which does need to be dissected with\nother \"field classes\".\n@param pkt: holds the whole packet\n@param s: holds only the remaining data which is not dissected yet."
  },
  {
    "code": "def assert_is_instance(value, types, message=None, extra=None):\n    assert isinstance(value, types), _assert_fail_message(message, value, types, 'is not an instance of', extra)",
    "nl": "Raises an AssertionError if value is not an instance of type(s).",
    "original_nl": "Raises an AssertionError if value is not an instance of type(s)."
  },
  {
    "code": "@classmethod\ndef expand(cls, input):\n\n    def parameterized_expand_wrapper(f):\n        stack = inspect.stack()\n        frame = stack[1]\n        frame_locals = frame[0].f_locals\n        base_name = f.__name__\n        get_input = cls.input_as_callable(input)\n        for (num, args) in enumerate(get_input()):\n            p = param.from_decorator(args)\n            name_suffix = ('_%s' % (num,))\n            if ((len(p.args) > 0) and isinstance(p.args[0], basestring)):\n                name_suffix += ('_' + cls.to_safe_name(p.args[0]))\n            name = (base_name + name_suffix)\n            frame_locals[name] = cls.param_as_standalone_func(p, f, name)\n        return nottest(f)\n    return parameterized_expand_wrapper",
    "nl": "A \"brute force\" method of parameterizing test cases. Creates new\ntest cases and injects them into the namespace that the wrapped\nfunction is being defined in. Useful for parameterizing tests in\nsubclasses of 'UnitTest', where Nose test generators don't work.\n\n>>> @parameterized.expand([(\"foo\", 1, 2)])\n... def test_add1(name, input, expected):\n...     actual = add1(input)\n...     assert_equal(actual, expected)\n...\n>>> locals()\n... 'test_add1_foo_0': <function ...> ...\n>>>",
    "original_nl": "A \"brute force\" method of parameterizing test cases. Creates new\ntest cases and injects them into the namespace that the wrapped\nfunction is being defined in. Useful for parameterizing tests in\nsubclasses of 'UnitTest', where Nose test generators don't work.\n\n>>> @parameterized.expand([(\"foo\", 1, 2)])\n... def test_add1(name, input, expected):\n...     actual = add1(input)\n...     assert_equal(actual, expected)\n...\n>>> locals()\n... 'test_add1_foo_0': <function ...> ...\n>>>"
  },
  {
    "code": "def test_properties():\n    telescope_obj = pyuvdata.Telescope()\n    prop_dict = dict(zip(required_properties, required_parameters))\n    for (k, v) in prop_dict.iteritems():\n        rand_num = np.random.rand()\n        setattr(telescope_obj, k, rand_num)\n        this_param = getattr(telescope_obj, v)\n        try:\n            nt.assert_equal(rand_num, this_param.value)\n        except AssertionError:\n            print('setting {prop_name} to a random number failed'.format(prop_name=k))\n            raise",
    "nl": "Test that properties can be get and set properly.",
    "original_nl": "Test that properties can be get and set properly."
  },
  {
    "code": "def get_task(self, task, view=False):\n    infos = self.get_task_infos(task)\n    if (infos is None):\n        answer = (None if (not view) else (None, None))\n        return answer\n    return (infos.cls if (not view) else (infos.cls, infos.view))",
    "nl": "Access a given task class.\n",
    "original_nl": "Access a given task class.\n\nParameters\n----------\ntask : unicode\n    Id of the task class for which to return the actual class.\n\nview : bool, optional\n    Whether or not to return the view assoicated with the task.\n\nReturns\n-------\ntask_cls : type or None\n    Class associated to the requested task or None if the task was not\n    found.\n\ntask_view : EnamlDefMeta or None, optional\n    Associated view if requested."
  },
  {
    "code": "def clear(self):\n    self.values = [0 for x in xrange(len(self.values))]\n    self.count = 0",
    "nl": "Clears the sample, setting all values to zero.",
    "original_nl": "Clears the sample, setting all values to zero."
  },
  {
    "code": "def p_preprocessor_block_5(p):\n    p[0] = p[1]",
    "nl": "preprocessor_block : interrupt_block",
    "original_nl": "preprocessor_block : interrupt_block"
  },
  {
    "code": "def contained_expr(container, ast_expr):\n    res = set()\n    for elmt in container:\n        if matches_expr(elmt, ast_expr):\n            res.add(elmt)\n    return list(res)",
    "nl": "Given a container, returns structural AST expression matches.",
    "original_nl": "Given a container, returns structural AST expression matches."
  },
  {
    "code": "def get_sitemap(base_url, filename, out_file_path, compressed=True):\n    response = urllib2.urlopen((base_url + filename))\n    compressed_file = StringIO.StringIO()\n    compressed_file.write(response.read())\n    compressed_file.seek(0)\n    if compressed:\n        decompressed_file = gzip.GzipFile(fileobj=compressed_file, mode='rb')\n        with open(out_file_path, 'w') as outfile:\n            outfile.write(decompressed_file.read())\n    else:\n        with open(out_file_path, 'w') as outfile:\n            outfile.write(compressed_file.read())",
    "nl": "Get the sitemap XML.\n",
    "original_nl": "Get the sitemap XML.\n\nParameters\n----------\nbase_url : str\n  Base url of file location\nfilename : str\n  Filename on web\nout_file_path : str\n  Desire file path to save to\n\nReturns\n-------\nNone (file is downloaded)"
  },
  {
    "code": "@app.route('/project/<pid>/import', methods=['GET', 'POST'])\ndef import_scan(pid):\n    project = get_project_db(pid)\n    db = database.ScanDatabase(project['dbfile'])\n    if (flask.request.method == 'GET'):\n        files = db.importdb.get_imported_files()\n        return flask.render_template('import.html', pid=pid, files=files, name=project['name'])\n    else:\n        i = importscan.Import(project['dbfile'])\n        scans = flask.request.files.getlist('scans[]')\n        for scan in scans:\n            res = i.import_scan(scan.read())\n            if (res is True):\n                db.importdb.add_import_file(scan.filename)\n        a = attacks.Attack(project['dbfile'])\n        a.find_attacks()\n        return flask.redirect(flask.url_for('get_project', pid=pid))",
    "nl": "Import scan data into the database associated with the pid.",
    "original_nl": "Import scan data into the database associated with the pid."
  },
  {
    "code": "def operon_map(self):\n    if (not self.__operon_mappings):\n        pairs = mo.get_operon_pairs(self.__microbes_online_db, self)\n        synonyms = self.thesaurus()\n        self.__operon_mappings = {synonyms[gene]: synonyms[head] for (head, gene) in pairs}\n    return self.__operon_mappings",
    "nl": "Returns the operon map for this particular organism.\nMicrobes Online works on VNG names, but RSAT is working on\nfeature ids, so this function also maps VNG names to feature ids",
    "original_nl": "Returns the operon map for this particular organism.\nMicrobes Online works on VNG names, but RSAT is working on\nfeature ids, so this function also maps VNG names to feature ids"
  },
  {
    "code": "def initialize_tree(self):\n    self.quadtree = QuadTree(0.0, 0.0, max(self.width, self.height), self.quadtree_threshold)\n    self.started = True",
    "nl": "Initialize the Quad Tree implementation.",
    "original_nl": "Initialize the Quad Tree implementation."
  },
  {
    "code": "def test_weighted_ilike_exception(self):\n    with pytest.raises(AssertionError):\n        ExampleTable.name.weighted_ilike('%John%', '5')",
    "nl": "String.weighted_ilike() fails as expected",
    "original_nl": "String.weighted_ilike() fails as expected"
  },
  {
    "code": "def ready(self):\n    connection_created.connect(self.activate_pragmas_per_connection)\n    self.activate_pragmas_on_start()\n    logger.info('Running Kolibri with the following settings: {settings}'.format(settings=os.environ['DJANGO_SETTINGS_MODULE']))",
    "nl": "Sets up PRAGMAs.",
    "original_nl": "Sets up PRAGMAs."
  },
  {
    "code": "def _setting(self, key, default):\n    env = 'STATSD_{}'.format(key).upper()\n    return self._settings.get(key, os.environ.get(env, default))",
    "nl": "Return the setting, checking config, then the appropriate\nenvironment variable, falling back to the default.\n\n",
    "original_nl": "Return the setting, checking config, then the appropriate\nenvironment variable, falling back to the default.\n\n:param str key: The key to get\n:param any default: The default value if not set\n:return: str"
  },
  {
    "code": "@abc.abstractmethod\ndef get_allowed_network_types(self, agent=None):\n    pass",
    "nl": "Return the agent's or driver's allowed network types.\n\nFor example: return ('flat', ...). You can also refer to the\nconfiguration the given agent exposes.",
    "original_nl": "Return the agent's or driver's allowed network types.\n\nFor example: return ('flat', ...). You can also refer to the\nconfiguration the given agent exposes."
  },
  {
    "code": "def import_module_from_string(source, name, add_to_sys_modules=True):\n    import imp\n    user_module = imp.new_module(name)\n    exec(source, user_module.__dict__)\n    if add_to_sys_modules:\n        sys.modules[name] = user_module\n    return user_module",
    "nl": "Well this seems dangerous.",
    "original_nl": "Well this seems dangerous."
  },
  {
    "code": "@pytest.fixture\ndef minmax_zoom():\n    path = os.path.join(TESTDATA_DIR, 'minmax_zoom.mapchete')\n    return ExampleConfig(path=path, dict=_dict_from_mapchete(path))",
    "nl": "Fixture for minmax_zoom.mapchete.",
    "original_nl": "Fixture for minmax_zoom.mapchete."
  },
  {
    "code": "def test_clear(self):\n    cart = Cart()\n    cart.add('rabbit')\n    cart.modified = False\n    cart.clear()\n    self.assertEqual(len(cart), 0)\n    self.assertEqual(cart.modified, True)",
    "nl": "Cart.clear() clears the cart and marks it as modified",
    "original_nl": "Cart.clear() clears the cart and marks it as modified"
  },
  {
    "code": "@classmethod\ndef count_stats(self, xs):\n    stats = {\n        'DIB': 0,\n        'EIB': 0,\n    }\n    for x in xs:\n        if (float(x.name) > 0):\n            stats['EIB'] += 1\n        else:\n            stats['DIB'] += 1\n    return stats",
    "nl": "xs is a bedtool instance where the name field holds the bin score",
    "original_nl": "xs is a bedtool instance where the name field holds the bin score"
  },
  {
    "code": "def print_exceptions(exceptions, max_print=10):\n    exceptions = sorted(exceptions, key=(lambda n: (n[2], n[1])), reverse=True)[:max_print]\n    for (url, stroke_count, _) in exceptions:\n        logging.info('\\t%s - %i strokes', url, stroke_count)",
    "nl": "Print the exceptions, but not too many.\n",
    "original_nl": "Print the exceptions, but not too many.\n\nParameters\n----------\nexceptions : list\n    Triplets (url, stroke_count, dist to closest mode)\nmax_print : int\n    Print not more then max_print lines"
  },
  {
    "code": "@manager.option('text')\ndef send(text):\n    data = current_app.config['TEST_DATA']\n    uri = current_app.config['SLACK_CALLBACK']\n    client = current_app.test_client()\n    data['text'] = text\n    rv = client.post(uri, data=data)\n    if (rv.status_code == 200):\n        body = rv.data\n        if (not body):\n            print('Response body is empty!')\n            return\n        obj = json.loads(body)\n        if (not obj.get('attachments')):\n            obj = obj['text']\n            print(obj)\n        else:\n            pprint(obj)\n    else:\n        print(('Error!\\nstatus code: %s\\nbody: %s' % (rv.status_code, rv.data)))",
    "nl": "Send text to slack callback url",
    "original_nl": "Send text to slack callback url"
  },
  {
    "code": "def ar1(func, method=ar_nitime):\n    func_centered = (func - func.mean(0))\n    ar_vals = np.apply_along_axis(method, 0, func_centered)\n    return ar_vals",
    "nl": "Apply the 'ar_nitime' function across the centered functional \ntimeseries.\n\n:type func: Nibabel data\n",
    "original_nl": "Apply the 'ar_nitime' function across the centered functional \ntimeseries.\n\n:type func: Nibabel data\n:param func: The functional timeseries data.\n:type method: Python function\n:param method: (default: ar_nitime) The algorithm to use to calculate AR1.\n:rtype: NumPy array\n:return: The vector of AR1 values."
  },
  {
    "code": "def create_badge(slug: str, label: str, image_filename: str, *, brand_id: Optional[BrandID]=None, description: Optional[str]=None, featured: bool=False) -> BadgeTuple:\n    badge = Badge(slug, label, image_filename, brand_id=brand_id, description=description, featured=featured)\n    db.session.add(badge)\n    db.session.commit()\n    return badge.to_tuple()",
    "nl": "Introduce a new badge.",
    "original_nl": "Introduce a new badge."
  },
  {
    "code": "def CompileReport(self, mediator):\n    lines_of_text = ['Listing file paths and hashes']\n    for (pathspec, hashes) in sorted(self._paths_with_hashes.items(), key=(lambda tuple: tuple[0].comparable)):\n        path_string = self._GeneratePathString(mediator, pathspec, hashes)\n        lines_of_text.append(path_string)\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)",
    "nl": "Compiles an analysis report.\n\nArgs:\n  mediator (AnalysisMediator): mediates interactions between analysis\n      plugins and other components, such as storage and dfvfs.\n",
    "original_nl": "Compiles an analysis report.\n\nArgs:\n  mediator (AnalysisMediator): mediates interactions between analysis\n      plugins and other components, such as storage and dfvfs.\n\nReturns:\n  AnalysisReport: report."
  },
  {
    "code": "@staticmethod\ndef complete_template(lazy_values):\n\n    def _template(self, text, line, start_index, end_index):\n        try:\n            values = lazy_values()\n        except TypeError:\n            values = (lazy_values or [])\n        return [v for v in values if v.startswith((text or '').strip())]\n    return _template",
    "nl": "Template method for handling auto-completion.",
    "original_nl": "Template method for handling auto-completion."
  },
  {
    "code": "def __init__(self, url='', credentials=None, get_credentials=True, http=None, model=None, log_request=False, log_response=False, credentials_args=None, default_global_params=None, additional_http_headers=None, response_encoding=None):\n    url = (url or self.BASE_URL)\n    super(DnsV1, self).__init__(url, credentials=credentials, get_credentials=get_credentials, http=http, model=model, log_request=log_request, log_response=log_response, credentials_args=credentials_args, default_global_params=default_global_params, additional_http_headers=additional_http_headers, response_encoding=response_encoding)\n    self.changes = self.ChangesService(self)\n    self.managedZones = self.ManagedZonesService(self)\n    self.projects = self.ProjectsService(self)\n    self.resourceRecordSets = self.ResourceRecordSetsService(self)",
    "nl": "Create a new dns handle.",
    "original_nl": "Create a new dns handle."
  },
  {
    "code": "def _client_receive(self):\n    try:\n        return self._client.readline()\n    except socket.error as e:\n        raise Error(self._ad, ('Encountered socket error reading RPC response \"%s\"' % e))",
    "nl": "Receives the server's response of an Rpc message.\n",
    "original_nl": "Receives the server's response of an Rpc message.\n\nReturns:\n    Raw byte string of the response.\n\nRaises:\n    Error: a socket error occurred during the read."
  },
  {
    "code": "def delete_files(log_printer, identifiers):\n    error_files = []\n    result = True\n    for identifier in identifiers:\n        try:\n            file_path = get_data_path(None, identifier)\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            else:\n                result = False\n        except (OSError, TypeError) as e:\n            error_files.append(hash_id(identifier))\n    if (len(error_files) > 0):\n        error_files = ', '.join(error_files)\n        logging.warning(\"There was a problem deleting the following files: {}. Please delete them manually from '{}'.\".format(error_files, Constants.USER_DATA_DIR))\n        result = False\n    return result",
    "nl": "Delete the given identifiers from the user's coala data directory.\n\n",
    "original_nl": "Delete the given identifiers from the user's coala data directory.\n\n:param log_printer: A LogPrinter object to use for logging.\n:param identifiers: The list of files to be deleted.\n:return:            True if all the given files were successfully deleted.\n                    False otherwise."
  },
  {
    "code": "def guess_ext(code):\n    lexer = guess_lexer(code)\n    mime = lexer.mimetypes[0]\n    ext = mimetypes.guess_extension(mime)[1:]\n    return ext",
    "nl": "Guess file ext with code",
    "original_nl": "Guess file ext with code"
  },
  {
    "code": "def __init__(self, path=None, start_token=START, end_token=END):\n    if ((type(path) != str) and (path != None)):\n        raise TypeError('path must be a string if provided')\n    self._chainmap = {\n        \n    }\n    self._excluded_words = []\n    self._path = path\n    self._start = start_token\n    self._end = end_token\n    if self._path:\n        try:\n            f = codecs.open(self._path, 'r', 'utf-8')\n            text = f.read()\n            for line in text.split('\\n'):\n                self.add_vocab(line, False)\n        except IOError:\n            print(((\"'\" + self._file) + \"' does not exist, creating it now.\"))\n            f = codecs.open(self._path, 'w', 'utf-8')\n        finally:\n            f.close()",
    "nl": "path: A string, indicating the phrase file to read from and write to\nstart_token: A string, used internally for denoting the start of phrases\nend_token: A string, used internally for denoting the end of phrases",
    "original_nl": "path: A string, indicating the phrase file to read from and write to\nstart_token: A string, used internally for denoting the start of phrases\nend_token: A string, used internally for denoting the end of phrases"
  },
  {
    "code": "def test_open_file_object(self):\n    if (not unittest.source):\n        return\n    file_object = open(unittest.source, 'rb')\n    bde_volume = pybde.volume()\n    if unittest.password:\n        bde_volume.set_password(unittest.password)\n    if unittest.recovery_password:\n        bde_volume.set_recovery_password(unittest.recovery_password)\n    bde_volume.open_file_object(file_object)\n    bde_volume.close()\n    with self.assertRaises(IOError):\n        bde_volume.open_file_object(None)\n    with self.assertRaises(ValueError):\n        bde_volume.open_file_object(file_object, mode='w')",
    "nl": "Tests the open_file_object function.",
    "original_nl": "Tests the open_file_object function."
  },
  {
    "code": "def __init__(self, jids, _id=None):\n    super(GetStatusesIqProtocolEntity, self).__init__(self.__class__.XMLNS, _id, _type='get', to=YowConstants.WHATSAPP_SERVER)\n    self.setGetStatusesProps(jids)",
    "nl": "Request the statuses of users. Should be sent once after login.\n\nArgs:\n    - jids: A list of jids representing the users whose statuses you are\n        trying to get.",
    "original_nl": "Request the statuses of users. Should be sent once after login.\n\nArgs:\n    - jids: A list of jids representing the users whose statuses you are\n        trying to get."
  },
  {
    "code": "def _parse_backtrace_frame(self, match_obj):\n    fredutil.fred_assert(False, 'Must be implemented in subclass.')",
    "nl": "Return a BacktraceFrame from the given re Match object.\nThe Match object should be a tuple (result of gre_backtrace_frame.)",
    "original_nl": "Return a BacktraceFrame from the given re Match object.\nThe Match object should be a tuple (result of gre_backtrace_frame.)"
  },
  {
    "code": "def union(self, other):\n    meet = self.concept.meet(other.concept)\n    return self._sibling(meet.index)",
    "nl": "Return the closest subsumed neighbor (unification, meet).",
    "original_nl": "Return the closest subsumed neighbor (unification, meet)."
  },
  {
    "code": "def show_storing_credentials_failed(self):\n    dialog = xbmcgui.Dialog()\n    dialog.ok(self.utils.get_addon_data().get('plugin'), self.utils.get_local_string(32008))",
    "nl": "Shows \"storing credentials failed\" modal\n\n",
    "original_nl": "Shows \"storing credentials failed\" modal\n\n:returns:  bool - Dialog shown"
  },
  {
    "code": "@staticmethod\ndef initializer(cls, *args):\n    global sketch_constructor\n\n    def sketch_constructor():\n        return cls(*args)",
    "nl": "FIXME: use of global not really nice (possible root of mysterious issue for user)",
    "original_nl": "FIXME: use of global not really nice (possible root of mysterious issue for user)"
  },
  {
    "code": "def _get_converted_filename(self, filename):\n    splitted_filename = list(os.path.splitext(filename))\n    splitted_filename.insert(1, '.converted')\n    logger.debug('converted file name')\n    return ''.join(splitted_filename)",
    "nl": "Returns the audio converted name associated to the standard audio filename\n* Example: /var/www/myproject/media/audio/picture_1.wav\n    will return /var/www/myproject/media/audio/picture_1.converted.wav",
    "original_nl": "Returns the audio converted name associated to the standard audio filename\n* Example: /var/www/myproject/media/audio/picture_1.wav\n    will return /var/www/myproject/media/audio/picture_1.converted.wav"
  },
  {
    "code": "def addLineAndNewlineIfNecessary(line, output):\n    output.write(line)\n    if (len(line) < 1):\n        return\n    if (not line.endswith('\\n')):\n        output.write('\\n')",
    "nl": "Add the line and if the line does not end with a newline add a newline.",
    "original_nl": "Add the line and if the line does not end with a newline add a newline."
  },
  {
    "code": "def url_replace_param(url, name, value):\n    url_components = urlparse(url)\n    query_params = parse_qs(url_components.query)\n    query_params[name] = value\n    query = urlencode(query_params, doseq=True)\n    return urlunparse([url_components.scheme, url_components.netloc, url_components.path, url_components.params, query, url_components.fragment])",
    "nl": "Replace a GET parameter in an URL",
    "original_nl": "Replace a GET parameter in an URL"
  },
  {
    "code": "def list(self):\n    response = self._client.get('groups')\n    return GroupList.from_json(response.text)",
    "nl": "Return the group list.\n\n:raise TenableIOApiException:  When API error is encountered.\n",
    "original_nl": "Return the group list.\n\n:raise TenableIOApiException:  When API error is encountered.\n:return: An instance of :class:`tenable_io.api.models.Grouplist`."
  },
  {
    "code": "def stringize(data):\n    for field in data.keys():\n        if (field == 'birth_date'):\n            data[field] = data[field].strftime('%d/%m/%Y')\n            data[field] = '/'.join([str(int(f)) for f in data[field].split('/')])\n        else:\n            data[field] = str(data[field])\n    return data",
    "nl": "Given data for a citizen where integers are really integers\nand such, make them all into strings.",
    "original_nl": "Given data for a citizen where integers are really integers\nand such, make them all into strings."
  },
  {
    "code": "@property\ndef signature(self):\n    assert (len(self.signatures) == 1)\n    return self.signatures[0]",
    "nl": "Get a singleton signature.\n\n:rtype: `signature_cls`",
    "original_nl": "Get a singleton signature.\n\n:rtype: `signature_cls`"
  },
  {
    "code": "def color_hex_to_tuple(hex: str):\n    return tuple((int(hex.replace('#', '')[i:(i + 2)], 16) for i in (0, 2, 4)))",
    "nl": "Convert a hex color value to an RGB tuple",
    "original_nl": "Convert a hex color value to an RGB tuple"
  },
  {
    "code": "@micropython.native\ndef p(c='X8'):\n    p = pyb.Pin(c, pyb.Pin.OUT_PP, pull=pyb.Pin.PULL_NONE)\n    p.low()\n    print(p.value())\n    print('time get a pin value')\n    u = pyb.micros()\n    p.value()\n    t = (pyb.micros() - u)\n    print(t)",
    "nl": "time get a pin value\n13",
    "original_nl": "time get a pin value\n13"
  },
  {
    "code": "@property\ndef floating(self):\n    return [a for a in self.attendees if (a.is_unassigned and (a.paid == c.PAID_BY_GROUP))]",
    "nl": "Returns the list of paid-by-group unassigned badges for this group.\nThis is a separate property from the \"Group.unassigned\" property\nbecause when automatically adding or removing unassigned badges, we\ncare specifically about paid-by-group badges rather than all unassigned\nbadges.",
    "original_nl": "Returns the list of paid-by-group unassigned badges for this group.\nThis is a separate property from the \"Group.unassigned\" property\nbecause when automatically adding or removing unassigned badges, we\ncare specifically about paid-by-group badges rather than all unassigned\nbadges."
  },
  {
    "code": "def _check_chol(self, chol):\n    chol = ops.convert_to_tensor(chol, name='chol')\n    if (not self.verify_pd):\n        return chol\n    shape = array_ops.shape(chol)\n    rank = array_ops.rank(chol)\n    is_matrix = check_ops.assert_rank_at_least(chol, 2)\n    is_square = check_ops.assert_equal(array_ops.gather(shape, (rank - 2)), array_ops.gather(shape, (rank - 1)))\n    deps = [is_matrix, is_square]\n    diag = array_ops.batch_matrix_diag_part(chol)\n    deps.append(check_ops.assert_positive(diag))\n    return control_flow_ops.with_dependencies(deps, chol)",
    "nl": "Verify that `chol` is proper.",
    "original_nl": "Verify that `chol` is proper."
  },
  {
    "code": "def computeRMS(v):\n    v = numpy.array(v)\n    assert (len(v) > 0)\n    return math.sqrt(((v ** 2).sum() / len(v)))",
    "nl": "The root mean square (RMS) of a list of values\n\nArgs:\n    `v` (array-like)\n        Values for which we compute the RMS.\n        ",
    "original_nl": "The root mean square (RMS) of a list of values\n\nArgs:\n    `v` (array-like)\n        Values for which we compute the RMS.\n        \nReturns:\n    The RMS of the values.\n\n>>> v = [1.2, 3.5, 6.8, 1.1]\n>>> numpy.allclose(computeRMS(v), 3.9096, atol=1e-4)\nTrue"
  },
  {
    "code": "def flushdb(self):\n    return self.__db.flushdb()",
    "nl": "Delete all keys in the current database",
    "original_nl": "Delete all keys in the current database"
  },
  {
    "code": "@atomic.action_timer('zaqar.create_queue')\ndef _queue_create(self, **kwargs):\n    name = self.generate_random_name()\n    return self.clients('zaqar').queue(name, **kwargs)",
    "nl": "Create a Zaqar queue with random name.\n\n",
    "original_nl": "Create a Zaqar queue with random name.\n\n:param kwargs: other optional parameters to create queues like\n               \"metadata\"\n:returns: Zaqar queue instance"
  },
  {
    "code": "def update(self):\n    self._logger.info('{\"event\":\"updating_updater\"}')\n    self._pull_products()",
    "nl": "Update should be called before any firmware loading is done, to ensure the\nmost up-to-date information is being used.",
    "original_nl": "Update should be called before any firmware loading is done, to ensure the\nmost up-to-date information is being used."
  },
  {
    "code": "def OnPaint(self, event):\n    dc = wx.BufferedPaintDC(self, self.buffer)",
    "nl": "Called when the window is exposed.",
    "original_nl": "Called when the window is exposed."
  },
  {
    "code": "def _include_paths_from_environ(env_prefix=''):\n    paths = os.environ.get((env_prefix + 'WSGI_AUTH_PATHS'))\n    if (not paths):\n        return []\n    return paths.split(';')",
    "nl": "Environment value via `/login;/register`",
    "original_nl": "Environment value via `/login;/register`"
  },
  {
    "code": "def test_NP_Mixture_Smoother(self):\n    mix = m_s.NP_Mixture_Smoother(self.e, self.b)\n    np.testing.assert_array_almost_equal(mix.r, np.array([0.10982278, 0.03445531, 0.11018404, 0.11018604]))\n    np.testing.assert_array_almost_equal(mix.category, np.array([1, 0, 1, 1]))\n    (left, right) = mix.getSeed()\n    np.testing.assert_array_almost_equal(left, np.array([0.5, 0.5]))\n    np.testing.assert_array_almost_equal(right, np.array([0.03333333, 0.15]))\n    d = mix.mixalg()\n    np.testing.assert_array_almost_equal(d['mix_den'], np.array([0.0, 0.0, 0.0, 0.0]))\n    np.testing.assert_array_almost_equal(d['gradient'], np.array([0.0]))\n    np.testing.assert_array_almost_equal(d['p'], np.array([1.0]))\n    np.testing.assert_array_almost_equal(d['grid'], np.array([11.27659574]))\n    self.assertEqual(d['k'], 1)\n    self.assertEqual(d['accuracy'], 1.0)\n    (left, right) = mix.getRateEstimates()\n    np.testing.assert_array_almost_equal(left, np.array([0.0911574, 0.0911574, 0.0911574, 0.0911574]))\n    np.testing.assert_array_almost_equal(right, np.array([1, 1, 1, 1]))",
    "nl": "Test the main class",
    "original_nl": "Test the main class"
  },
  {
    "code": "def gemset_copy(source, destination, runas=None):\n    return _rvm('gemset copy {0} {1}'.format(source, destination), runas=runas)",
    "nl": "Copy all gems from one gemset to another.\n\nsource\n    The name of the gemset to copy, complete with ruby version.\ndestination\n    The destination gemset.\nrunas : None\n    The user to run rvm as.\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' rvm.gemset_copy foobar bazquo",
    "original_nl": "Copy all gems from one gemset to another.\n\nsource\n    The name of the gemset to copy, complete with ruby version.\ndestination\n    The destination gemset.\nrunas : None\n    The user to run rvm as.\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' rvm.gemset_copy foobar bazquo"
  },
  {
    "code": "def attach_ide_drive(self, vm_name, path, ctrller_addr, drive_addr, drive_type=constants.IDE_DISK):\n    vm = self._lookup_vm_check(vm_name)\n    ctrller_path = self._get_vm_ide_controller(vm, ctrller_addr)\n    if (drive_type == constants.IDE_DISK):\n        res_sub_type = self._DISK_RES_SUB_TYPE\n    elif (drive_type == constants.IDE_DVD):\n        res_sub_type = self._DVD_RES_SUB_TYPE\n    drive = self._get_new_resource_setting_data(res_sub_type)\n    drive.Parent = ctrller_path\n    drive.Address = drive_addr\n    drive.AddressOnParent = drive_addr\n    new_resources = self._add_virt_resource(drive, vm.path_())\n    drive_path = new_resources[0]\n    if (drive_type == constants.IDE_DISK):\n        res_sub_type = self._IDE_DISK_RES_SUB_TYPE\n    elif (drive_type == constants.IDE_DVD):\n        res_sub_type = self._IDE_DVD_RES_SUB_TYPE\n    res = self._get_new_resource_setting_data(res_sub_type, self._STORAGE_ALLOC_SETTING_DATA_CLASS)\n    res.Parent = drive_path\n    res.HostResource = [path]\n    self._add_virt_resource(res, vm.path_())",
    "nl": "Create an IDE drive and attach it to the vm.",
    "original_nl": "Create an IDE drive and attach it to the vm."
  },
  {
    "code": "def chStatus(cid, status, rdb, r_server, logger):\n    success = False\n    cacheonly = False\n    try:\n        results = r.table('monitors').get(cid).update({\n            'status': status,\n        }).run(rdb)\n        if (results['replaced'] == 1):\n            success = True\n            cacheonly = False\n        else:\n            success = False\n    except (RqlDriverError, RqlRuntimeError) as e:\n        success = False\n        cacheonly = True\n        line = ('chstatus: RethinkDB is inaccessible cannot change status for %s' % cid)\n        logger.info(line)\n        line = ('chstatus: RethinkDB Error: %s' % e.message)\n        logger.info(line)\n    try:\n        r_server.set((('monitor:' + cid) + ':status'), status)\n        success = True\n    except:\n        line = ('chstatus: Redis is inaccessible cannot change status for %s' % cid)\n        logger.info(line)\n        success = False\n    return success",
    "nl": "This method will be called to change a users status in the db",
    "original_nl": "This method will be called to change a users status in the db"
  },
  {
    "code": "def __iter__(self):\n    cur = 0\n    while (cur < len(self)):\n        (yield self[cur])\n        cur += 1",
    "nl": "@rtype: float",
    "original_nl": "@rtype: float"
  },
  {
    "code": "def test_to_xml_element():\n    C = nuclide.Nuclide()\n    C.name = 'C'\n    C.half_life = 0.123\n    C.decay_modes = [nuclide.DecayTuple('beta-', 'B', 0.99), nuclide.DecayTuple('alpha', 'D', 0.01)]\n    C.reactions = [nuclide.ReactionTuple('fission', None, 200000000.0, 1.0), nuclide.ReactionTuple('(n,gamma)', 'A', 0.0, 1.0)]\n    C.yield_energies = [0.0253]\n    C.yield_data = {\n        0.0253: [('A', 0.0292737), ('B', 0.002566345)],\n    }\n    element = C.to_xml_element()\n    assert (element.get('half_life') == '0.123')\n    decay_elems = element.findall('decay')\n    assert (len(decay_elems) == 2)\n    assert (decay_elems[0].get('type') == 'beta-')\n    assert (decay_elems[0].get('target') == 'B')\n    assert (decay_elems[0].get('branching_ratio') == '0.99')\n    assert (decay_elems[1].get('type') == 'alpha')\n    assert (decay_elems[1].get('target') == 'D')\n    assert (decay_elems[1].get('branching_ratio') == '0.01')\n    rx_elems = element.findall('reaction')\n    assert (len(rx_elems) == 2)\n    assert (rx_elems[0].get('type') == 'fission')\n    assert (float(rx_elems[0].get('Q')) == 200000000.0)\n    assert (rx_elems[1].get('type') == '(n,gamma)')\n    assert (rx_elems[1].get('target') == 'A')\n    assert (float(rx_elems[1].get('Q')) == 0.0)\n    assert (element.find('neutron_fission_yields') is not None)",
    "nl": "Test writing nuclide data to an XML element.",
    "original_nl": "Test writing nuclide data to an XML element."
  },
  {
    "code": "def should_restart(self, config):\n    net_ids = set([net.id for net in config.networks if net.is_tenant_network])\n    try:\n        config_dict = json.load(open(CONF_PATH))\n    except:\n        return True\n    orchestrator_addr = config_dict.get('orchestrator_metadata_address')\n    orchestrator_port = config_dict.get('orchestrator_metadata_port')\n    return ((net_ids != set(config_dict.get('networks', {\n        \n    }).keys())) or (orchestrator_addr != config.metadata_address) or (orchestrator_port != config.metadata_port))",
    "nl": "This function determines if the networks have changed since <config>\nwas initialized.\n\n:type config: astara_router.models.Configuration\n",
    "original_nl": "This function determines if the networks have changed since <config>\nwas initialized.\n\n:type config: astara_router.models.Configuration\n:param config: An astara_router.models.Configuration object containing\n               the current configuration of the system's networks.\n:rtype: bool"
  },
  {
    "code": "@property\ndef current_buffer(self):\n    ui_control = self.current_control\n    if isinstance(ui_control, BufferControl):\n        return ui_control.buffer",
    "nl": "The currently focused :class:`~.Buffer` or `None`.",
    "original_nl": "The currently focused :class:`~.Buffer` or `None`."
  },
  {
    "code": "def download_file(self, url, data='', more_headers={\n    \n}):\n    (content, headers) = self.perform_request_next(url, data, more_headers, response_headers=['Content-Disposition'])\n    if (not content):\n        from api.exceptions import FailedDownloadingSubtitleBuffer\n        raise FailedDownloadingSubtitleBuffer(('Failed downloading: %s' % url))\n    file_name = ''\n    if ((not ('Content-Disposition' in headers)) or ('filename' not in headers['Content-Disposition'])):\n        logger.debug('Failed getting the file name for the download.')\n        splitted_url = url.rsplit('/', 1)\n        if (len(splitted_url) == 2):\n            file_name = splitted_url[1]\n    else:\n        file_name = utils.take_first(utils.get_regex_results(headers['Content-Disposition'], '(?<=filename\\\\=).*(?=$)'))\n        file_name = file_name.strip('\"\\'')\n    logger.debug(('Downloaded file name is: %s' % file_name))\n    return (file_name, content)",
    "nl": "Downloads the file from the given URL. Returns the name for the \ndownloaded file, and the file buffer itself.\n\nTries to extract the file name from the Content-Disposition header, \notherwise, the last portion of the URL is returned.",
    "original_nl": "Downloads the file from the given URL. Returns the name for the \ndownloaded file, and the file buffer itself.\n\nTries to extract the file name from the Content-Disposition header, \notherwise, the last portion of the URL is returned."
  },
  {
    "code": "def _build_graph(sess, tf_dtype):\n    x = tf.placeholder(tf_dtype, shape=[None, _tensor_size], name=_tensor_input_name)\n    _ = tf.reduce_max(x, axis=1, name=_tensor_output_name)",
    "nl": "Given a session (implicitly), adds nodes of computations\n\nIt takes a vector input, with `_tensor_size` columns and returns an float64 scalar.",
    "original_nl": "Given a session (implicitly), adds nodes of computations\n\nIt takes a vector input, with `_tensor_size` columns and returns an float64 scalar."
  },
  {
    "code": "def empty():\n    for f in range(6):\n        for row in range(3):\n            for col in range(3):\n                if ((row != 1) or (col != 1)):\n                    canvas.itemconfig(facelet_id[f][row][col], fill='grey')",
    "nl": "Removes the facelet colors except the center facelets colors.",
    "original_nl": "Removes the facelet colors except the center facelets colors."
  },
  {
    "code": "@staticmethod\ndef _is_a_match(frame_occurrence_part, official_frame_part):\n    if isinstance(official_frame_part['elem'], set):\n        return (frame_occurrence_part['elem'] in official_frame_part['elem'])\n    else:\n        return (frame_occurrence_part['elem'] == official_frame_part['elem'])",
    "nl": "Tell wether two elements can be considered as a match\n\nframe_occurrence_part is a seen element, while official_frame_elem can\ncontain a set of possible elements, such as prepositions",
    "original_nl": "Tell wether two elements can be considered as a match\n\nframe_occurrence_part is a seen element, while official_frame_elem can\ncontain a set of possible elements, such as prepositions"
  },
  {
    "code": "def callb(x):\n    callback(Wtrafoinv(x))",
    "nl": "Callback that displays the inverse wavelet transform of current iter.",
    "original_nl": "Callback that displays the inverse wavelet transform of current iter."
  },
  {
    "code": "@line_magic\ndef lprun(self, parameter_s=''):\n    opts_def = Struct(D=[''], T=[''], f=[], m=[], u=None)\n    parameter_s = parameter_s.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n    (opts, arg_str) = self.parse_options(parameter_s, 'rsf:m:D:T:u:', list_all=True)\n    opts.merge(opts_def)\n    global_ns = self.shell.user_global_ns\n    local_ns = self.shell.user_ns\n    funcs = []\n    for name in opts.f:\n        try:\n            funcs.append(eval(name, global_ns, local_ns))\n        except Exception as e:\n            raise UsageError(('Could not find function %r.\\n%s: %s' % (name, e.__class__.__name__, e)))\n    profile = LineProfiler(*funcs)\n    for modname in opts.m:\n        try:\n            mod = __import__(modname, fromlist=[''])\n            profile.add_module(mod)\n        except Exception as e:\n            raise UsageError(('Could not find module %r.\\n%s: %s' % (modname, e.__class__.__name__, e)))\n    if (opts.u is not None):\n        try:\n            output_unit = float(opts.u[0])\n        except Exception as e:\n            raise TypeError('Timer unit setting must be a float.')\n    else:\n        output_unit = None\n    if PY3:\n        import builtins\n    else:\n        import __builtin__ as builtins\n    if ('profile' in builtins.__dict__):\n        had_profile = True\n        old_profile = builtins.__dict__['profile']\n    else:\n        had_profile = False\n        old_profile = None\n    builtins.__dict__['profile'] = profile\n    try:\n        try:\n            profile.runctx(arg_str, global_ns, local_ns)\n            message = ''\n        except SystemExit:\n            message = '*** SystemExit exception caught in code being profiled.'\n        except KeyboardInterrupt:\n            message = '*** KeyboardInterrupt exception caught in code being profiled.'\n    finally:\n        if had_profile:\n            builtins.__dict__['profile'] = old_profile\n    stdout_trap = StringIO()\n    profile.print_stats(stdout_trap, output_unit=output_unit, stripzeros=('s' in opts))\n    output = stdout_trap.getvalue()\n    output = output.rstrip()\n    page(output)\n    print(message, end='')\n    dump_file = opts.D[0]\n    if dump_file:\n        profile.dump_stats(dump_file)\n        print(('\\n*** Profile stats pickled to file %r. %s' % (dump_file, message)))\n    text_file = opts.T[0]\n    if text_file:\n        pfile = open(text_file, 'w')\n        pfile.write(output)\n        pfile.close()\n        print(('\\n*** Profile printout saved to text file %r. %s' % (text_file, message)))\n    return_value = None\n    if ('r' in opts):\n        return_value = profile\n    return return_value",
    "nl": "Execute a statement under the line-by-line profiler from the\nline_profiler module.\n\nUsage:\n  %lprun -f func1 -f func2 <statement>\n\nThe given statement (which doesn't require quote marks) is run via the\nLineProfiler. Profiling is enabled for the functions specified by the -f\noptions. The statistics will be shown side-by-side with the code through the\npager once the statement has completed.\n\nOptions:\n\n-f <function>: LineProfiler only profiles functions and methods it is told\nto profile.  This option tells the profiler about these functions. Multiple\n-f options may be used. The argument may be any expression that gives\na Python function or method object. However, one must be careful to avoid\nspaces that may confuse the option parser.\n\n-m <module>: Get all the functions/methods in a module\n\nOne or more -f or -m options are required to get any useful results.\n\n-D <filename>: dump the raw statistics out to a pickle file on disk. The\nusual extension for this is \".lprof\". These statistics may be viewed later\nby running line_profiler.py as a script.\n\n-T <filename>: dump the text-formatted statistics with the code side-by-side\nout to a text file.\n\n-r: return the LineProfiler object after it has completed profiling.\n\n-s: strip out all entries from the print-out that have zeros.\n\n-u: specify time unit for the print-out in seconds.",
    "original_nl": "Execute a statement under the line-by-line profiler from the\nline_profiler module.\n\nUsage:\n  %lprun -f func1 -f func2 <statement>\n\nThe given statement (which doesn't require quote marks) is run via the\nLineProfiler. Profiling is enabled for the functions specified by the -f\noptions. The statistics will be shown side-by-side with the code through the\npager once the statement has completed.\n\nOptions:\n\n-f <function>: LineProfiler only profiles functions and methods it is told\nto profile.  This option tells the profiler about these functions. Multiple\n-f options may be used. The argument may be any expression that gives\na Python function or method object. However, one must be careful to avoid\nspaces that may confuse the option parser.\n\n-m <module>: Get all the functions/methods in a module\n\nOne or more -f or -m options are required to get any useful results.\n\n-D <filename>: dump the raw statistics out to a pickle file on disk. The\nusual extension for this is \".lprof\". These statistics may be viewed later\nby running line_profiler.py as a script.\n\n-T <filename>: dump the text-formatted statistics with the code side-by-side\nout to a text file.\n\n-r: return the LineProfiler object after it has completed profiling.\n\n-s: strip out all entries from the print-out that have zeros.\n\n-u: specify time unit for the print-out in seconds."
  },
  {
    "code": "def check_permissions(self, group_name, permission_codenames):\n    self.assertEqual(set(permission_codenames), {p.codename for p in Group.objects.get(name=group_name).permissions.all()})",
    "nl": "DRY helper.",
    "original_nl": "DRY helper."
  },
  {
    "code": "def test_json(self):\n    self.assertEqual(self.prop.from_json(self.prop.to_json()), self.prop)",
    "nl": "Test json related methods.",
    "original_nl": "Test json related methods."
  },
  {
    "code": "@ops.RegisterGradient('MatrixInverse')\ndef _MatrixInverseGrad(op, grad):\n    ainv = op.outputs[0]\n    return (- math_ops.matmul(ainv, math_ops.matmul(grad, ainv, transpose_b=True), transpose_a=True))",
    "nl": "Gradient for MatrixInverse.",
    "original_nl": "Gradient for MatrixInverse."
  },
  {
    "code": "def __del__(self):\n    try:\n        self.cancel()\n    except AttributeError:\n        pass",
    "nl": "Calls `cancel()`.",
    "original_nl": "Calls `cancel()`."
  },
  {
    "code": "def __init__(self, contents, **kwargs):\n    super(BokehPlotBlock, self).__init__(**kwargs)\n    if (not isinstance(contents, BokehFigure)):\n        raise ValueError('Expected bokeh.plotting.figure.Figure type but got %s', type(contents))\n    self._contents = file_html(contents, INLINE, 'test')",
    "nl": "Writes out the content as raw text or HTML.\n\n",
    "original_nl": "Writes out the content as raw text or HTML.\n\n:param contents: Bokeh plotting figure.\n:param kwargs: Optional styling arguments. The `style` keyword argument has special\n               meaning in that it allows styling to be grouped as one argument.\n               It is also useful in case a styling parameter name clashes with a standard\n               block parameter."
  },
  {
    "code": "def register_definition_helper(self, func):\n    self._definition_helpers.append(func)",
    "nl": "Register a new definition helper. The helper **must** meet the following conditions:\n\n- Receive the `APISpec` instance as the first argument.\n- Receive the definition `name` as the second argument.\n- Include ``**kwargs`` in its signature.\n- Return a `dict` representation of the definition's Schema object.\n\nThe helper may define any named arguments after the `name` argument.\n``kwargs`` will include (among other things):\n- definition (dict): current state of the definition\n\nhttps://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#definitionsObject\n\n",
    "original_nl": "Register a new definition helper. The helper **must** meet the following conditions:\n\n- Receive the `APISpec` instance as the first argument.\n- Receive the definition `name` as the second argument.\n- Include ``**kwargs`` in its signature.\n- Return a `dict` representation of the definition's Schema object.\n\nThe helper may define any named arguments after the `name` argument.\n``kwargs`` will include (among other things):\n- definition (dict): current state of the definition\n\nhttps://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#definitionsObject\n\n:param callable func: The definition helper function."
  },
  {
    "code": "def create_resource(options):\n    deserializer = wsgi.JSONRequestDeserializer()\n    serializer = serializers.JSONResponseSerializer()\n    return wsgi.Resource(ActionController(options), deserializer, serializer)",
    "nl": "Actions action factory method.",
    "original_nl": "Actions action factory method."
  },
  {
    "code": "def __init__(self, iterations, track_time, stream, title, monitor, update_interval=None):\n    self.cnt = 0\n    self.title = title\n    self.max_iter = float(iterations)\n    self.track = track_time\n    self.start = time.time()\n    self.end = None\n    self.item_id = None\n    self.eta = None\n    self.total_time = 0.0\n    self.last_time = self.start\n    self.monitor = monitor\n    self.stream = stream\n    self.active = True\n    self._stream_out = self._no_stream\n    self._stream_flush = self._no_stream\n    self._check_stream()\n    self._print_title()\n    self.update_interval = update_interval\n    self._cached_output = ''\n    sys.stdout.flush()\n    sys.stderr.flush()\n    if monitor:\n        if (not psutil_import):\n            raise ValueError('psutil package is required when using the `monitor` option.')\n        else:\n            self.process = psutil.Process()\n    if self.track:\n        self.eta = 1",
    "nl": "Initializes tracking object.",
    "original_nl": "Initializes tracking object."
  },
  {
    "code": "def test_creating_tools_edition_panel(exopy_qtbot, edition_view, dialog_sleep):\n    edition_view.show()\n    wait_for_window_displayed(exopy_qtbot, edition_view)\n    exopy_qtbot.wait(dialog_sleep)\n    ed = edition_view.widget.dock_widget().widgets()[0]\n    btn = ed.widgets()[4]\n    btn.clicked = True\n\n    def assert_created():\n        assert (len(edition_view.area.dock_items()) == 2)\n    exopy_qtbot.wait_until(assert_created)",
    "nl": "Test creating the tool edition panel using the button.",
    "original_nl": "Test creating the tool edition panel using the button."
  },
  {
    "code": "def feed_words(self, text):\n    for word in re.findall('(\\\\w+)', text, re.UNICODE):\n        word = word.lower()\n        count = self.words.get(word, 0)\n        count += 1\n        self.words[word] = count",
    "nl": "Add new words using the provided text.\n\nEach word in this text will be added to the list of words for\na future auto-completion.",
    "original_nl": "Add new words using the provided text.\n\nEach word in this text will be added to the list of words for\na future auto-completion."
  },
  {
    "code": "def test_Trace_record():\n    t = Trace(variables=['step', 'r'])\n    t.record(step=1, r=[1.0, 2.0])\n    t.record(step=2, r=[15.0, 2.0])\n    t.record(step=3, r=[4.0, 2.0])\n    assert (len(t['step']) == 3)\n    assert (len(t['r']) == 3)\n    t.record(step=4)\n    assert (len(t['r']) == 3)\n    assert (len(t['step']) == 4)\n    assert_raises(KeyError, t.record, step=1, g=[1.0, 2.0, 3.0])",
    "nl": "Test record function of Trace",
    "original_nl": "Test record function of Trace"
  },
  {
    "code": "def close(self):\n    logger.info('Closing open sockets ...')\n    for scheduler in self.scheduler_pool:\n        scheduler.close()\n    logger.info('Worker was closed successfully ...')",
    "nl": "Unregister open sockets from poller objects and close them.",
    "original_nl": "Unregister open sockets from poller objects and close them."
  },
  {
    "code": "def get_brick_lookup_options(self):\n    lookup = self.kwargs[(self.brick_lookup_url_kwarg or self.lookup_url_kwarg or self.lookup_field)]\n    return dict(part_name=lookup)",
    "nl": "Parses kwargs and returns corresponding querying keyword arguments.",
    "original_nl": "Parses kwargs and returns corresponding querying keyword arguments."
  },
  {
    "code": "def indexBaseTick(na, nb, pagesize, pna):\n    indlista = range(na)\n    indlistb = range(nb)\n    pnb = (pagesize - pna)\n    ma = (na - pna)\n    mb = (nb - pnb)\n    (npagea, npageb) = (0, 0)\n    if (ma > 0):\n        npagea = ((ma / pagesize) + sign((ma % pagesize)))\n    if (mb > 0):\n        npageb = ((mb / pagesize) + sign((mb % pagesize)))\n    ipages = range((- npageb), (npagea + 1))\n    yindex = {\n        \n    }\n    ybases = {\n        \n    }\n    yticks = {\n        \n    }\n    ia1 = min(pna, na)\n    ib0 = max(0, (nb - pnb))\n    inda = range(ia1)\n    indb = range(ib0, nb)\n    yindex[0] = [inda, indb]\n    for ipage in range(1, (npagea + 1)):\n        i1 = (pna + (ipage * pagesize))\n        i0 = (i1 - pagesize)\n        inda = range(i0, min(i1, na))\n        yindex[ipage] = [inda, []]\n    for ipage in range((- npageb), 0):\n        i1 = ((ib0 + (ipage * pagesize)) + pagesize)\n        i0 = max(0, (i1 - pagesize))\n        indb = range(i0, i1)\n        yindex[ipage] = [[], indb]\n    for ipage in range((- npageb), (npagea + 1)):\n        ybases[ipage] = [[], []]\n        yticks[ipage] = [[], []]\n    for ipage in range(0, (npagea + 1)):\n        ybases[ipage][0] = [((- 1) - ind) for ind in yindex[ipage][0]]\n        yticks[ipage][0] = [(1 + ind) for ind in yindex[ipage][0]]\n    for ipage in range((- npageb), 1):\n        ybases[ipage][1] = [(nb - ind) for ind in yindex[ipage][1]]\n        yticks[ipage][1] = [(ind - nb) for ind in yindex[ipage][1]]\n    return (ipages, yindex, ybases, yticks)",
    "nl": "Indexing for page navigation with two lists of length na and nb.\n\n Example:\n         list b (nb=5)    list a (na=11)\n         [ 0, 1, 2, 3, 4] [0,  1, 2, 3, 4, 5, 6, 7, 8,  9, 10] <-- yindex\n         [ 5, 4, 3, 2, 1] [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11] <-- ybases\n         [-5,-4,-3,-2,-1] [1,  2, 3, 4, 5, 6, 7, 8, 9, 10, 11] <-- yticks\n--page -1] [----page 0----] [---page 1---] [---page 2--- \n              [pnb=2] [pna=3 ]                   \n\n         yindex for na and nb:\n         {-1: [[], [0, 1, 2]],\n          0: [[0, 1, 2], [3, 4]],\n          1: [[3, 4, 5, 6, 7], []],\n          2: [[8, 9, 10, 11, 12], []]}\n\n         yybase:\n         {-1: [[], [5, 4, 3]],\n          0: [[-1, -2, -3], [2, 1]],\n          1: [[-4, -5, -6, -7, -8], []],\n          2: [[-9, -10, -11, -12, -13], []]}\n\n         yticks:\n         {-1: [[], [-5, -4, -3]],\n          0: [[1, 2, 3], [-2, -1]],\n          1: [[4, 5, 6, 7, 8], []],\n          2: [[9, 10, 11, 12, 13], []]}",
    "original_nl": "Indexing for page navigation with two lists of length na and nb.\n\n Example:\n         list b (nb=5)    list a (na=11)\n         [ 0, 1, 2, 3, 4] [0,  1, 2, 3, 4, 5, 6, 7, 8,  9, 10] <-- yindex\n         [ 5, 4, 3, 2, 1] [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11] <-- ybases\n         [-5,-4,-3,-2,-1] [1,  2, 3, 4, 5, 6, 7, 8, 9, 10, 11] <-- yticks\n--page -1] [----page 0----] [---page 1---] [---page 2--- \n              [pnb=2] [pna=3 ]                   \n\n         yindex for na and nb:\n         {-1: [[], [0, 1, 2]],\n          0: [[0, 1, 2], [3, 4]],\n          1: [[3, 4, 5, 6, 7], []],\n          2: [[8, 9, 10, 11, 12], []]}\n\n         yybase:\n         {-1: [[], [5, 4, 3]],\n          0: [[-1, -2, -3], [2, 1]],\n          1: [[-4, -5, -6, -7, -8], []],\n          2: [[-9, -10, -11, -12, -13], []]}\n\n         yticks:\n         {-1: [[], [-5, -4, -3]],\n          0: [[1, 2, 3], [-2, -1]],\n          1: [[4, 5, 6, 7, 8], []],\n          2: [[9, 10, 11, 12, 13], []]}"
  },
  {
    "code": "def find_seamicro15k_servers(ip, username, password, power_control):\n    api_versions = select_seamicro15k_api_version(power_control)\n    for version in api_versions:\n        servers = get_seamicro15k_servers(version, ip, username, password)\n        if (servers is not None):\n            return servers\n    raise SeaMicroError('Failure to retrieve servers.')",
    "nl": "Returns the list of servers, using the latest supported api version.",
    "original_nl": "Returns the list of servers, using the latest supported api version."
  },
  {
    "code": "def _get_strategy_function(self, locator):\n    locator_function = STRATEGIES.get(locator.lower().replace(' ', ''), None)\n    return (getattr(self, locator_function) if locator_function else None)",
    "nl": "Get the function for the provided locator",
    "original_nl": "Get the function for the provided locator"
  },
  {
    "code": "def _sfn(l, mask, myrad, bcast_var):\n    clf = bcast_var[2]\n    data = l[0][mask, :].T\n    skf = model_selection.StratifiedKFold(n_splits=bcast_var[1], shuffle=False)\n    accuracy = np.mean(model_selection.cross_val_score(clf, data, y=bcast_var[0], cv=skf, n_jobs=1))\n    return accuracy",
    "nl": "Score classifier on searchlight data using cross-validation.\n\nThe classifier is in `bcast_var[2]`. The labels are in `bast_var[0]`. The\nnumber of cross-validation folds is in `bast_var[1].",
    "original_nl": "Score classifier on searchlight data using cross-validation.\n\nThe classifier is in `bcast_var[2]`. The labels are in `bast_var[0]`. The\nnumber of cross-validation folds is in `bast_var[1]."
  },
  {
    "code": "def before_start(self):\n    pass",
    "nl": "Before the process starts",
    "original_nl": "Before the process starts"
  },
  {
    "code": "def enumerate_fresh_vars_outside(fvs):\n    next_i = 0\n    while True:\n        next_i += 1\n        next_var = hslowlevel.HsType(('a' + str(next_i)))\n        if (next_var not in fvs):\n            (yield next_var)",
    "nl": "Given a set fvs of free variables (represented as strings) that\nare deemed already 'used', return a Python generater which yields\nan infinite succession of legal Haskell free variables which are\nnot 'used'.",
    "original_nl": "Given a set fvs of free variables (represented as strings) that\nare deemed already 'used', return a Python generater which yields\nan infinite succession of legal Haskell free variables which are\nnot 'used'."
  },
  {
    "code": "def error_message(self, error):\n    if (error == QProcess.FailedToStart):\n        self.message(_('Could not start {program}.\\nPlease check path and permissions.').format(program=self.command[0]), FAILURE)\n    elif (error == QProcess.ReadError):\n        self.message(_('Could not read from the process.'), FAILURE)\n    elif (self._process.state() == QProcess.NotRunning):\n        self.message(_('An unknown error occurred.'), FAILURE)",
    "nl": "Called when there is an error (by _error()).\n\nOutputs a message describing the given QProcess.Error.",
    "original_nl": "Called when there is an error (by _error()).\n\nOutputs a message describing the given QProcess.Error."
  },
  {
    "code": "def keys(self, sort=True, limit=100):\n    return [key[0] for key in self.key_details(sort=sort, limit=limit)]",
    "nl": "Return a list of keys in use",
    "original_nl": "Return a list of keys in use"
  },
  {
    "code": "@property\ndef AUTHORIZATION_URL(self):\n    url_root = self.setting('PUBLIC_URL_ROOT')\n    if (not url_root):\n        url_root = self.setting('URL_ROOT')\n    return '{}/authorize/'.format(url_root)",
    "nl": "URL of the auth provider's authorization endpoint.",
    "original_nl": "URL of the auth provider's authorization endpoint."
  },
  {
    "code": "def reaction_formula(reaction, compound_formula):\n\n    def multiply_formula(compound_list):\n        for (compound, count) in compound_list:\n            (yield (count * compound_formula[compound.name]))\n    for (compound, _) in reaction.compounds:\n        if (compound.name not in compound_formula):\n            return None\n    else:\n        left_form = reduce(operator.or_, multiply_formula(reaction.left), Formula())\n        right_form = reduce(operator.or_, multiply_formula(reaction.right), Formula())\n    return (left_form, right_form)",
    "nl": "Calculate formula compositions for both sides of the specified reaction.\n\nIf the compounds in the reaction all have formula, then calculate and",
    "original_nl": "Calculate formula compositions for both sides of the specified reaction.\n\nIf the compounds in the reaction all have formula, then calculate and\nreturn the chemical compositions for both sides, otherwise return `None`.\n\nArgs:\n    reaction: :class:`psamm.reaction.Reaction`.\n    compound_formula: a map from compound id to formula."
  },
  {
    "code": "def get_rms_map_qual(var):\n    return _safe_single_attr(var.INFO.get('MQ'))",
    "nl": "Return the RMS mapping quality,\nor None if it isn't present in the VCF.",
    "original_nl": "Return the RMS mapping quality,\nor None if it isn't present in the VCF."
  },
  {
    "code": "def make_venv(self, venv_dir):\n    lgr.debug('Creating virtualenv in {0}'.format(venv_dir))\n    return sh.virtualenv(venv_dir)",
    "nl": "creates a virtualenv\n\n",
    "original_nl": "creates a virtualenv\n\n:param string venv_dir: venv path to create"
  },
  {
    "code": "def create_workbook_with_sheet(name):\n    book = xlwt.Workbook()\n    valid_name = re.sub('[^\\\\.0-9a-zA-Z]+', '', os.path.basename(name))\n    sheet1 = book.add_sheet(valid_name)\n    return (book, sheet1)",
    "nl": "Removes non-alpha-numerical values in name.",
    "original_nl": "Removes non-alpha-numerical values in name."
  },
  {
    "code": "def finish(self):\n    return self.find_and_kill_process('tor -f /etc/tor/torrc-raspap', self.interface)",
    "nl": "Shutdown TOR\n",
    "original_nl": "Shutdown TOR\n:return:"
  },
  {
    "code": "def expectScreen(self, filename, maxrms=0):\n    log.debug('expectScreen %s', filename)\n    return self._expectFramebuffer(filename, 0, 0, maxrms)",
    "nl": "Wait until the display matches a target image\n\nfilename: an image file to read and compare against\nmaxrms: the maximum root mean square between histograms of the\n        screen and target image",
    "original_nl": "Wait until the display matches a target image\n\nfilename: an image file to read and compare against\nmaxrms: the maximum root mean square between histograms of the\n        screen and target image"
  },
  {
    "code": "def compareVersion(ver1, ver2):\n    ver1 = ver1.split('.')\n    ver2 = ver2.split('.')\n    for i in range(3):\n        if (int(ver1[i]) > int(ver2[i])):\n            return 1\n        elif (int(ver1[i]) < int(ver2[i])):\n            return 2\n    return 0",
    "nl": "Se compara entre dos versiones y se retorna el ganador\n",
    "original_nl": "Se compara entre dos versiones y se retorna el ganador\n:param ver1: Versi\u00f3n actual\n:param ver2: Versi\u00f3n de sistema\n:return: ver1 or ver2"
  },
  {
    "code": "def test_link(self):\n    expected = 'www.example.com'\n    self.assertEqual(expected, md.gen_link('www.example.com'))",
    "nl": "Simple link generation where only the URL is provided.",
    "original_nl": "Simple link generation where only the URL is provided."
  },
  {
    "code": "def grab_files(directory):\n    if (not os.path.isdir(directory)):\n        return\n        (yield)\n    else:\n        for name in os.listdir(directory):\n            full_path = os.path.join(directory, name)\n            if os.path.isdir(full_path):\n                for entry in grab_files(full_path):\n                    (yield entry)\n            elif os.path.isfile(full_path):\n                (yield full_path)",
    "nl": "Generator that returns all files in a directory.",
    "original_nl": "Generator that returns all files in a directory."
  },
  {
    "code": "def unregister(self, collector):\n    if self.is_registered:\n        driver_id = self.id\n        try:\n            del collector.contributions[driver_id]\n        except KeyError:\n            pass\n        self.is_registered = False",
    "nl": "Remove contributed infos from the collector.",
    "original_nl": "Remove contributed infos from the collector."
  },
  {
    "code": "def handle_msg(self, msg_text, _chan, nick):\n    lol_regexp = '[lI1]+[o0u]+[lI1]+z?'\n    lulz = len(re.findall(lol_regexp, msg_text, flags=re.IGNORECASE))\n    if ((lulz > 0) and (not msg_text.startswith('!'))):\n        current_ts = TimeSlice()\n        if (current_ts != self.lol_rate[0]):\n            self.lol_rate.insert(0, current_ts)\n        if (len(self.lol_rate) > self.bot.lolRateDepth):\n            self.lol_rate.pop()\n        self.lol_rate[0].lol(nick, lulz)",
    "nl": "If msg_text matches the lol regexp,\nincrement the lolness for the current timeslice.",
    "original_nl": "If msg_text matches the lol regexp,\nincrement the lolness for the current timeslice."
  },
  {
    "code": "def check_network_connection(server='www.baidu.com'):\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Checking network connection to server '%s'...\", server)\n    try:\n        host = socket.gethostbyname(server)\n        socket.create_connection((host, 80), 2)\n    except Exception:\n        logger.debug('Network connection not working')\n        return False\n    else:\n        logger.debug('Network connection working')\n        return True",
    "nl": "@Brief:\n    Checks if pi can connect a network server.\n\n@Arguments:\n    server -- (optional) the server to connect with (Default:\n              \"www.baidu.com\")\n\n@Returns:\n    True or False",
    "original_nl": "@Brief:\n    Checks if pi can connect a network server.\n\n@Arguments:\n    server -- (optional) the server to connect with (Default:\n              \"www.baidu.com\")\n\n@Returns:\n    True or False"
  },
  {
    "code": "@property\ndef measure_number(self):\n    return self._measure_number",
    "nl": "The scalar expression involved in the definition of\nthis DyadicMul.",
    "original_nl": "The scalar expression involved in the definition of\nthis DyadicMul."
  },
  {
    "code": "def __getattr__(self, item):\n    return getattr(self.desc, item)",
    "nl": "Redirects unknown attribute lookups to the wrapped descriptor\n",
    "original_nl": "Redirects unknown attribute lookups to the wrapped descriptor\n:param item: attribute being looked up"
  },
  {
    "code": "def _get_item_from_module(module_name, item_name):\n    try:\n        module = __import__(module_name, fromlist=[item_name])\n        klass = getattr(module, item_name)\n    except ImportError as error:\n        message = 'Module \"{modulename}\" could not be loaded: {e}'\n        raise Exception(message.format(modulename=module_name, e=error))\n    except AttributeError as error:\n        message = 'No item \"{itemname}\" in module \"{modulename}\": {e}'\n        raise Exception(message.format(modulename=module_name, itemname=item_name, e=error))\n    return klass",
    "nl": "Load classes/modules/functions/... from given config",
    "original_nl": "Load classes/modules/functions/... from given config"
  },
  {
    "code": "@QtCore.pyqtSlot(QgsVectorLayer)\ndef _vlayer_modified(self, vlayer):\n    logger.debug('Vector layer {v} was modified'.format(v=vlayer.id()))\n    idx = self.combox_vector.findData(vlayer.id())\n    if (idx == self.combox_vector.currentIndex()):\n        self._vlayer_changed(idx)",
    "nl": "Push an update when a vector layer has been modified\n\nNote that this only takes action if `vlayer` modified is one currently\nselected.\n\nArgs:\n  vlayer (QgsVectorLayer): vector layer modified",
    "original_nl": "Push an update when a vector layer has been modified\n\nNote that this only takes action if `vlayer` modified is one currently\nselected.\n\nArgs:\n  vlayer (QgsVectorLayer): vector layer modified"
  },
  {
    "code": "def world_to_pixel(self, x, y):\n    sx = self.scale\n    sy = (- sx)\n    (ogx, ogy) = self.__offset\n    return (((x + ogx) / sx), ((y + ogy) / sy))",
    "nl": "Returns the pixel of the world coordinate (x,y).\n\nEg.\n>>> view = WorldToViewTransform([0,0,100,100],500,500)\n>>> view.world_to_pixel(0,0)\n(0.0, 500.0)\n>>> view.world_to_pixel(100,100)\n(500.0, -0.0)",
    "original_nl": "Returns the pixel of the world coordinate (x,y).\n\nEg.\n>>> view = WorldToViewTransform([0,0,100,100],500,500)\n>>> view.world_to_pixel(0,0)\n(0.0, 500.0)\n>>> view.world_to_pixel(100,100)\n(500.0, -0.0)"
  },
  {
    "code": "@classmethod\ndef lookup(cls, value):\n    try:\n        return cls(value)\n    except ValueError:\n        return cls[value]",
    "nl": "Look up an enumeration by name or value.",
    "original_nl": "Look up an enumeration by name or value."
  },
  {
    "code": "def getDirAppSocket():\n    global SCHEME\n    global APP_HOST\n    global DIR_PORT\n    app_socket = ('%s://%s:%d' % (SCHEME, APP_HOST, DIR_PORT))\n    return app_socket",
    "nl": "Same as getAppSocket() but for  directory service.",
    "original_nl": "Same as getAppSocket() but for  directory service."
  },
  {
    "code": "def get_logger(name=None, more_frames=0):\n    if (name is None):\n        stack = inspect.stack()[(1 + more_frames)]\n        name = stack[1]\n        if name.endswith('.py'):\n            name = name[0:(- 3)]\n        elif name.endswith('.pyc'):\n            name = name[0:(- 4)]\n        if name.startswith(PATH):\n            name = name[len(PATH):]\n        elif name.startswith(EXT_PATH):\n            name = name[len(EXT_PATH):]\n        name = name.replace('/', '.').replace('\\\\', '.')\n        if (name.find('.') != (- 1)):\n            toks = name.split('.')\n            if (len(toks) >= 2):\n                if (toks[(- 1)] == toks[(- 2)]):\n                    del toks[(- 1)]\n                    name = '.'.join(toks)\n        if name.startswith('ext.'):\n            name = name.split('ext.', 1)[1]\n        if name.endswith('.__init__'):\n            name = name.rsplit('.__init__', 1)[0]\n    return logging.getLogger(name)",
    "nl": "Logger factory.",
    "original_nl": "Logger factory."
  },
  {
    "code": "@pytest.mark.parametrize('kernel', KERNELS)\ndef test_centered_makekernel(self, kernel):\n    shape = kernel.array.shape\n    x = np.zeros(shape)\n    xslice = [slice((sh // 2), ((sh // 2) + 1)) for sh in shape]\n    x[xslice] = 1.0\n    c2 = convolve_fft(x, kernel, boundary='fill')\n    c1 = convolve(x, kernel, boundary='fill')\n    assert_almost_equal(c1, c2, decimal=12)",
    "nl": "Test smoothing of an image with a single positive pixel",
    "original_nl": "Test smoothing of an image with a single positive pixel"
  },
  {
    "code": "def test_push_pop1(self):\n    q = self.queue()\n    q.push('a')\n    q.push('b')\n    q.push('c')\n    self.assertEqual(q.pop(), 'a')\n    self.assertEqual(q.pop(), 'b')\n    self.assertEqual(q.pop(), 'c')\n    self.assertEqual(q.pop(), None)",
    "nl": "Basic push/pop test",
    "original_nl": "Basic push/pop test"
  },
  {
    "code": "def converter_obj(expected, optional=False):\n\n    def converter(value):\n        if (optional and (value is None)):\n            return None\n        if isinstance(value, expected):\n            return value\n        elif isinstance(value, dict):\n            return expected(**value)\n        else:\n            raise TypeError(('%r is not of type %s or dict' % (value, expected)))\n    return converter",
    "nl": "Convert the given value to an object of type `expected`.",
    "original_nl": "Convert the given value to an object of type `expected`."
  },
  {
    "code": "def is_protected_type(obj):\n    import Decimal\n    import datetime\n    return isinstance(obj, (integer_types + (type(None), float, Decimal, datetime.datetime, datetime.date, datetime.time)))",
    "nl": "Determine if the object instance is of a protected type.\n\nObjects of protected types are preserved as-is when passed to\nforce_text(strings_only=True).",
    "original_nl": "Determine if the object instance is of a protected type.\n\nObjects of protected types are preserved as-is when passed to\nforce_text(strings_only=True)."
  },
  {
    "code": "def setPathSearch(self, path_search):\n    self.settingsStore('path_search', qvariant_converter.convertBool(path_search))",
    "nl": "Stores the path search value in the QSettings.",
    "original_nl": "Stores the path search value in the QSettings."
  },
  {
    "code": "def set_logger(name, level='INFO', fmt=None, datefmt=None, propagate=1, remove_handlers=False):\n    logger = logging.getLogger(name)\n    logger.setLevel(getattr(logging, level))\n    logger.propagate = propagate\n    if remove_handlers:\n        logger.handlers = []\n        return\n    handler = None\n    for h in logger.handlers:\n        if isinstance(h, logging.StreamHandler):\n            handler = h\n            break\n    if (not handler):\n        handler = logging.StreamHandler()\n        logger.addHandler(handler)\n    formatter_kwgs = {\n        \n    }\n    for i in ('fmt', 'datefmt'):\n        if (locals()[i] is not None):\n            formatter_kwgs[i] = locals()[i]\n    handler.setFormatter(BaseFormatter(**formatter_kwgs))",
    "nl": "This function will clear the previous handlers and set only one handler,\nwhich will only be StreamHandler for the logger.\n\nThis function is designed to be able to called multiple times in a context.\n\nNote that if a logger has no handlers, it will be added a handler automatically when it is used.",
    "original_nl": "This function will clear the previous handlers and set only one handler,\nwhich will only be StreamHandler for the logger.\n\nThis function is designed to be able to called multiple times in a context.\n\nNote that if a logger has no handlers, it will be added a handler automatically when it is used."
  },
  {
    "code": "@httpretty.activate\n@patch('courses.presenters.engagement.CourseEngagementVideoPresenter.sections', Mock(return_value=dict()))\ndef test_missing_sections(self):\n    self.mock_course_detail(CourseSamples.DEMO_COURSE_ID)\n    response = self.client.get(self.path(course_id=CourseSamples.DEMO_COURSE_ID))\n    self.assertEqual(response.status_code, 200)",
    "nl": "Every video page will use sections and will return 200 if sections aren't available.",
    "original_nl": "Every video page will use sections and will return 200 if sections aren't available."
  },
  {
    "code": "@mock.patch('treadmill.cli.admin.cloud.vpc.VPC')\ndef test_list_all_vpc(self, vpc_mock):\n    result = self.runner.invoke(self.configure_cli, ['--domain=treadmill.org', 'list', 'vpc'], obj={\n        \n    })\n    self.assertEqual(result.exit_code, 0)\n    vpc_mock.all.assert_called_once()",
    "nl": "Test cloud list all vpc",
    "original_nl": "Test cloud list all vpc"
  },
  {
    "code": "def search(self, query, size=1000, recent=False, days=0):\n    es = pyelasticsearch.ElasticSearch(self._url)\n    args = {\n        'size': size,\n    }\n    if (recent or days):\n        datefmt = self._indexfmt\n        now = datetime.datetime.utcnow()\n        indexes = []\n        latest_index = now.strftime(datefmt)\n        if self._is_valid_index(es, latest_index):\n            indexes.append(latest_index)\n        if recent:\n            lasthr = (now - datetime.timedelta(hours=1))\n            lasthr_index = lasthr.strftime(datefmt)\n            if (lasthr_index != latest_index):\n                if self._is_valid_index(es, lasthr_index):\n                    indexes.append(lasthr.strftime(datefmt))\n        for day in range(1, days):\n            lastday = (now - datetime.timedelta(days=day))\n            index_name = lastday.strftime(datefmt)\n            if self._is_valid_index(es, index_name):\n                indexes.append(index_name)\n        args['index'] = indexes\n    results = es.search(query, **args)\n    return ResultSet(results)",
    "nl": "Search an elasticsearch server.\n\n`query` parameter is the complicated query structure that\npyelasticsearch uses. More details in their documentation.\n\n`size` is the max number of results to return from the search\nengine. We default it to 1000 to ensure we don't loose things.\nFor certain classes of queries (like faceted ones), this can actually\nbe set very low, as it won't impact the facet counts.\n\n`recent` search only most recent indexe(s), assuming this is basically\na real time query that you only care about the last hour of time.\nUsing recent dramatically reduces the load on the ES cluster.\n\n`days` search only the last number of days.\n\nThe returned result is a ResultSet query.",
    "original_nl": "Search an elasticsearch server.\n\n`query` parameter is the complicated query structure that\npyelasticsearch uses. More details in their documentation.\n\n`size` is the max number of results to return from the search\nengine. We default it to 1000 to ensure we don't loose things.\nFor certain classes of queries (like faceted ones), this can actually\nbe set very low, as it won't impact the facet counts.\n\n`recent` search only most recent indexe(s), assuming this is basically\na real time query that you only care about the last hour of time.\nUsing recent dramatically reduces the load on the ES cluster.\n\n`days` search only the last number of days.\n\nThe returned result is a ResultSet query."
  },
  {
    "code": "@doc_public\ndef sine(self, f, duration):\n    scale = maxv(numbits=self.bits)\n    ret = []\n    T = (1.0 / self.rate)\n    N = ((self.rate * duration) // 1000)\n    for n in xrange(N):\n        t = (n * T)\n        vsin = math.sin((((t * f) * 2) * math.pi))\n        A = (scale * (self.amplitude / 100))\n        s = (A * vsin)\n        if ((scale == 65535) or (scale == 255)):\n            s = ((s + scale) / 2)\n        ret.append(int(s))\n    return ret",
    "nl": "Return a sine wave\n\n@param f: frequency\n@type f: integer        \n\n@param duration: duration in milliseconds\n@type duration: integer \n\n@return: values sample \n@rtype: list of integer",
    "original_nl": "Return a sine wave\n\n@param f: frequency\n@type f: integer        \n\n@param duration: duration in milliseconds\n@type duration: integer \n\n@return: values sample \n@rtype: list of integer"
  },
  {
    "code": "def get(self, session_id):\n    return self._items.get(session_id, None)",
    "nl": "Return session object or None if it is not available\n\n`session_id`\n    Session identifier",
    "original_nl": "Return session object or None if it is not available\n\n`session_id`\n    Session identifier"
  },
  {
    "code": "def Update(self, data):\n    self._sha256_context.update(data)",
    "nl": "Updates the current state of the hasher with a new block of data.\n\nRepeated calls to update are equivalent to one single call with the\nconcatenation of the arguments.\n\nArgs:\n  data(bytes): block of data with which to update the context of the hasher.",
    "original_nl": "Updates the current state of the hasher with a new block of data.\n\nRepeated calls to update are equivalent to one single call with the\nconcatenation of the arguments.\n\nArgs:\n  data(bytes): block of data with which to update the context of the hasher."
  },
  {
    "code": "def _find_in_path(path, file):\n    for p in path.split(os.pathsep):\n        candidate = os.path.join(p, file)\n        if os.path.exists(os.path.join(p, file)):\n            return candidate\n    return False",
    "nl": "Find a file in a given path string.",
    "original_nl": "Find a file in a given path string."
  },
  {
    "code": "@templatefilter('hex')\ndef hexfilter(text):\n    return node.hex(text)",
    "nl": "Any text. Convert a binary Mercurial node identifier into\nits long hexadecimal representation.",
    "original_nl": "Any text. Convert a binary Mercurial node identifier into\nits long hexadecimal representation."
  },
  {
    "code": "def test_internal():\n    tests_passed = (subprocess.call(['./droopescan', 'test']) == 0)\n    if (not tests_passed):\n        f.error('Unit tests failed... abort.')",
    "nl": "Runs unit tests.",
    "original_nl": "Runs unit tests."
  },
  {
    "code": "def blackScholes(cp, s, k, t, r, d, v, full=False, comp=inf):\n    if (comp != inf):\n        r = toExp(r, comp)\n        d = toExp(d, comp)\n    f = (s * exp((t * (d - r))))\n    results = black(cp, f, k, t, r, v, full, comp=inf)\n    if (not full):\n        return results\n    else:\n        for (key, item) in results.items():\n            if (key in ['delta', 'gamma']):\n                item /= (f / s)\n        results['fwd'] = f\n        return results",
    "nl": "blackScholes, risk-free-rate and dividend-yield make up the forward rate\ncp   = \"c\" for a call, anything else assumes a put\nf    = Forward Price of the underlying asset\nk    = Strike Price\nt    = Time until maturity (in years)\nr    = Interest rate\nd    = Dividend yield\nv    = Implied volatility\ncomp = How many times interest is compounded a year (default = inf, i.e. continous rates)\nfull = If True, function returns a dictionary with price and all sensitivities\n       Otherwise only the calculated/calibrated parameter will be returned",
    "original_nl": "blackScholes, risk-free-rate and dividend-yield make up the forward rate\ncp   = \"c\" for a call, anything else assumes a put\nf    = Forward Price of the underlying asset\nk    = Strike Price\nt    = Time until maturity (in years)\nr    = Interest rate\nd    = Dividend yield\nv    = Implied volatility\ncomp = How many times interest is compounded a year (default = inf, i.e. continous rates)\nfull = If True, function returns a dictionary with price and all sensitivities\n       Otherwise only the calculated/calibrated parameter will be returned"
  },
  {
    "code": "def create_session(self, item=None):\n    return ItemCreateSessionRequestBuilder(self.append_to_request_url('upload.createSession'), self._client, item=item)",
    "nl": "Executes the createSession method\n\nArgs:\n    item (:class:`ChunkedUploadSessionDescriptor<onedrivesdk.model.chunked_upload_session_descriptor.ChunkedUploadSessionDescriptor>`):\n        Defaults to None, The item to use in the method request\n",
    "original_nl": "Executes the createSession method\n\nArgs:\n    item (:class:`ChunkedUploadSessionDescriptor<onedrivesdk.model.chunked_upload_session_descriptor.ChunkedUploadSessionDescriptor>`):\n        Defaults to None, The item to use in the method request\n\nReturns:\n    :class:`ItemCreateSessionRequestBuilder<onedrivesdk.request.item_create_session.ItemCreateSessionRequestBuilder>`:\n        A ItemCreateSessionRequestBuilder for the method"
  },
  {
    "code": "def getBrailleGenerator(self):\n    return None",
    "nl": "Returns the braille generator for this script.",
    "original_nl": "Returns the braille generator for this script."
  },
  {
    "code": "def getRequiredError(self):\n    raise NotImplementedError",
    "nl": "Gets the error message that is to be displayed if a required\nfield is empty.\n\n@return: Error message.",
    "original_nl": "Gets the error message that is to be displayed if a required\nfield is empty.\n\n@return: Error message."
  },
  {
    "code": "def state_events(self, t, y, sw):\n    if sw[0]:\n        smax_event = self.dS_dt\n    else:\n        smax_event = (- 1.0)\n    t_cutoff_event = (t - self.t_cutoff)\n    return np.array([(smax_event > 0), (t_cutoff_event < 0)])",
    "nl": "Check whether an 'event' has occurred. We want to see if the\nsupersaturation is decreasing or not.",
    "original_nl": "Check whether an 'event' has occurred. We want to see if the\nsupersaturation is decreasing or not."
  },
  {
    "code": "def transform(self, input_dir):\n    if (not self._swdb):\n        self._initSwdb(input_dir)\n    else:\n        logger.error(_('Error: database is already initialized'))",
    "nl": "Interface for database transformation",
    "original_nl": "Interface for database transformation"
  },
  {
    "code": "@manager.command\n@manager.option('-l', '--language', dest='language', nargs='*')\n@manager.option('-c', '--country', dest='country', nargs='*')\n@manager.option('-f', '--foreign_id', dest='foreign_id')\ndef crawldir(path, language=None, country=None, foreign_id=None):\n    path = decode_path(os.path.abspath(os.path.normpath(path)))\n    if ((path is None) or (not os.path.exists(path))):\n        log.error('Invalid path: %r', path)\n        return\n    path_name = os.path.basename(path)\n    if (foreign_id is None):\n        foreign_id = ('directory:%s' % slugify(path))\n    role = Role.load_cli_user()\n    collection = Collection.by_foreign_id(foreign_id)\n    if (collection is None):\n        collection = Collection.create({\n            'foreign_id': foreign_id,\n            'label': path_name,\n            'casefile': False,\n        }, role=role)\n    if (language is not None):\n        collection.languages = [language]\n    if (country is not None):\n        collection.countries = [country]\n    db.session.commit()\n    update_collection(collection)\n    log.info('Crawling %r to %r...', path, collection.foreign_id)\n    document = Document.by_keys(collection=collection, foreign_id=path)\n    document.file_name = path_name\n    ingest_document(document, path, role_id=role.id)",
    "nl": "Crawl the given directory.",
    "original_nl": "Crawl the given directory."
  },
  {
    "code": "def p_cmdexpr_rare(p):\n    p[0] = p[1]",
    "nl": "cmdexpr : rarecmd",
    "original_nl": "cmdexpr : rarecmd"
  },
  {
    "code": "def timedelta_div(t1, t2):\n    if isinstance(t2, timedelta):\n        return (t1.total_seconds() / t2.total_seconds())\n    else:\n        return timedelta(seconds=(t1.total_seconds() / t2))",
    "nl": "divides a timedelta by a timedelta or a number.\nshould be a method of timedelta...",
    "original_nl": "divides a timedelta by a timedelta or a number.\nshould be a method of timedelta..."
  },
  {
    "code": "def resolve_ipv6_address(hostname):\n    for addrinfo in socket.getaddrinfo(hostname, 0):\n        (family, socktype, proto, name, sockaddr) = addrinfo\n        if (family == socket.AF_INET6):\n            sockaddr6 = sockaddr\n            (address, port, flow, scope) = sockaddr6\n            return address\n    raise LookupError(hostname)",
    "nl": "Resolve a hostname to an IP version 6 address",
    "original_nl": "Resolve a hostname to an IP version 6 address"
  },
  {
    "code": "def allowScan(record):\n    pass",
    "nl": "Return True to allow SCAN='I/O Intr'\nor False to prevent this.\n\nIf a callable object is returned then if\nwill be invoked when I/O Intr scanning\nis disabled.  A Record instance is passed\nas the first (and only) argument.",
    "original_nl": "Return True to allow SCAN='I/O Intr'\nor False to prevent this.\n\nIf a callable object is returned then if\nwill be invoked when I/O Intr scanning\nis disabled.  A Record instance is passed\nas the first (and only) argument."
  },
  {
    "code": "def handle_endtag(self, tag_name):\n    self.__handle_data_if_exists()\n    if (self.__get_cur_tag()['name'] == tag_name):\n        self.__close_tag(self.__tag_stack.pop())\n    else:\n        for tag_id in xrange((len(self.__tag_stack) - 1), (- 1), (- 1)):\n            if (self.__tag_stack[tag_id]['name'] == tag_name):\n                for tag in reversed(self.__tag_stack[(tag_id + 1):]):\n                    self.__close_tag(tag, forced=True)\n                    self.__tag_stack.pop()\n                self.__close_tag(self.__tag_stack.pop())\n                break\n        else:\n            LOG.debug('Dropping excess end tag \"%s\"...', tag_name)",
    "nl": "Handles end of a tag.",
    "original_nl": "Handles end of a tag."
  },
  {
    "code": "def update(self):\n    self.reLayout()\n    if self._scaleChanged:\n        self.scaleChanged.emit(self._scale)\n        self._scaleChanged = False\n    self.changed.emit()",
    "nl": "Performs the layout (positions the Pages and adjusts our size).",
    "original_nl": "Performs the layout (positions the Pages and adjusts our size)."
  },
  {
    "code": "def validate_modules():\n    try:\n        jwt.rsa_load\n    except AttributeError:\n        raise ImproperlyConfigured('PyJWT-Mozilla not imported. This is because there is another JWT module installed. The JWT module imported is at: {0}. This can usually be fixed by running: `pip uninstall PyJWT` and `pip uninstall PyJWT-mozilla` and `pip install --force --no-deps PyJWT-mozilla`'.format(jwt.__file__))",
    "nl": "Validate that the modules that have been set up correctly.",
    "original_nl": "Validate that the modules that have been set up correctly."
  },
  {
    "code": "def _InnerModelClassesSupported(self):\n    return False",
    "nl": "Gets whether or not inner model classes are supported.",
    "original_nl": "Gets whether or not inner model classes are supported."
  },
  {
    "code": "def fixup_sigterm_behaviour():\n\n    def quit_handler(sig, frame):\n        pygame.quit()\n        sys.exit((128 + sig))\n    signal.signal(signal.SIGTERM, quit_handler)",
    "nl": "SDL registers its own signal handler for SIGTERM, which pushes a SDL_QUIT event to the event\nloop, instead of killing the process right away. This is a problem for us, because when using\nthe fbcon drivers, the process activates and locks a virtual terminal which survives after the\nprocess dies. We need to ensure that the process cleans up this virtual terminal, otherwise the\nTingbot needs a reboot.\n\nWe do this by calling the cleanup and exiting straight away on SIGTERM.",
    "original_nl": "SDL registers its own signal handler for SIGTERM, which pushes a SDL_QUIT event to the event\nloop, instead of killing the process right away. This is a problem for us, because when using\nthe fbcon drivers, the process activates and locks a virtual terminal which survives after the\nprocess dies. We need to ensure that the process cleans up this virtual terminal, otherwise the\nTingbot needs a reboot.\n\nWe do this by calling the cleanup and exiting straight away on SIGTERM."
  },
  {
    "code": "def setup_course(self, default_store=None):\n    if (not default_store):\n        default_store = self.store.default_modulestore.get_modulestore_type()\n    with self.store.default_store(default_store):\n        self.course = CourseFactory.create(**self.course_options())\n        chapter = ItemFactory.create(parent=self.course, category='chapter')\n        self.html_block = ItemFactory.create(parent=chapter, category='html', data='<p>Test HTML Content<p>')",
    "nl": "Helper method to create the course.",
    "original_nl": "Helper method to create the course."
  },
  {
    "code": "def __init__(self, context):\n    BaseStorage.__init__(self, context)\n    AwsStorage.__init__(self, context, 'TC_AWS_RESULT_STORAGE')\n    self.storage_expiration_seconds = context.config.get('RESULT_STORAGE_EXPIRATION_SECONDS', 3600)",
    "nl": "Constructor\n",
    "original_nl": "Constructor\n:param Context context: Thumbor's context"
  },
  {
    "code": "def levenshtein_distance(s1, s2):\n    if (len(s1) < len(s2)):\n        return levenshtein_distance(s2, s1)\n    if (len(s2) == 0):\n        return len(s1)\n    previous_row = range((len(s2) + 1))\n    for (i, c1) in enumerate(s1):\n        current_row = [(i + 1)]\n        for (j, c2) in enumerate(s2):\n            insertions = (previous_row[(j + 1)] + 1)\n            deletions = (current_row[j] + 1)\n            substitutions = (previous_row[j] + (c1 != c2))\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n    return previous_row[(- 1)]",
    "nl": "Computes the string edit distance based on the Levenshtein Distance.\n\nAll credit for implementation goes to:\nhttps://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\nMuch appreciated.\n\n.. note::\n    If ``s1`` and ``s2`` must both be strings or both be list. Cannot mix\n    types.\n",
    "original_nl": "Computes the string edit distance based on the Levenshtein Distance.\n\nAll credit for implementation goes to:\nhttps://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\nMuch appreciated.\n\n.. note::\n    If ``s1`` and ``s2`` must both be strings or both be list. Cannot mix\n    types.\n\nParameters\n----------\ns1 : str or list\n    String or list.\n\ns1 : str or list\n    Other string or list.\n\nReturns\n--------\nInteger equal to the number of edits to get from ``s1`` to ``s2``."
  },
  {
    "code": "def __init__(self):\n    super(ChromePreferencesClearHistoryEventData, self).__init__(data_type=self.DATA_TYPE)\n    self.message = None",
    "nl": "Initializes event data.",
    "original_nl": "Initializes event data."
  },
  {
    "code": "def test_matrix_representation_product_to_lin_space():\n    n = 3\n    A = np.random.rand(n, n)\n    Aop = odl.MatrixOperator(A)\n    B = np.random.rand(n, n)\n    Bop = odl.MatrixOperator(B)\n    ABop = ProductSpaceOperator([[Aop, Bop]])\n    matrix_repr = matrix_representation(ABop)\n    assert (matrix_repr.shape == (1, n, 2, n))\n    assert (np.linalg.norm((A - matrix_repr[0, :, 0, :])) == pytest.approx(0))\n    assert (np.linalg.norm((B - matrix_repr[0, :, 1, :])) == pytest.approx(0))",
    "nl": "Verify that the matrix repr works for product spaces.\n\nHere, since the domain shape ``(2, 3)`` and the range has shape ``(1, 3)``,\nthe shape of the matrix representation will be ``(2, 3, 1, 3)``.",
    "original_nl": "Verify that the matrix repr works for product spaces.\n\nHere, since the domain shape ``(2, 3)`` and the range has shape ``(1, 3)``,\nthe shape of the matrix representation will be ``(2, 3, 1, 3)``."
  },
  {
    "code": "def has_module_perms(self, user_obj, app_label):\n    for perm in self.get_all_permissions(user_obj):\n        if (perm[:perm.index('.')] == app_label):\n            return True\n    return False",
    "nl": "Returns True if user_obj has any permissions in the given app_label.",
    "original_nl": "Returns True if user_obj has any permissions in the given app_label."
  },
  {
    "code": "def on_exception(wait_gen, exception, max_tries=None, max_time=None, jitter=full_jitter, giveup=(lambda e: False), on_success=None, on_backoff=None, on_giveup=None, **wait_gen_kwargs):\n\n    def decorate(target):\n        retry = None\n        if (sys.version_info[:2] >= (3, 4)):\n            import asyncio\n            if asyncio.iscoroutinefunction(target):\n                import backoff._async\n                retry = backoff._async.retry_exception\n            else:\n                try:\n                    asyncio.get_event_loop()\n                except RuntimeError:\n                    pass\n                else:\n                    if (asyncio.Task.current_task() is not None):\n                        raise TypeError('backoff.on_exception applied to a regular function inside coroutine, this will lead to event loop hiccups. Use backoff.on_exception on coroutines in asynchronous code.')\n        if (retry is None):\n            retry = _sync.retry_exception\n        return retry(target, wait_gen, exception, max_tries, max_time, jitter, giveup, on_success, on_backoff, on_giveup, wait_gen_kwargs)\n    return decorate",
    "nl": "Returns decorator for backoff and retry triggered by exception.\n\nArgs:\n    wait_gen: A generator yielding successive wait times in\n        seconds.\n    exception: An exception type (or tuple of types) which triggers\n        backoff.\n    max_tries: The maximum number of attempts to make before giving\n        up. Once exhausted, the exception will be allowed to escape.\n        The default value of None means their is no limit to the\n        number of tries. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    max_time: The maximum total amount of time to try for before\n        giving up. Once expired, the exception will be allowed to\n        escape. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    jitter: A function of the value yielded by wait_gen returning\n        the actual time to wait. This distributes wait times\n        stochastically in order to avoid timing collisions across\n        concurrent clients. Wait times are jittered by default\n        using the full_jitter function. Jittering may be disabled\n        altogether by passing jitter=None.\n    giveup: Function accepting an exception instance and\n        returning whether or not to give up. Optional. The default\n        is to always continue.\n    on_success: Callable (or iterable of callables) with a unary\n        signature to be called in the event of success. The\n        parameter is a dict containing details about the invocation.\n    on_backoff: Callable (or iterable of callables) with a unary\n        signature to be called in the event of a backoff. The\n        parameter is a dict containing details about the invocation.\n    on_giveup: Callable (or iterable of callables) with a unary\n        signature to be called in the event that max_tries\n        is exceeded.  The parameter is a dict containing details\n        about the invocation.\n    **wait_gen_kwargs: Any additional keyword args specified will be\n        passed to wait_gen when it is initialized.  Any callable\n        args will first be evaluated and their return values passed.\n        This is useful for runtime configuration.",
    "original_nl": "Returns decorator for backoff and retry triggered by exception.\n\nArgs:\n    wait_gen: A generator yielding successive wait times in\n        seconds.\n    exception: An exception type (or tuple of types) which triggers\n        backoff.\n    max_tries: The maximum number of attempts to make before giving\n        up. Once exhausted, the exception will be allowed to escape.\n        The default value of None means their is no limit to the\n        number of tries. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    max_time: The maximum total amount of time to try for before\n        giving up. Once expired, the exception will be allowed to\n        escape. If a callable is passed, it will be\n        evaluated at runtime and its return value used.\n    jitter: A function of the value yielded by wait_gen returning\n        the actual time to wait. This distributes wait times\n        stochastically in order to avoid timing collisions across\n        concurrent clients. Wait times are jittered by default\n        using the full_jitter function. Jittering may be disabled\n        altogether by passing jitter=None.\n    giveup: Function accepting an exception instance and\n        returning whether or not to give up. Optional. The default\n        is to always continue.\n    on_success: Callable (or iterable of callables) with a unary\n        signature to be called in the event of success. The\n        parameter is a dict containing details about the invocation.\n    on_backoff: Callable (or iterable of callables) with a unary\n        signature to be called in the event of a backoff. The\n        parameter is a dict containing details about the invocation.\n    on_giveup: Callable (or iterable of callables) with a unary\n        signature to be called in the event that max_tries\n        is exceeded.  The parameter is a dict containing details\n        about the invocation.\n    **wait_gen_kwargs: Any additional keyword args specified will be\n        passed to wait_gen when it is initialized.  Any callable\n        args will first be evaluated and their return values passed.\n        This is useful for runtime configuration."
  },
  {
    "code": "def __get_plugin_element(self, plugin):\n    if (plugin not in self.__elements):\n        element = None\n        try:\n            element = plugin.setup_element()\n        except Exception:\n            util.print_exc()\n        if (not element):\n            util.print_w((_(\"GStreamer plugin '%(name)s' could not be initialized\") % {\n                'name': plugin.PLUGIN_ID,\n            }))\n            return\n        plugin.update_element(element)\n        self.__elements[plugin] = element\n    return self.__elements[plugin]",
    "nl": "Setup element and cache it, so we can pass the linked/active\none to the plugin for live updates",
    "original_nl": "Setup element and cache it, so we can pass the linked/active\none to the plugin for live updates"
  },
  {
    "code": "def ecdh(self, identity, pubkey):\n    raise NotImplementedError()",
    "nl": "Get shared session key using Elliptic Curve Diffie-Hellman.",
    "original_nl": "Get shared session key using Elliptic Curve Diffie-Hellman."
  },
  {
    "code": "def _mount_coord_to_skycoord(self, mount_coords):\n    if isinstance(mount_coords, dict):\n        ra = mount_coords['ra']\n        dec = mount_coords['dec']\n    else:\n        (ra, dec) = mount_coords.split(' ')\n    ra = (float(ra) * u.hourangle).to(u.degree)\n    dec = (float(dec) * u.deg)\n    coords = SkyCoord(ra, dec)\n    return coords",
    "nl": "Converts between iOptron RA/Dec format and a SkyCoord\n\nArgs:\n    mount_coords (str): Coordinates as returned by mount\n",
    "original_nl": "Converts between iOptron RA/Dec format and a SkyCoord\n\nArgs:\n    mount_coords (str): Coordinates as returned by mount\n\nReturns:\n    astropy.SkyCoord:   Mount coordinates as astropy SkyCoord with\n        EarthLocation included."
  },
  {
    "code": "def isquit(self):\n    try:\n        CalcThread.isquit(self)\n    except KeyboardInterrupt:\n        raise KeyboardInterrupt",
    "nl": "@raise KeyboardInterrupt: when the thread is interrupted",
    "original_nl": "@raise KeyboardInterrupt: when the thread is interrupted"
  },
  {
    "code": "def testInitialization(self):\n    event_formatter = winevtx.WinEVTXFormatter()\n    self.assertIsNotNone(event_formatter)",
    "nl": "Tests the initialization.",
    "original_nl": "Tests the initialization."
  },
  {
    "code": "def find_bricks(host=None, name=None, filename=None):\n    if name:\n        matches = glob(('/dev/*%s*' % name))\n    elif filename:\n        matches = glob(filename)\n    else:\n        matches = glob('/dev/*-DevB')\n    for match in matches:\n        (yield DeviceSocket(match))",
    "nl": "Supply either name=brickname (path will be guessed),\nfilename=absolute path to brick file, or neither. host is ignored.",
    "original_nl": "Supply either name=brickname (path will be guessed),\nfilename=absolute path to brick file, or neither. host is ignored."
  },
  {
    "code": "def test_get_fields_for_model_many_to_one(self):\n    Base = declarative_base()\n\n    class Parent(Base):\n        __tablename__ = 'parent'\n        id = Column(Integer, primary_key=True)\n        child_id = Column(Integer, ForeignKey('child.id'))\n        child = relationship('Child', backref='parents')\n\n    class Child(Base):\n        __tablename__ = 'child'\n        id = Column(Integer, primary_key=True)\n    resp = _get_fields_for_model(Parent)\n    self.assertAllIn(resp, ('id', 'child.id', 'child_id'))\n    resp = _get_fields_for_model(Child)\n    self.assertAllIn(resp, ('id', 'parents.id'))",
    "nl": "Tests getting the fields for a many_to_one",
    "original_nl": "Tests getting the fields for a many_to_one"
  },
  {
    "code": "@pytest.mark.xfail()\ndef test_skip_fixture(skip_fixture):\n    pass",
    "nl": ">>> allure_report = getfixture('allure_report')\n>>> assert_that(allure_report,\n...             has_test_case('test_skip_fixture',\n...                           with_status('skipped'),\n...                           has_status_details(with_message_contains(\"Skipped: <Skipped instance>\")),\n...                           has_container(allure_report,\n...                                         has_before('skip_fixture',\n...                                                    with_status('skipped'),\n...                                                    has_status_details(with_message_contains(\"Skipped: <Skipped instance>\"),\n...                                                                       with_trace_contains(\"skip_fixture\")\n...                                                    ),\n...                                         ),\n...                           )\n...             )\n... )",
    "original_nl": ">>> allure_report = getfixture('allure_report')\n>>> assert_that(allure_report,\n...             has_test_case('test_skip_fixture',\n...                           with_status('skipped'),\n...                           has_status_details(with_message_contains(\"Skipped: <Skipped instance>\")),\n...                           has_container(allure_report,\n...                                         has_before('skip_fixture',\n...                                                    with_status('skipped'),\n...                                                    has_status_details(with_message_contains(\"Skipped: <Skipped instance>\"),\n...                                                                       with_trace_contains(\"skip_fixture\")\n...                                                    ),\n...                                         ),\n...                           )\n...             )\n... )"
  },
  {
    "code": "def now():\n    return datetime.datetime.now(tz=pytz.utc).isoformat()",
    "nl": "Returns a datetime now object at utc. For convenience.",
    "original_nl": "Returns a datetime now object at utc. For convenience."
  },
  {
    "code": "@property\ndef clean_prefix(self):\n    user = self.context.bot.user\n    return self.context.prefix.replace(user.mention, ('@' + user.name))",
    "nl": "The cleaned up invoke prefix. i.e. mentions are ``@name`` instead of ``<@id>``.",
    "original_nl": "The cleaned up invoke prefix. i.e. mentions are ``@name`` instead of ``<@id>``."
  },
  {
    "code": "def testGetFileEntry(self):\n    session = sessions.Session()\n    storage_writer = fake_writer.FakeStorageWriter(session)\n    parsers_mediator = self._CreateParserMediator(storage_writer)\n    file_entry = parsers_mediator.GetFileEntry()\n    self.assertIsNone(file_entry)",
    "nl": "Tests the GetFileEntry function.",
    "original_nl": "Tests the GetFileEntry function."
  },
  {
    "code": "def op(scalars_layout, collections=None):\n    assert isinstance(scalars_layout, layout_pb2.Layout)\n    return tf.summary.tensor_summary(name=metadata.CONFIG_SUMMARY_TAG, tensor=tf.constant(scalars_layout.SerializeToString(), dtype=tf.string), collections=collections, summary_metadata=_create_summary_metadata())",
    "nl": "Creates a summary that contains a layout.\n\nWhen users navigate to the custom scalars dashboard, they will see a layout\nbased on the proto provided to this function.\n\nArgs:\n  scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n      layout.\n  collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n",
    "original_nl": "Creates a summary that contains a layout.\n\nWhen users navigate to the custom scalars dashboard, they will see a layout\nbased on the proto provided to this function.\n\nArgs:\n  scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n      layout.\n  collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n\nReturns:\n  A tensor summary op that writes the layout to disk."
  },
  {
    "code": "def gatherHistoryData(self, ert, case, key):\n    if ((not self.canGatherDataForKey(key)) or (not self.hasHistoryGatherFunction())):\n        raise UserWarning(('Unable to gather history data for key: %s' % key))\n    return self._historyGatherFunc(ert, case, key)",
    "nl": ":rtype: pandas.DataFrame",
    "original_nl": ":rtype: pandas.DataFrame"
  },
  {
    "code": "def test100():\n    return reader_creator(paddle.dataset.common.download(CIFAR100_URL, 'cifar', CIFAR100_MD5), 'test')",
    "nl": "CIFAR-100 test set creator.\n\nIt returns a reader creator, each sample in the reader is image pixels in\n[0, 1] and label in [0, 9].\n\n",
    "original_nl": "CIFAR-100 test set creator.\n\nIt returns a reader creator, each sample in the reader is image pixels in\n[0, 1] and label in [0, 9].\n\n:return: Test reader creator.\n:rtype: callable"
  },
  {
    "code": "def get_url(self, absolute=True):\n    return absolutify(urlparams(reverse('phonebook:register'), code=self.code))",
    "nl": "A url that can be used to redeem this invite.",
    "original_nl": "A url that can be used to redeem this invite."
  },
  {
    "code": "@staticmethod\ndef ping_from_remotehost(session_object, ip_type, dest_address, prompt, count):\n    ping_cmd = 'ping'\n    if (ip_type == 'ipv6'):\n        ping_cmd = 'ping6'\n    command = (ping_cmd + ' -c {} {}'.format(count, dest_address))\n    (_, output) = session_object.send_command('.*', prompt, command)\n    if ((' 0% packet loss' in output) or ('alive' in output)):\n        pNote('ping successfully completed')\n        status = True\n    else:\n        pNote('ping command failed')\n        status = False\n    return status",
    "nl": "ping  to dest_system from remote host\n\n:Arguments:\n    1. session_object(string)  = expect session object\n    2. command(string) = command to be executed\n    3. ip_type = ip/ipv6/dns\n    4. dest_address(string) = ip or dns name\n    3. prompt(string) = prompt\n\n",
    "original_nl": "ping  to dest_system from remote host\n\n:Arguments:\n    1. session_object(string)  = expect session object\n    2. command(string) = command to be executed\n    3. ip_type = ip/ipv6/dns\n    4. dest_address(string) = ip or dns name\n    3. prompt(string) = prompt\n\n:Returns:\n    1. bool (True/False)"
  },
  {
    "code": "def test_serialize(api_rf, media_resource_category_factory, serialized_time):\n    category = media_resource_category_factory()\n    api_rf.user = category.km_user.user\n    request = api_rf.get(category.get_absolute_url())\n    serializer = serializers.MediaResourceCategorySerializer(category, context={\n        'request': request,\n    })\n    expected = {\n        'id': category.id,\n        'url': request.build_absolute_uri(),\n        'created_at': serialized_time(category.created_at),\n        'updated_at': serialized_time(category.updated_at),\n        'km_user_id': category.km_user.id,\n        'name': category.name,\n        'permissions': {\n            'read': category.has_object_read_permission(request),\n            'write': category.has_object_write_permission(request),\n        },\n    }\n    assert (serializer.data == expected)",
    "nl": "Test serializing a media resource category.",
    "original_nl": "Test serializing a media resource category."
  },
  {
    "code": "def __getattr__(self, name):\n    try:\n        return self._dynamic_properties[name]()\n    except KeyError:\n        msg = \"'{0}' object has no attribute '{1}'\"\n        raise AttributeError(msg.format(type(self).__name__, name))",
    "nl": "It is not possible to create property bounded just to object\nand not class at runtime, therefore it is necessary to\noverride __getattr__ and make fake 'properties' by storing them in\nthe protected attribute _dynamic_attributes and returning result\nof the method associated with the specified attribute.\n\nThis way the feeling of having regions accessed as 'properties'\nis created, which is one of the requirement of page object pattern.",
    "original_nl": "It is not possible to create property bounded just to object\nand not class at runtime, therefore it is necessary to\noverride __getattr__ and make fake 'properties' by storing them in\nthe protected attribute _dynamic_attributes and returning result\nof the method associated with the specified attribute.\n\nThis way the feeling of having regions accessed as 'properties'\nis created, which is one of the requirement of page object pattern."
  },
  {
    "code": "def set_point_coordinate(self, name, x, y, z):\n    try:\n        pt = self.session.query(Point).filter_by(name=name).first()\n        if (pt is None):\n            raise Exception(\"Point doesn't exists.\")\n        scale = self.scale()\n        pt.x = (x * scale['L'])\n        pt.y = (y * scale['L'])\n        pt.z = (z * scale['L'])\n        self.session.add(pt)\n        return True\n    except Exception as e:\n        logger.info(str(e))\n        self.session.rollback()\n        return False",
    "nl": "Set point coordinate.\nif a point in same location exists, the name of the point will be returned.\n        ",
    "original_nl": "Set point coordinate.\nif a point in same location exists, the name of the point will be returned.\n        \nparam:\n    x,y,z: float-like, coordinates in current unit.\n    [name]: str, name, optional.\nreturn:\n    str, the new point's name."
  },
  {
    "code": "def command_set_brightness(self, value):\n    if ((value >= 0) and (value <= 9)):\n        self.write((12, 1, (value + 2)))",
    "nl": "Set Brightness (value between 0 and 9)",
    "original_nl": "Set Brightness (value between 0 and 9)"
  },
  {
    "code": "def get_default_backend():\n    global default_backend\n    if (not default_backend):\n        default_backend = DictCache()\n    return default_backend",
    "nl": "Returns the currently configured `default_backend`.\n\nIf not set, the `default_backend` is a `supycache.DictCache` instance. Use\n`supycache.set_default_backend` to change this. A `backend` is any\n(caching) object that has at least the `.get()`, `.set()` and `.delete()`\nmethods.",
    "original_nl": "Returns the currently configured `default_backend`.\n\nIf not set, the `default_backend` is a `supycache.DictCache` instance. Use\n`supycache.set_default_backend` to change this. A `backend` is any\n(caching) object that has at least the `.get()`, `.set()` and `.delete()`\nmethods."
  },
  {
    "code": "def apply(self, action, monotone=False):\n    new_preds = set(self.predicates)\n    new_preds |= set(action.add_effects)\n    if (not monotone):\n        new_preds -= set(action.del_effects)\n    new_functions = dict()\n    new_functions.update(self.functions)\n    for (function, value) in action.num_effects:\n        new_functions[function] += value\n    return State(new_preds, new_functions, (self.cost + 1), (self, action))",
    "nl": "Apply the action to this state to produce a new state.\nIf monotone, ignore the delete list (for A* heuristic)",
    "original_nl": "Apply the action to this state to produce a new state.\nIf monotone, ignore the delete list (for A* heuristic)"
  },
  {
    "code": "def __init__(self):\n    super(list, self).__init__()",
    "nl": "Contains a collection of MemoryMapped objects\nWhen a system is defined with numerous modules connect via a\nmemory-mapped bus this object is used to manage the complete\ncollection.",
    "original_nl": "Contains a collection of MemoryMapped objects\nWhen a system is defined with numerous modules connect via a\nmemory-mapped bus this object is used to manage the complete\ncollection."
  },
  {
    "code": "def run(self, package, body, operation='replace'):\n    package_path = self._zip_package(package)\n    try:\n        package = self._import_package(package_path)\n        self._update_package(package, body, operation)\n        self._delete_package(package)\n    finally:\n        os.remove(package_path)",
    "nl": "Import Murano package, modify it and then delete it.\n\nMeasure the Murano import, update and delete package\ncommands performance.\nIt imports Murano package from \"package\" (if it is not a zip archive\nthen zip archive will be prepared), modifies it (using data from\n\"body\") and deletes.\n\n",
    "original_nl": "Import Murano package, modify it and then delete it.\n\nMeasure the Murano import, update and delete package\ncommands performance.\nIt imports Murano package from \"package\" (if it is not a zip archive\nthen zip archive will be prepared), modifies it (using data from\n\"body\") and deletes.\n\n:param package: path to zip archive that represents Murano\n                application package or absolute path to folder with\n                package components\n:param body: dict object that defines what package property will be\n             updated, e.g {\"tags\": [\"tag\"]} or {\"enabled\": \"true\"}\n:param operation: string object that defines the way of how package\n                  property will be updated, allowed operations are\n                  \"add\", \"replace\" or \"delete\".\n                  Default value is \"replace\"."
  },
  {
    "code": "def test_get_size(self):\n    self.assertEqual(self.char1.get_size(), 3)\n    self.assertEqual(self.char2.get_size(), 5)",
    "nl": "[Foundation/BasicTypes/Char] - get_size.",
    "original_nl": "[Foundation/BasicTypes/Char] - get_size."
  },
  {
    "code": "def delete_single_preprocessor(instance_id=None, **kw):\n    pass",
    "nl": "Accepts a single argument, `instance_id`, which is the primary key\nof the instance which will be deleted.",
    "original_nl": "Accepts a single argument, `instance_id`, which is the primary key\nof the instance which will be deleted."
  },
  {
    "code": "@classmethod\ndef from_response(cls, response_data):\n    identity = (lambda x: x)\n    return Mail(**_transform_dict(response_data, {\n        'guid': ('mail_id', identity),\n        'subject': ('mail_subject', identity),\n        'sender': ('mail_from', identity),\n        'datetime': ('mail_timestamp', (lambda x: datetime.utcfromtimestamp(int(x)).replace(tzinfo=utc))),\n        'read': ('mail_read', int),\n        'excerpt': ('mail_excerpt', identity),\n        'body': ('mail_body', identity),\n    }))",
    "nl": "Factory method to create a Mail instance from a Guerrillamail response\ndict.",
    "original_nl": "Factory method to create a Mail instance from a Guerrillamail response\ndict."
  },
  {
    "code": "def connect(self):\n    host = mpd_config['host'].get(unicode)\n    port = mpd_config['port'].get(int)\n    if (host[0] in ['/', '~']):\n        host = os.path.expanduser(host)\n    self._log.info('connecting to {0}:{1}', host, port)\n    try:\n        self.client.connect(host, port)\n    except socket.error as e:\n        raise ui.UserError('could not connect to MPD: {0}'.format(e))\n    password = mpd_config['password'].get(unicode)\n    if password:\n        try:\n            self.client.password(password)\n        except mpd.CommandError as e:\n            raise ui.UserError('could not authenticate to MPD: {0}'.format(e))",
    "nl": "Connect to the MPD.",
    "original_nl": "Connect to the MPD."
  },
  {
    "code": "def dehydrate(self, bundle):\n    return bundle.obj.get_name_display()",
    "nl": "only need the name of the service",
    "original_nl": "only need the name of the service"
  },
  {
    "code": "def _unstack(obj):\n    if ('temp_dim' in obj.dims):\n        obj_unstacked = obj.unstack('temp_dim')\n    else:\n        obj_unstacked = obj\n    return obj_unstacked",
    "nl": "Private function used to reorder to use the work dimension as the first\ndimension, stack all the dimensions except the first one",
    "original_nl": "Private function used to reorder to use the work dimension as the first\ndimension, stack all the dimensions except the first one"
  },
  {
    "code": "def GetNormalizedOutputAndLeakyTests(output):\n    output = ToUnixLineEnding(output)\n    output = RemoveReportHeaderAndFooter(output)\n    output = NormalizeErrorMarker(output)\n    output = RemoveLocations(output)\n    output = RemoveMemoryAddresses(output)\n    return (RemoveTestNamesOfLeakedMocks(output), GetLeakyTests(output))",
    "nl": "Normalizes the output of gmock_output_test_.\n\nArgs:\n  output: The test output.\n",
    "original_nl": "Normalizes the output of gmock_output_test_.\n\nArgs:\n  output: The test output.\n\nReturns:\n  A tuple (the normalized test output, the list of test names that have\n  leaked mocks)."
  },
  {
    "code": "def init_connection_state(self):\n    pass",
    "nl": "no state to be init-ed",
    "original_nl": "no state to be init-ed"
  },
  {
    "code": "def test_nonlinear_variational_solver_custom_comm():\n    if (MPI.rank(mpi_comm_world()) == 0):\n        mesh = UnitIntervalMesh(mpi_comm_self(), 2)\n        V = FunctionSpace(mesh, 'CG', 1)\n        f = Constant(1)\n        u = Function(V)\n        v = TestFunction(V)\n        F = ((inner(u, v) * dx) - (inner(f, v) * dx))\n        solve((F == 0), u)\n        solve((F == 0), u, solver_parameters={\n            'nonlinear_solver': 'newton',\n        })\n        if has_petsc():\n            solve((F == 0), u, solver_parameters={\n                'nonlinear_solver': 'snes',\n            })",
    "nl": "Check that nonlinear variational solver works on subset of comm_world",
    "original_nl": "Check that nonlinear variational solver works on subset of comm_world"
  },
  {
    "code": "@asyncio.coroutine\ndef update_async(self, tag):\n    entity = (yield from self.request().update_async(tag))\n    return entity",
    "nl": "Updates the specified Tag in async\n\nArgs:\n    tag (:class:`Tag<onedrivesdk.model.tag.Tag>`):\n        The Tag to update.\n",
    "original_nl": "Updates the specified Tag in async\n\nArgs:\n    tag (:class:`Tag<onedrivesdk.model.tag.Tag>`):\n        The Tag to update.\n\nReturns:\n    :class:`Tag<onedrivesdk.model.tag.Tag>`:\n        The updated Tag."
  },
  {
    "code": "def predict_by_pct(self, recur_pct, del_pct, total):\n    recur_ct = (recur_pct * total)\n    del_ct = (del_pct * total)\n    if (self.kind == 'vogelstein'):\n        if ((recur_pct >= self.onco_threshold) and (recur_ct >= self.onco_min)):\n            if (del_pct <= 0.05):\n                return self.onco_label\n            elif (del_ct >= self.tsg_min):\n                return self.tsg_label\n            else:\n                return self.other_label\n        elif ((del_pct >= self.tsg_threshold) and (del_ct >= self.tsg_min)):\n            return self.tsg_label\n        else:\n            return self.other_label\n    elif (self.kind == 'min'):\n        if (total < self.min_count):\n            return self.other_label\n        elif (recur_pct >= self.onco_threshold):\n            if (recur_pct >= del_pct):\n                return self.onco_label\n            else:\n                return self.tsg_label\n        elif (del_pct >= self.tsg_threshold):\n            return self.tsg_label\n        else:\n            return self.other_label",
    "nl": "The actual 20/20 rule logic to classify genes.",
    "original_nl": "The actual 20/20 rule logic to classify genes."
  },
  {
    "code": "def get_remote_group_names(self):\n    return frozenset(self.filter(local=False).values_list('name', flat=True))",
    "nl": "Return names of groups related to external authentication.",
    "original_nl": "Return names of groups related to external authentication."
  },
  {
    "code": "def authorize_user_context(context, user_id):\n    if is_user_context(context):\n        if (not context.user_id):\n            raise exception.Forbidden()\n        elif (context.user_id != user_id):\n            raise exception.Forbidden()",
    "nl": "Ensures a request has permission to access the given user.",
    "original_nl": "Ensures a request has permission to access the given user."
  },
  {
    "code": "@parse_command([('name', 1), ('password', '?')], launch_invalid=False)\ndef admin_command_channel_join(self, msg, args):\n    self.join(args.name[0], args.password)",
    "nl": "Joins a channel.\n\nSyntax: channel_join CHANNEL_NAME [CHANNEL_PASSWORD]",
    "original_nl": "Joins a channel.\n\nSyntax: channel_join CHANNEL_NAME [CHANNEL_PASSWORD]"
  },
  {
    "code": "def PushEvents(self, events):\n    for event in events:\n        self.PushEvent(event)",
    "nl": "Pushes events onto the heap.\n\nArgs:\n  events list[EventObject]: events.",
    "original_nl": "Pushes events onto the heap.\n\nArgs:\n  events list[EventObject]: events."
  },
  {
    "code": "def handle_response(self, lvap):\n    lvaps = RUNTIME.tenants[self.tenant_id].lvaps\n    if (lvap.addr not in lvaps):\n        return\n    self.handle_callback(lvap)",
    "nl": "Handle an LVAL_LEAVE message.\nArgs:\n    lvap, an LVAP object",
    "original_nl": "Handle an LVAL_LEAVE message.\nArgs:\n    lvap, an LVAP object\nReturns:\n    None"
  },
  {
    "code": "def get_port_id(self, tg_id, port_id):\n    port_name = self.tgs[tg_id].ports[(port_id - 1)]\n    return (self.ports.index(Port(tg_id, port_name)) + 1)",
    "nl": "Return port's sequence number in list of ports.\n\nArgs:\n    tg_id(int):  TG instance ID\n    port_id(int):  TG instance port's sequence number\n\nRaises:\n    ValueError:  in case expected port is not in list of ports\n",
    "original_nl": "Return port's sequence number in list of ports.\n\nArgs:\n    tg_id(int):  TG instance ID\n    port_id(int):  TG instance port's sequence number\n\nRaises:\n    ValueError:  in case expected port is not in list of ports\n\nReturns:\n    int:  Port sequence number in list of ports starting from 1"
  },
  {
    "code": "def assertRealNotEqual(self, a, b, msg=None):\n    self.assertTrue((a != b), '{!r} != {!r}'.format(a, b))\n    self.assertTrue((b != a), '{!r} != {!r}'.format(b, a))\n    self.assertFalse((a == b), '{!r} == {!r}'.format(a, b))\n    self.assertFalse((b == a), '{!r} == {!r}'.format(b, a))",
    "nl": "checks the == and != operators, also check symmetry",
    "original_nl": "checks the == and != operators, also check symmetry"
  },
  {
    "code": "def clear(self):\n    if (not os.path.exists(self._dir)):\n        return\n    for fname in self._list_cache_files():\n        self._delete(fname)",
    "nl": "Remove all the cache files.",
    "original_nl": "Remove all the cache files."
  },
  {
    "code": "def setup(hass, config):\n    from toonapilib.toonapilibexceptions import InvalidConsumerSecret, InvalidConsumerKey, InvalidCredentials\n    try:\n        hass.data[TOON_HANDLE] = ToonDataStore(config['toon'][CONF_USERNAME], config['toon'][CONF_PASSWORD], config['toon'][CONF_KEY], config['toon'][CONF_SECRET], config['toon'][CONF_GAS], config['toon'][CONF_SOLAR], config['toon'][CONF_TENANT], config['toon'][CONF_NAME])\n    except InvalidCredentials:\n        _LOGGER.warning('The credentials in your config are invalid')\n        return False\n    except InvalidConsumerKey:\n        _LOGGER.warning('Your customer key is invalid')\n        return False\n    except InvalidConsumerSecret:\n        _LOGGER.warning('Your customer secret is invalid')\n        return False\n    for platform in ('climate', 'sensor', 'switch'):\n        load_platform(hass, platform, DOMAIN, {\n            \n        }, config)\n    return True",
    "nl": "Setup toon.",
    "original_nl": "Setup toon."
  },
  {
    "code": "def test_handle_update(self):\n    test_stack = self.create_stack(template=alarm_template)\n    update_mock = self.patchobject(self.fa.alarm, 'update')\n    test_stack.create()\n    rsrc = test_stack['cps_alarm']\n    self.assertEqual((rsrc.CREATE, rsrc.COMPLETE), rsrc.state)\n    after_props = copy.deepcopy(rsrc.properties.data)\n    update_props = {\n        'enabled': False,\n        'repeat_actions': False,\n        'insufficient_data_actions': [],\n        'ok_actions': ['signal_handler'],\n    }\n    after_props.update(update_props)\n    snippet = rsrc_defn.ResourceDefinition(rsrc.name, rsrc.type(), after_props)\n    scheduler.TaskRunner(rsrc.update, snippet)()\n    self.assertEqual((rsrc.UPDATE, rsrc.COMPLETE), rsrc.state)\n    self.assertEqual(1, update_mock.call_count)",
    "nl": "Test update the composite alarm.",
    "original_nl": "Test update the composite alarm."
  },
  {
    "code": "def _plotGaussianKDE(axes, plot_config, data, label):\n    style = plot_config.histogramStyle()\n    if (data.dtype == 'object'):\n        try:\n            data = pd.to_numeric(data, errors='coerce')\n        except AttributeError:\n            data = data.convert_objects(convert_numeric=True)\n    if (data.dtype == 'object'):\n        pass\n    else:\n        sample_range = (data.max() - data.min())\n        indexes = numpy.linspace((data.min() - (0.5 * sample_range)), (data.max() + (0.5 * sample_range)), 1000)\n        gkde = gaussian_kde(data.values)\n        evaluated_gkde = gkde.evaluate(indexes)\n        lines = axes.plot(indexes, evaluated_gkde, linewidth=style.width, color=style.color, alpha=style.alpha)\n        if (len(lines) > 0):\n            plot_config.addLegendItem(label, lines[0])",
    "nl": "@type axes: matplotlib.axes.Axes\n@type plot_config: PlotConfig\n@type data: DataFrame\n@type label: Str",
    "original_nl": "@type axes: matplotlib.axes.Axes\n@type plot_config: PlotConfig\n@type data: DataFrame\n@type label: Str"
  },
  {
    "code": "def _do_edit_config(self, target, config, default_operation, test_option, error_option):\n    try:\n        log.debug('edit-config', target=target, config=config)\n        response = self._session.edit_config(target=target, config=config)\n        log.debug('response', response=response)\n    except RPCError as e:\n        log.exception('do_edit_config', e=e)\n        raise\n    return response",
    "nl": "Lock the configuration system",
    "original_nl": "Lock the configuration system"
  },
  {
    "code": "def collapse(self):\n    if self.cardinality:\n        r = None\n        for si in self._si_set:\n            r = (r._union(si) if (r is not None) else si)\n        return r\n    else:\n        return StridedInterval.empty(self._bits)",
    "nl": "Collapse into a StridedInterval instance.\n\n",
    "original_nl": "Collapse into a StridedInterval instance.\n\n:return: A new StridedInterval instance."
  },
  {
    "code": "@classmethod\ndef create(cls, name, template):\n    try:\n        fw_template = IPSTemplatePolicy(template).href\n    except ElementNotFound:\n        raise LoadPolicyFailed('Cannot find specified firewall template: {}'.format(template))\n    json = {\n        'name': name,\n        'template': fw_template,\n    }\n    try:\n        return ElementCreator(cls, json)\n    except CreateElementFailed as err:\n        raise CreatePolicyFailed(err)",
    "nl": "Create an IPS Policy\n\n",
    "original_nl": "Create an IPS Policy\n\n:param str name: Name of policy\n:param str template: name of template\n:raises CreatePolicyFailed: policy failed to create\n:return: IPSPolicy"
  },
  {
    "code": "def cleanup(self, *args, **kwargs):\n    super(OpenMPI, self).cleanup(*args, **kwargs)\n    tmpdir = os.environ.get('TMPDIR')\n    if (tmpdir != self.orig_tmpdir):\n        try:\n            shutil.rmtree(tmpdir)\n        except OSError as err:\n            print_warning('Failed to clean up temporary directory %s: %s', tmpdir, err)\n        env.setvar('TMPDIR', self.orig_tmpdir)\n        self.log.info('$TMPDIR restored to %s', self.orig_tmpdir)",
    "nl": "Clean up after using OpenMPI in toolchain.",
    "original_nl": "Clean up after using OpenMPI in toolchain."
  },
  {
    "code": "def test_auto_coord(self):\n    (x, y) = (3.14, 2.72)\n    b = Block('Spam', 'eggs', (x, y), '1 cm')\n    self.assertEqual(b.get_tikz_coordinate('g'), '\\\\coordinate (eggs--coord) at (3.14, 2.72);')",
    "nl": "Test auto TikZ coordinate specification for a block.",
    "original_nl": "Test auto TikZ coordinate specification for a block."
  },
  {
    "code": "@task\ndef docs(ctx, clean=False, browse=False, watch=False):\n    if clean:\n        clean_docs(ctx)\n    ctx.run(('sphinx-build %s %s' % (docs_dir, build_dir)), echo=True)\n    if browse:\n        browse_docs(ctx)\n    if watch:\n        watch_docs(ctx)",
    "nl": "Build the docs.",
    "original_nl": "Build the docs."
  },
  {
    "code": "def _plot(self):\n    for serie in self.series:\n        self.funnel(serie)",
    "nl": "Plot the funnel",
    "original_nl": "Plot the funnel"
  },
  {
    "code": "def contains(self, item):\n    lock = RLock()\n    isPresent = False\n    lock.acquire()\n    isPresent = (item in self.all_items)\n    lock.release()\n    return isPresent",
    "nl": "Attempt to implement thread-safety for contains method using Re-entrant locks.\n## I'm still not sure whether this implementation is correct.",
    "original_nl": "Attempt to implement thread-safety for contains method using Re-entrant locks.\n## I'm still not sure whether this implementation is correct."
  },
  {
    "code": "def __init__(self, id_, name, human_readable_name, cost, colors, min_app_version=None):\n    self.id = id_\n    self.name = name\n    self.human_readable_name = human_readable_name\n    self.colors = colors\n    self.cost = cost\n    self.min_app_version = min_app_version",
    "nl": "`colors` is a list of Color instances.",
    "original_nl": "`colors` is a list of Color instances."
  },
  {
    "code": "def func1(param1):\n    return None",
    "nl": "Function 1\nwith 1 param\n\n",
    "original_nl": "Function 1\nwith 1 param\n\n:param param1: 1st parameter\n:type param1: type\n:returns: None"
  },
  {
    "code": "def forward(self, inputs):\n    out = np.array(inputs)\n    for key in self.W.keys():\n        W = self.W[key]\n        b = self.b[key]\n        act = self.activation[key]\n        z = (np.dot(out, W) + b)\n        out = act(z)\n    return out",
    "nl": "Forward propagate the inputs through the neural network.\nINPUTS\n    inputs - array_like\n        An input to the neural network. Must match the specfied input\n        dimension or errors will be thrown.\nOUTPUTS\n    output - array_like\n        The output of the neural network. Its dimension is determined\n        by the number of units in the final specified layer.",
    "original_nl": "Forward propagate the inputs through the neural network.\nINPUTS\n    inputs - array_like\n        An input to the neural network. Must match the specfied input\n        dimension or errors will be thrown.\nOUTPUTS\n    output - array_like\n        The output of the neural network. Its dimension is determined\n        by the number of units in the final specified layer."
  },
  {
    "code": "def test_vamp_bg(self):\n    vamp_bg_test(nz=1000, ny=500, ns=10, verbose=False)",
    "nl": "Run VAMP with a BG prior",
    "original_nl": "Run VAMP with a BG prior"
  },
  {
    "code": "def interpolate(self):\n    m = ((- 1.0) * (self.start - self.end))\n    b = self.start\n    x = (self.count / (self.expectedIterations - 1))\n    return ((m * x) + b)",
    "nl": "This is confusing enough just needing to know algebra...\nYou also need to realize that start, end, and the return value are all Assets classes\nthat represent collections of stocks, bonds, etc. Good luck.",
    "original_nl": "This is confusing enough just needing to know algebra...\nYou also need to realize that start, end, and the return value are all Assets classes\nthat represent collections of stocks, bonds, etc. Good luck."
  },
  {
    "code": "def c_moves_s(client):\n    return 'south'",
    "nl": "move through south exit if available",
    "original_nl": "move through south exit if available"
  },
  {
    "code": "def forever(self, key, value):\n    self.put(key, value, 0)",
    "nl": "Store an item in the cache indefinitely.\n\n",
    "original_nl": "Store an item in the cache indefinitely.\n\n:param key: The cache key\n:type key: str\n\n:param value: The value\n:type value: mixed"
  },
  {
    "code": "def call_monitor_plugin(self, callback, *args, **kwargs):\n    try:\n        reservation_flags = callback(*args, **kwargs)\n    except Exception as e:\n        LOG.exception('Caught an exception while executing a callback. %s', str(e))\n    if reservation_flags:\n        self._update_flags(reservation_flags)",
    "nl": "Call a callback and update lease/reservation flags.",
    "original_nl": "Call a callback and update lease/reservation flags."
  },
  {
    "code": "def hasLayers(self):\n    return (len(self.getTimeLayerList()) > 0)",
    "nl": "returns true if the manager has at least one layer registered",
    "original_nl": "returns true if the manager has at least one layer registered"
  },
  {
    "code": "def test_add(self):\n    self.fsdb.add(self.createTestFile())",
    "nl": "test insertion through path",
    "original_nl": "test insertion through path"
  },
  {
    "code": "@property\ndef if_none_match(self):\n    for option in self.options:\n        if (option.number == defines.OptionRegistry.IF_NONE_MATCH.number):\n            return True\n    return False",
    "nl": "Get the if-none-match option of a request.\n\n",
    "original_nl": "Get the if-none-match option of a request.\n\n:return: True, if if-none-match is present\n:rtype : bool"
  },
  {
    "code": "def to_scipy_sparse(m, **options):\n    dtype = options.get('dtype', 'complex')\n    if isinstance(m, (Matrix, Expr)):\n        return sympy_to_scipy_sparse(m, dtype=dtype)\n    elif isinstance(m, numpy_ndarray):\n        if (not sparse):\n            raise ImportError\n        return sparse.csr_matrix(m)\n    elif isinstance(m, scipy_sparse_matrix):\n        return m\n    raise TypeError(('Expected sympy/numpy/scipy.sparse matrix, got: %r' % m))",
    "nl": "Convert a sympy/numpy matrix to a scipy.sparse matrix.",
    "original_nl": "Convert a sympy/numpy matrix to a scipy.sparse matrix."
  },
  {
    "code": "@property\ndef output(self):\n    return self.args[2]",
    "nl": "The output of the test runner (if applicable).",
    "original_nl": "The output of the test runner (if applicable)."
  },
  {
    "code": "def onNameChanged(self, event):\n    role = event.source.getRole()\n    try:\n        focusRole = orca_state.locusOfFocus.getRole()\n    except:\n        focusRole = None\n    if ((role == pyatspi.ROLE_FRAME) and (focusRole == pyatspi.ROLE_TABLE_CELL)):\n        return\n    default.Script.onNameChanged(self, event)",
    "nl": "Callback for object:property-change:accessible-name events.",
    "original_nl": "Callback for object:property-change:accessible-name events."
  },
  {
    "code": "def showCountDialog(countType, counts):\n    isNoStats = (not counts)\n    noStatsMsg = \"Usage stats aren't available yet, press any key...\"\n    if isNoStats:\n        (popup, width, height) = cli.popups.init(3, (len(noStatsMsg) + 4))\n    else:\n        (popup, width, height) = cli.popups.init((4 + max(1, len(counts))), 80)\n    if (not popup):\n        return\n    try:\n        control = cli.controller.getController()\n        popup.win.box()\n        if (countType == CountType.CLIENT_LOCALE):\n            title = 'Client Locales'\n        elif (countType == CountType.EXIT_PORT):\n            title = 'Exiting Port Usage'\n        else:\n            title = ''\n            log.warn(('Unrecognized count type: %s' % countType))\n        popup.addstr(0, 0, title, curses.A_STANDOUT)\n        if isNoStats:\n            popup.addstr(1, 2, noStatsMsg, (curses.A_BOLD | uiTools.getColor('cyan')))\n        else:\n            sortedCounts = sorted(counts.iteritems(), key=operator.itemgetter(1))\n            sortedCounts.reverse()\n            (keyWidth, valWidth, valueTotal) = (3, 1, 0)\n            for (k, v) in sortedCounts:\n                keyWidth = max(keyWidth, len(k))\n                valWidth = max(valWidth, len(str(v)))\n                valueTotal += v\n            if (countType == CountType.EXIT_PORT):\n                keyWidth += EXIT_USAGE_WIDTH\n            labelFormat = ('%%-%is %%%ii (%%%%%%-2i)' % (keyWidth, valWidth))\n            for i in range((height - 4)):\n                (k, v) = sortedCounts[i]\n                if (countType == CountType.EXIT_PORT):\n                    usage = connections.getPortUsage(k)\n                    if usage:\n                        keyFormat = ('%%-%is   %%s' % (keyWidth - EXIT_USAGE_WIDTH))\n                        k = (keyFormat % (k, usage[:(EXIT_USAGE_WIDTH - 3)]))\n                label = (labelFormat % (k, v, ((v * 100) / valueTotal)))\n                popup.addstr((i + 1), 2, label, (curses.A_BOLD | uiTools.getColor('green')))\n                labelWidth = len(label)\n                fillWidth = ((v * ((width - 4) - labelWidth)) / valueTotal)\n                for j in range(fillWidth):\n                    popup.addstr((i + 1), ((3 + labelWidth) + j), ' ', (curses.A_STANDOUT | uiTools.getColor('red')))\n            popup.addstr((height - 2), 2, 'Press any key...')\n        popup.win.refresh()\n        curses.cbreak()\n        control.getScreen().getch()\n    finally:\n        cli.popups.finalize()",
    "nl": "Provides a dialog with bar graphs and percentages for the given set of\ncounts. Pressing any key closes the dialog.\n\nArguments:\n  countType - type of counts being presented\n  counts    - mapping of labels to counts",
    "original_nl": "Provides a dialog with bar graphs and percentages for the given set of\ncounts. Pressing any key closes the dialog.\n\nArguments:\n  countType - type of counts being presented\n  counts    - mapping of labels to counts"
  },
  {
    "code": "@property\ndef kml(self):\n    return ('<MultiGeometry>%s</MultiGeometry>' % ''.join((g.kml for g in self)))",
    "nl": "Returns the KML for this Geometry Collection.",
    "original_nl": "Returns the KML for this Geometry Collection."
  },
  {
    "code": "@cmdfilter\ndef acls(self, msg, cmd, args, dry_run):\n    self.log.debug(('Check %s for ACLs.' % cmd))\n    f = self._bot.all_commands[cmd]\n    cmd_str = '{plugin}:{command}'.format(plugin=f.__self__.name, command=cmd)\n    usr = get_acl_usr(msg)\n    acl = self.bot_config.ACCESS_CONTROLS_DEFAULT.copy()\n    for (pattern, acls) in self.bot_config.ACCESS_CONTROLS.items():\n        if (':' not in pattern):\n            pattern = '*:{command}'.format(command=pattern)\n        if ciglob(cmd_str, (pattern,)):\n            acl.update(acls)\n            break\n    self.log.info(('Matching ACL %s against username %s for command %s' % (acl, usr, cmd_str)))\n    if (('allowusers' in acl) and (not glob(usr, acl['allowusers']))):\n        return self.access_denied(msg, \"You're not allowed to access this command from this user\", dry_run)\n    if (('denyusers' in acl) and glob(usr, acl['denyusers'])):\n        return self.access_denied(msg, \"You're not allowed to access this command from this user\", dry_run)\n    if msg.is_group:\n        if (not isinstance(msg.frm, RoomOccupant)):\n            raise Exception(('msg.frm is not a RoomOccupant. Class of frm: %s' % msg.frm.__class__))\n        room = str(msg.frm.room)\n        if (('allowmuc' in acl) and (acl['allowmuc'] is False)):\n            return self.access_denied(msg, \"You're not allowed to access this command from a chatroom\", dry_run)\n        if (('allowrooms' in acl) and (not glob(room, acl['allowrooms']))):\n            return self.access_denied(msg, \"You're not allowed to access this command from this room\", dry_run)\n        if (('denyrooms' in acl) and glob(room, acl['denyrooms'])):\n            return self.access_denied(msg, \"You're not allowed to access this command from this room\", dry_run)\n    elif (('allowprivate' in acl) and (acl['allowprivate'] is False)):\n        return self.access_denied(msg, \"You're not allowed to access this command via private message to me\", dry_run)\n    self.log.info(('Check if %s is admin only command.' % cmd))\n    if f._err_command_admin_only:\n        if (not glob(get_acl_usr(msg), self.bot_config.BOT_ADMINS)):\n            return self.access_denied(msg, 'This command requires bot-admin privileges', dry_run)\n        if (msg.is_group and (not acl.get('allowmuc', False))):\n            return self.access_denied(msg, 'This command may only be issued through a direct message', dry_run)\n    return (msg, cmd, args)",
    "nl": "Check command against ACL rules as defined in the bot configuration.\n\n",
    "original_nl": "Check command against ACL rules as defined in the bot configuration.\n\n:param msg: The original chat message.\n:param cmd: The command name itself.\n:param args: Arguments passed to the command.\n:param dry_run: True when this is a dry-run."
  },
  {
    "code": "@classmethod\ndef create(cls, name, form, update_if_exists=False):\n    survey = cls.get(name, throw_if_not_found=False)\n    if (not survey):\n        survey = SurveyForm(name=name, form=form)\n    elif update_if_exists:\n        survey.form = form\n    else:\n        raise SurveyFormNameAlreadyExists()\n    survey.save()\n    return survey",
    "nl": "Helper class method to create a new Survey Form.\n\nupdate_if_exists=True means that if a form already exists with that name, then update it.\nOtherwise throw an SurveyFormAlreadyExists exception",
    "original_nl": "Helper class method to create a new Survey Form.\n\nupdate_if_exists=True means that if a form already exists with that name, then update it.\nOtherwise throw an SurveyFormAlreadyExists exception"
  },
  {
    "code": "def run_tests(test_names):\n    import xmlrunner\n    suite = unittest.TestSuite((unittest.defaultTestLoader.loadTestsFromModule(module) for module in import_tests(test_names)))\n    if test_names:\n        suite = filter_suite(suite, test_names)\n        if (suite.countTestCases() == 0):\n            raise Exception('No matching tests found')\n    runner = xmlrunner.XMLTestRunner(output='test.results', outsuffix='')\n    results = runner.run(suite)\n    return (len(results.errors) + len(results.failures))",
    "nl": "Runs tests using unittest, returns the number of failures.",
    "original_nl": "Runs tests using unittest, returns the number of failures."
  },
  {
    "code": "@salt.setter\ndef salt(self, value):\n    self._salt = value",
    "nl": "Updates the salt (integer) used when encrypting messages.",
    "original_nl": "Updates the salt (integer) used when encrypting messages."
  },
  {
    "code": "@staticmethod\ndef _extract_address(bitcoin_url):\n    (address_text, _) = bitcoin_url.split('?')\n    address = address_text.split(':')[1]\n    return address",
    "nl": "Extract address from bitcoin url\n",
    "original_nl": "Extract address from bitcoin url\n:param bitcoin_url: bitcoin url\n:return: Bitcoin address"
  },
  {
    "code": "def get_wrapper(self, module_class):\n    for wrapper in self.module_wrappers:\n        if (wrapper.name == get_ident_string(module_class)):\n            return wrapper\n    return None",
    "nl": "Checks if a module is loaded. Returns ModuleWrapper or None on\nfailure.",
    "original_nl": "Checks if a module is loaded. Returns ModuleWrapper or None on\nfailure."
  },
  {
    "code": "def is_celery_error(err):\n    return (isinstance(err, Exception) and (err.__class__.__module__ == 'celery.backends.base'))",
    "nl": "Because Celery when using json serialization cannot (de)serialize original exceptions,\nerrors it throws to a client are derived ones dynamically generated within package\n'celery.backends.base'. It means that static 'except' blocks are impossible and we\nmust catch Exception and investigate further. This function helps with that.",
    "original_nl": "Because Celery when using json serialization cannot (de)serialize original exceptions,\nerrors it throws to a client are derived ones dynamically generated within package\n'celery.backends.base'. It means that static 'except' blocks are impossible and we\nmust catch Exception and investigate further. This function helps with that."
  },
  {
    "code": "def quotereplacechar(char, sub, string):\n    quote = False\n    fixed_string = ''\n    for i in string:\n        if (i in [\"'\", '\"']):\n            if (not quote):\n                quote = i\n                fixed_string += i\n            elif (quote == i):\n                quote = False\n                fixed_string += i\n            else:\n                fixed_string += i\n        elif quote:\n            fixed_string += i\n        elif (i == char):\n            fixed_string += sub\n        else:\n            fixed_string += i\n    return fixed_string",
    "nl": "Quote respecting replace.",
    "original_nl": "Quote respecting replace."
  },
  {
    "code": "def len(_: Seq) -> int:\n    pass",
    "nl": "Return the number of items in a container.",
    "original_nl": "Return the number of items in a container."
  },
  {
    "code": "def read_arguments():\n    parser = ArgumentParser(description=('Convert RTI Connext logs in ' + 'human-readable format.'))\n    parser.add_argument('-i', '--input', help='log file path, by default stdin')\n    parser.add_argument('-v', action='count', help=\"verbosity level - increased by multiple 'v'\")\n    parser.add_argument('--output', '-o', help='write the output into the specified file')\n    parser.add_argument('--overwrite-output', '-oo', help='write the output into a new/truncated file')\n    parser.add_argument('--write-original', help='write the original log output into a file')\n    parser.add_argument('--show-ip', action='store_true', help='show the IP address instead of an assigned name')\n    parser.add_argument('--obfuscate', action='store_true', help='hide sensitive information like IP addresses')\n    parser.add_argument('--salt', '-s', help='salt for obfuscation - from random if not set')\n    parser.add_argument('--show-timestamp', '-t', action='store_true', help='show timestamp log field')\n    parser.add_argument('--show-lines', action='store_true', help='print the original and parsed log lines')\n    parser.add_argument('--only', help='show only log messages that match the regex')\n    parser.add_argument('--colors', '-c', action='store_true', help='apply colors to log messages (e.g.: warnings)')\n    parser.add_argument('--highlight', help='show in bold regex matched logs, requires -c')\n    parser.add_argument('--local-host', help='set the local address')\n    parser.add_argument('--no-network', action='store_true', help='do not show the network related logs')\n    parser.add_argument('--no-inline', action='store_true', help='do not show warnigns and errors in network logs')\n    parser.add_argument('--no-stats', action='store_true', help='do not show the network and packet statistics')\n    parser.add_argument('--no-progress', action='store_true', help='do not show the interative info at the bottom')\n    parser.add_argument('--debug', action='store_true', help='debug mode - export unmatched logs')\n    parser.add_argument('--version', action='version', help='show the program version', version=('%(prog)s ' + __version__))\n    return parser.parse_args()",
    "nl": "Parse the command-line arguments.",
    "original_nl": "Parse the command-line arguments."
  },
  {
    "code": "def testAddArguments(self):\n    argument_parser = argparse.ArgumentParser(prog='cli_helper.py', description='Test argument parser.', add_help=False, formatter_class=cli_test_lib.SortedArgumentsHelpFormatter)\n    zeromq.ZeroMQArgumentsHelper.AddArguments(argument_parser)\n    output = self._RunArgparseFormatHelp(argument_parser)\n    self.assertEqual(output, self._EXPECTED_OUTPUT)",
    "nl": "Tests the AddArguments function.",
    "original_nl": "Tests the AddArguments function."
  },
  {
    "code": "def retrieve_alias(self, email, **kwargs):\n    uri = self.MakeMultidomainAliasProvisioningUri(email=email)\n    return self.GetEntry(uri, desired_class=gdata.apps.multidomain.data.AliasEntry, **kwargs)",
    "nl": "Retrieves a single alias in the domain.\n\nArgs:\n  email: string The email address of the alias to be retrieved\n  kwargs: The other parameters to pass to gdata.client.GDClient.GetEntry()\n",
    "original_nl": "Retrieves a single alias in the domain.\n\nArgs:\n  email: string The email address of the alias to be retrieved\n  kwargs: The other parameters to pass to gdata.client.GDClient.GetEntry()\n\nReturns:\n  A gdata.apps.multidomain.data.AliasEntry representing the alias"
  },
  {
    "code": "@property\ndef seekable(self):\n    return self.props.seekable",
    "nl": "If the current song can be seeked, in case it's not clear defaults\nto True. See the \"seekable\" GObject property for notifications.",
    "original_nl": "If the current song can be seeked, in case it's not clear defaults\nto True. See the \"seekable\" GObject property for notifications."
  },
  {
    "code": "def all(self):\n    for (checksum, path) in self.hash_db.items():\n        (yield (checksum, path))",
    "nl": "Generator to get all entries from self.hash_db\n\n",
    "original_nl": "Generator to get all entries from self.hash_db\n\n:returns tuple(string)"
  },
  {
    "code": "def attach(self, widget):\n    if self.is_attached:\n        return\n    if (not self.events):\n        return\n    if self.debug:\n        print('Attach:', self)\n    self.doAttach(widget.real_widget)\n    self.widget_ref = weakref.ref(widget)\n    self.is_attached = True",
    "nl": "Start receiving events.\nNo need to call this manually.",
    "original_nl": "Start receiving events.\nNo need to call this manually."
  },
  {
    "code": "def get_package_versions(lines):\n    versions = {\n        \n    }\n    for line in lines:\n        line = line.strip()\n        if ((len(line) == 0) or line.startswith('#') or line.startswith('-r ')):\n            continue\n        if line.startswith('https://'):\n            continue\n        (name, version_plus) = line.split('==', 1)\n        versions[name.lower()] = version_plus.split(' ', 1)[0]\n    return versions",
    "nl": "Return a dictionary of package versions.",
    "original_nl": "Return a dictionary of package versions."
  },
  {
    "code": "@patch('__builtin__.open', mock_open())\ndef test_failure_on_diffquality_jshint(self):\n    self._mock_paver_sh.side_effect = CustomShMock().fail_on_jshint\n    _mock_pep8_violations = MagicMock(return_value=(0, []))\n    with patch('pavelib.quality._get_pep8_violations', _mock_pep8_violations):\n        with self.assertRaises(SystemExit):\n            pavelib.quality.run_quality('')\n            self.assertRaises(BuildFailure)\n    self.assertEqual(_mock_pep8_violations.call_count, 1)\n    self.assertEqual(self._mock_paver_sh.call_count, 2)",
    "nl": "If diff-quality fails on jshint, the paver task should also fail",
    "original_nl": "If diff-quality fails on jshint, the paver task should also fail"
  },
  {
    "code": "def getEstimatorParamMaps(self):\n    return self.getOrDefault(self.estimatorParamMaps)",
    "nl": "Gets the value of estimatorParamMaps or its default value.",
    "original_nl": "Gets the value of estimatorParamMaps or its default value."
  },
  {
    "code": "def parse_url(url):\n    parsed = urlparse.urlparse(url)\n    queries = urlparse.parse_qs(parsed.query)\n    path = parsed.path.split('/')\n    return (queries['id'][0] if ('id' in queries) else path[3])",
    "nl": "Return the 3rd part of the url or get id param if it exists.\n\nArgs:\n    url: google drive url\n",
    "original_nl": "Return the 3rd part of the url or get id param if it exists.\n\nArgs:\n    url: google drive url\n\nReturns:\n    url id"
  },
  {
    "code": "@mock.patch('olympia.devhub.utils.chain')\ndef test_run_once_file_upload(self, chain):\n    task = mock.Mock()\n    chain.return_value = task\n    task.delay.return_value = mock.Mock(task_id='42')\n    assert isinstance(tasks.validate(self.file_upload, listed=True), mock.Mock)\n    assert (task.delay.call_count == 1)\n    assert isinstance(tasks.validate(self.file_upload, listed=True), AsyncResult)\n    assert (task.delay.call_count == 1)",
    "nl": "Tests that only a single validation task is run for a given file\nupload.",
    "original_nl": "Tests that only a single validation task is run for a given file\nupload."
  },
  {
    "code": "def eval_instance(self, best_path, gold):\n    total_labels = len(best_path)\n    correct_labels = np.sum(np.equal(best_path, gold))\n    gold_chunks = utils.iobes_to_spans(gold, self.r_l_map)\n    gold_count = len(gold_chunks)\n    guess_chunks = utils.iobes_to_spans(best_path, self.r_l_map)\n    guess_count = len(guess_chunks)\n    overlap_chunks = (gold_chunks & guess_chunks)\n    overlap_count = len(overlap_chunks)\n    return (correct_labels, total_labels, gold_count, guess_count, overlap_count)",
    "nl": "update statics for one instance\n\nargs: \n    best_path (seq_len): predicted\n    gold (seq_len): ground-truth",
    "original_nl": "update statics for one instance\n\nargs: \n    best_path (seq_len): predicted\n    gold (seq_len): ground-truth"
  },
  {
    "code": "def load_bitmap(filename, width=(- 1), height=(- 1)):\n    i = load_icon(filename, width, height)\n    b = wx.EmptyBitmap(i.GetWidth(), i.GetHeight())\n    b.CopyFromIcon(i)\n    return b",
    "nl": "load icon as a wx.Bitmap",
    "original_nl": "load icon as a wx.Bitmap"
  },
  {
    "code": "def setup_masquerade(request, course_key, staff_access=False, reset_masquerade_data=False):\n    if ((request.user is None) or (not settings.FEATURES.get('ENABLE_MASQUERADE', False)) or (not staff_access)):\n        return (None, request.user)\n    if reset_masquerade_data:\n        request.session.pop(MASQUERADE_DATA_KEY, None)\n    masquerade_settings = request.session.setdefault(MASQUERADE_SETTINGS_KEY, {\n        \n    })\n    request.user.masquerade_settings = masquerade_settings\n    course_masquerade = masquerade_settings.get(course_key, None)\n    masquerade_user = None\n    if (course_masquerade and course_masquerade.user_name):\n        try:\n            masquerade_user = CourseEnrollment.objects.users_enrolled_in(course_key).get(username=course_masquerade.user_name)\n        except User.DoesNotExist:\n            course_masquerade = None\n            del masquerade_settings[course_key]\n            request.session.modified = True\n        else:\n            masquerade_user.masquerade_settings = request.user.masquerade_settings\n            masquerade_user.real_user = request.user\n    return (course_masquerade, (masquerade_user or request.user))",
    "nl": "Sets up masquerading for the current user within the current request. The request's user is\nupdated to have a 'masquerade_settings' attribute with the dict of all masqueraded settings if\ncalled from within a request context. The function then returns a pair (CourseMasquerade, User)\nwith the masquerade settings for the specified course key or None if there isn't one, and the\nuser we are masquerading as or request.user if masquerading as a specific user is not active.\n\nIf the reset_masquerade_data flag is set, the field data stored in the session will be cleared.",
    "original_nl": "Sets up masquerading for the current user within the current request. The request's user is\nupdated to have a 'masquerade_settings' attribute with the dict of all masqueraded settings if\ncalled from within a request context. The function then returns a pair (CourseMasquerade, User)\nwith the masquerade settings for the specified course key or None if there isn't one, and the\nuser we are masquerading as or request.user if masquerading as a specific user is not active.\n\nIf the reset_masquerade_data flag is set, the field data stored in the session will be cleared."
  },
  {
    "code": "@staticmethod\ndef from_tsvs(tsvs, bucketized_float_cols=[], string_cols=[], raw_float_cols=[]):\n    d = _DataStore()\n    d.load_tsv(tsvs, bucketized_float_cols=bucketized_float_cols, string_cols=string_cols, raw_float_cols=raw_float_cols)\n    return DataStore(d)",
    "nl": "Loads data from tsvs.\nInputs:\n  tsvs: Blocks of tsvs, among which only the first contains header.\n  bucketized_float_cols: Float columns that will be bucketized. All features will be bucketized.\n  string_cols: String cols.\n  raw_float_cols: Float columns that are loaded raw. Target columns are usually not bucketized.",
    "original_nl": "Loads data from tsvs.\nInputs:\n  tsvs: Blocks of tsvs, among which only the first contains header.\n  bucketized_float_cols: Float columns that will be bucketized. All features will be bucketized.\n  string_cols: String cols.\n  raw_float_cols: Float columns that are loaded raw. Target columns are usually not bucketized."
  },
  {
    "code": "def unzip_file(self, filename, pattern='*'):\n    tar = tarfile.open(name=filename)\n    dir = tempfile.mkdtemp(prefix='tmp_import_custom')\n    tar.extractall(path=dir)\n    return (dir, (glob.glob(('%s/%s' % (dir, pattern))) + glob.glob(('%s/*/%s' % (dir, pattern)))))",
    "nl": "extract *.tar.gz files\n",
    "original_nl": "extract *.tar.gz files\n\nreturns list of extracted file names"
  },
  {
    "code": "def validate(self, attrs):\n    verification_deadline = attrs.get('verification_deadline', None)\n    if verification_deadline:\n        upgrade_deadline = None\n        for mode in attrs['modes']:\n            expires = mode.get('expiration_datetime')\n            if expires:\n                upgrade_deadline = min(expires, (upgrade_deadline or datetime.max.replace(tzinfo=pytz.utc)))\n        if ((upgrade_deadline is not None) and (verification_deadline < upgrade_deadline)):\n            raise serializers.ValidationError('Verification deadline must be after the course mode upgrade deadlines.')\n    return attrs",
    "nl": "Ensure the verification deadline occurs AFTER the course mode enrollment deadlines.",
    "original_nl": "Ensure the verification deadline occurs AFTER the course mode enrollment deadlines."
  },
  {
    "code": "def handle_lib_name(lib_name, basedir):\n    if isinstance(lib_name, dict):\n        lib_names = to_tuple(select_platform_value(lib_name))\n    else:\n        lib_names = to_tuple(lib_name)\n    for try_name in lib_names:\n        if os.path.isabs(try_name):\n            path = try_name\n        else:\n            path = os.path.join(basedir, try_name)\n        if os.path.exists(path):\n            return path\n    for try_name in lib_names:\n        path = find_library(try_name)\n        if path:\n            return path\n    raise ValueError(\"Cannot find library '{}'\".format(lib_names))",
    "nl": "Find the path to the library\n\n`lib_name` can be specified directly as a string (or sequence of strings), or within a\n\"platform\" dict. If multiple libs are specified, the first one found will be returned.",
    "original_nl": "Find the path to the library\n\n`lib_name` can be specified directly as a string (or sequence of strings), or within a\n\"platform\" dict. If multiple libs are specified, the first one found will be returned."
  },
  {
    "code": "@property\ndef datafile(self):\n    return self.outfile",
    "nl": "LAMMPS data file name.",
    "original_nl": "LAMMPS data file name."
  },
  {
    "code": "def print_variant(variant_line=None, variant_dict=None, header_line=None, priority=None, outfile=None, mode='vcf', silent=False):\n    if variant_dict:\n        if (not header_line):\n            raise IOError('Print line needs a header_line when printing variant dict.')\n        print_line = [variant_dict.get(entry, '.') for entry in header_line]\n    else:\n        print_line = variant_line.rstrip().split('\\t')\n    if (mode == 'modified'):\n        print_line = print_line[1:]\n    elif priority:\n        print_line = ([priority] + print_line)\n    print_string = '\\t'.join(print_line)\n    if (not isinstance(print_string, str)):\n        print_string = print_string.encode('utf-8')\n    if outfile:\n        outfile.write((print_string + '\\n'))\n    elif (not silent):\n        print(print_string)\n    return",
    "nl": "Print a variant line.\n\nIf a result file is provided the variante will be appended to the file, \notherwise they are printed to stdout.\n\nThere are two modes, 'vcf' or 'modified'.\nIf 'vcf' we expect plain vcf variants and print them as they came in.\nIf 'modified' the first column has been used for sorting so we skip \nthat one.\n\nArgs:\n    variant_line (str): A vcf formatted variant line\n    variant_dict (dict): A variant dictionary\n    header_line (list): A list with haeder columns\n    priority (str): the priority for this variant\n    outfile (FileHandle): An opened file_handle\n    mode (str): 'vcf' or 'modified'\n    silent (bool): Bool. If nothing should be printed.",
    "original_nl": "Print a variant line.\n\nIf a result file is provided the variante will be appended to the file, \notherwise they are printed to stdout.\n\nThere are two modes, 'vcf' or 'modified'.\nIf 'vcf' we expect plain vcf variants and print them as they came in.\nIf 'modified' the first column has been used for sorting so we skip \nthat one.\n\nArgs:\n    variant_line (str): A vcf formatted variant line\n    variant_dict (dict): A variant dictionary\n    header_line (list): A list with haeder columns\n    priority (str): the priority for this variant\n    outfile (FileHandle): An opened file_handle\n    mode (str): 'vcf' or 'modified'\n    silent (bool): Bool. If nothing should be printed."
  },
  {
    "code": "def test_prod2d(self):\n    npoints = 2000\n    variance = self.prod_variance(2)\n    self.run_all(self.prod, npoints, 0.25, variance, xl=[0.0, 0.0], xu=[1.0, 1.0])",
    "nl": "f(x,y) = x*y between 0 and 1.",
    "original_nl": "f(x,y) = x*y between 0 and 1."
  },
  {
    "code": "@staticmethod\ndef get_elastic_mappings(es_major):\n    if (es_major != '2'):\n        mapping = '\\n            {\\n                \"properties\": {\\n                    \"title_analyzed\": {\\n                        \"type\": \"text\"\\n                    }\\n               }\\n            } '\n    else:\n        mapping = '\\n                {\\n                    \"properties\": {\\n                        \"title_analyzed\": {\\n                            \"type\": \"string\",\\n                             \"index\": \"analyzed\"\\n                        }\\n                   }\\n                } '\n    return {\n        'items': mapping,\n    }",
    "nl": "Get Elasticsearch mapping.\n\n",
    "original_nl": "Get Elasticsearch mapping.\n\n:param es_major: major version of Elasticsearch, as string\n:returns:        dictionary with a key, 'items', with the mapping"
  },
  {
    "code": "def clear_cookies(self):\n    lib.ph_context_clear_cookies()",
    "nl": "Clear all cookies.",
    "original_nl": "Clear all cookies."
  },
  {
    "code": "def align_to_target(self, target_term_doc_mat):\n    self.priors = self.priors[target_term_doc_mat.get_terms()].fillna(0)\n    return self",
    "nl": "Parameters\n----------\ntarget_term_doc_mat : TermDocMatrix\n",
    "original_nl": "Parameters\n----------\ntarget_term_doc_mat : TermDocMatrix\n\nReturns\n-------\nPriorFactory"
  },
  {
    "code": "def test_DoubleQuoteString(self):\n    manhole.lastColorizedLine('\"1\"')",
    "nl": "Colorize an integer in double quotes.",
    "original_nl": "Colorize an integer in double quotes."
  },
  {
    "code": "def test_perform1(self):\n    self.task.write_in_database('val', 'World')\n    self.task.message = 'Hello {Test_val}'\n    self.root.prepare()\n    self.task.perform()\n    assert (self.task.get_from_database('Test_message') == 'Hello World')",
    "nl": "Test checking that the message value gets written to the database",
    "original_nl": "Test checking that the message value gets written to the database"
  },
  {
    "code": "def buildFlattener():\n    with IsolatedSession() as issn:\n        mat_input = tf.placeholder(tf.float32, [None, None])\n        mat_output = tf.identity(tf.reshape(mat_input, shape=[(- 1)]), name='output')\n        gfn = issn.asGraphFunction([mat_input], [mat_output])\n    return gfn",
    "nl": "Build a flattening layer to remove the extra leading tensor dimension.\ne.g. a tensor of shape [1, W, H, C] will have a shape [W, H, C] after applying this.",
    "original_nl": "Build a flattening layer to remove the extra leading tensor dimension.\ne.g. a tensor of shape [1, W, H, C] will have a shape [W, H, C] after applying this."
  },
  {
    "code": "def parse_term(s):\n    (size, s) = s.split(b':', 1)\n    size = int(size)\n    return (s[:size], s[size:])",
    "nl": "Parse single s-expr term from bytes.",
    "original_nl": "Parse single s-expr term from bytes."
  },
  {
    "code": "def verify_tooltips_displayed(self):\n    for (index, tab) in enumerate(self.q(css='#sequence-list > li')):\n        ActionChains(self.browser).move_to_element(tab).perform()\n        self.wait_for_element_visibility('#tab_{index} > .sequence-tooltip'.format(index=index), 'Tab {index} should appear'.format(index=index))",
    "nl": "Verify that all sequence navigation bar tooltips are being displayed upon mouse hover.\n\nIf a tooltip does not appear, raise a BrokenPromise.",
    "original_nl": "Verify that all sequence navigation bar tooltips are being displayed upon mouse hover.\n\nIf a tooltip does not appear, raise a BrokenPromise."
  },
  {
    "code": "def test_error_log_message(self, skip_if_no_fixture, caplog, simple_log, request):\n    simple_log.error('test message for error')\n    if ('.' in request.module.__name__):\n        mod_name = request.module.__name__.split('.', 1)[1]\n    else:\n        mod_name = request.module.__name__\n    for records in caplog.records():\n        assert (records.message == 'test message for error')\n        assert (records.levelname == 'ERROR')\n        assert (records.module == mod_name)\n        assert (records.classname == 'TestLogger.')\n        assert (records.funcName == 'test_error_log_message')\n        thread_name = threading.current_thread().name\n        assert (records.threadName == thread_name)",
    "nl": "Verify that log message for level ERROR contains correct values.",
    "original_nl": "Verify that log message for level ERROR contains correct values."
  },
  {
    "code": "@property\ndef hasz(self):\n    return self._z",
    "nl": "Returns whether this coordinate sequence is 3D.  This property value is\ninherited from the parent Geometry.",
    "original_nl": "Returns whether this coordinate sequence is 3D.  This property value is\ninherited from the parent Geometry."
  },
  {
    "code": "def listele(self):\n    key = self.current.task_data['etkinlik_basvuru_id']\n    etkinlik = BAPEtkinlikProje.objects.get(key)\n    self.output['objects'] = [[_('Etkinlik Ba\u015fl\u0131\u011f\u0131'), _('Ba\u015fvuran'), _('Durum')]]\n    list_item = {\n        'fields': [etkinlik.bildiri_basligi, etkinlik.basvuru_yapan.__unicode__(), etkinlik.durum],\n        'actions': [{\n            'name': _('G\u00f6r\u00fcnt\u00fcle'),\n            'cmd': 'goruntule',\n            'mode': 'normal',\n            'show_as': 'button',\n        }],\n        'key': key,\n    }\n    self.output['objects'].append(list_item)\n    form = JsonForm(title=_(('%s Etkinlik Ba\u015fvuru De\u011ferlendirmesi' % etkinlik.__unicode__())))\n    form.daha_sonra_karar_ver = fields.Button(_('Daha Sonra Karar Ver'), cmd='daha_sonra_karar_ver')\n    self.form_out(form)",
    "nl": "Komisyon \u00fcyesinin kendisine gelen etkinlik ba\u015fvurusunu liste \u015feklinde g\u00f6rd\u00fc\u011f\u00fc ad\u0131md\u0131r.",
    "original_nl": "Komisyon \u00fcyesinin kendisine gelen etkinlik ba\u015fvurusunu liste \u015feklinde g\u00f6rd\u00fc\u011f\u00fc ad\u0131md\u0131r."
  },
  {
    "code": "def _GetProvides(sources):\n    provides = set()\n    for source in sources:\n        provides.update(source.provides)\n    return provides",
    "nl": "Get all namespaces provided by a collection of sources.",
    "original_nl": "Get all namespaces provided by a collection of sources."
  },
  {
    "code": "def test_060_statement(self):\n    attr = self.mock_attr_factory('name60')\n    self.assertEqual('attribute name60;', attr.statement())",
    "nl": "TypeAttribute basic statement",
    "original_nl": "TypeAttribute basic statement"
  },
  {
    "code": "def get_command_info(self):\n    return self.command_info",
    "nl": "Returns the ChildInfo object for the current command or level",
    "original_nl": "Returns the ChildInfo object for the current command or level"
  },
  {
    "code": "def all_parents(self):\n    result = {self}\n    for parent in self.parents:\n        result.update(parent.all_parents())\n    return result",
    "nl": "Returns all transitive parent nodes.",
    "original_nl": "Returns all transitive parent nodes."
  },
  {
    "code": "def __init__(self, pin):\n    Illuminator__.__init__(self, pin)",
    "nl": "create an instance of a LED connected to the \npin provided.\nPin should be configured as Pin('X1', Pin.OUT_PP)",
    "original_nl": "create an instance of a LED connected to the \npin provided.\nPin should be configured as Pin('X1', Pin.OUT_PP)"
  },
  {
    "code": "def _get_num_pages(self):\n    if (self._num_pages is None):\n        hits = ((self.count - 1) - self.orphans)\n        if (hits < 1):\n            hits = 0\n        if ((hits == 0) and (not self.allow_empty_first_page)):\n            self._num_pages = 0\n        else:\n            self._num_pages = ((hits // self.per_page) + 1)\n    return self._num_pages",
    "nl": "Returns the total number of pages.",
    "original_nl": "Returns the total number of pages."
  },
  {
    "code": "def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\n    placemarks = []\n    klass = get_model(label, model)\n    if (not klass):\n        raise Http404(('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model)))\n    if field_name:\n        try:\n            info = klass._meta.get_field_by_name(field_name)\n            if (not isinstance(info[0], GeometryField)):\n                raise Exception\n        except:\n            raise Http404('Invalid geometry field.')\n    connection = connections[using]\n    if connection.ops.postgis:\n        placemarks = klass._default_manager.using(using).kml(field_name=field_name)\n    else:\n        placemarks = []\n        if connection.ops.oracle:\n            qs = klass._default_manager.using(using).transform(4326, field_name=field_name)\n        else:\n            qs = klass._default_manager.using(using).all()\n        for mod in qs:\n            setattr(mod, 'kml', getattr(mod, field_name).kml)\n            placemarks.append(mod)\n    if compress:\n        render = render_to_kmz\n    else:\n        render = render_to_kml\n    return render('gis/kml/placemarks.kml', {\n        'places': placemarks,\n    })",
    "nl": "This view generates KML for the given app label, model, and field name.\n\nThe model's default manager must be GeoManager, and the field name\nmust be that of a geographic field.",
    "original_nl": "This view generates KML for the given app label, model, and field name.\n\nThe model's default manager must be GeoManager, and the field name\nmust be that of a geographic field."
  },
  {
    "code": "def get_entity_class(resource):\n    reg = get_current_registry()\n    if (IInterface in provided_by(resource)):\n        ent_cls = reg.getUtility(resource, name='entity-class')\n    else:\n        ent_cls = reg.getAdapter(resource, IEntity, name='entity-class')\n    return ent_cls",
    "nl": "Returns the entity class registered for the given registered resource.\n\n",
    "original_nl": "Returns the entity class registered for the given registered resource.\n\n:param resource: registered resource\n:type collection: class implementing or instance providing a registered\n    resource interface.\n:return: entity class\n    (class implementing `everest.entities.interfaces.IEntity`)"
  },
  {
    "code": "def SpecToYAML(spec, level=0):\n    lines = []\n    for (k, v) in spec.iteritems():\n        if (k[0] is '_'):\n            continue\n        elif isinstance(v, Param):\n            com = ('# ' if (v.default is None) else '')\n            sv = v.decoratedString()\n            lines.append(('%s%s%s: %s' % (('    ' * level), com, k, sv)))\n        elif isinstance(v, dict):\n            lines.append(('%s%s:' % (('    ' * level), k)))\n            lines.append(SpecToYAML(v, (level + 1)))\n        else:\n            raise Exception(('Malformed config spec. ' + 'Should be dict tree with Param leaves'))\n    return '\\n'.join(lines)",
    "nl": "Convert a config spec to YAML",
    "original_nl": "Convert a config spec to YAML"
  },
  {
    "code": "def refresh_access_token(self, raw_token):\n    raise NotImplementedError('Defined in a sub-class')",
    "nl": "Refreshing an OAuth2 token using a refresh token.",
    "original_nl": "Refreshing an OAuth2 token using a refresh token."
  },
  {
    "code": "def to_glyphs_master_user_data(self, ufo, master):\n    target_user_data = master.userData\n    for (key, value) in ufo.lib.items():\n        if _user_data_has_no_special_meaning(key):\n            target_user_data[key] = value\n    if ufo.data.fileNames:\n        from glyphsLib.types import BinaryData\n        ufo_data = {\n            \n        }\n        for os_filename in ufo.data.fileNames:\n            filename = posixpath.join(*os_filename.split(os.path.sep))\n            ufo_data[filename] = BinaryData(ufo.data[os_filename])\n        master.userData[UFO_DATA_KEY] = ufo_data",
    "nl": "Set the GSFontMaster userData from the UFO master-specific lib data.",
    "original_nl": "Set the GSFontMaster userData from the UFO master-specific lib data."
  },
  {
    "code": "def analyze_apk(self, eandro_apk):\n    if (eandro_apk is not None):\n        res = AnalyzeUtil.analyze_apk(eandro_apk, self.androscripts, self.min_script_needs, reset_scripts=True)\n        if (res is not None):\n            (fastapk, scripts) = res\n            res[1] = deepcopy(scripts)\n            clilog.debug('analyzed %s', fastapk.short_description())\n            return res",
    "nl": "Analyze the `eandro_apk` and return the analysis results.\n",
    "original_nl": "Analyze the `eandro_apk` and return the analysis results.\n\nParameters\n----------\neandro_apk : EAndroApk\n    The apk to analyze.\n\nReturns\n-------\nlist<FastApk, AndroScript>\nNone\n    If error happened."
  },
  {
    "code": "def test_match_slack_github_username(self):\n    name = slack._match_slack_github_username(self.USERS, GENERIC_USERNAME)\n    self.assertEqual(name, GENERIC_USERNAME)\n    users = []\n    name = slack._match_slack_github_username(users, GENERIC_USERNAME)\n    self.assertIsNone(name)",
    "nl": "Test matching a slack and github username.",
    "original_nl": "Test matching a slack and github username."
  },
  {
    "code": "@wrappers.Request.application\ndef _serve_runs(self, request):\n    if self._db_connection_provider:\n        db = self._db_connection_provider()\n        cursor = db.execute('\\n        SELECT\\n          run_name,\\n          started_time IS NULL as started_time_nulls_last,\\n          started_time\\n        FROM Runs\\n        ORDER BY started_time_nulls_last, started_time, run_name\\n      ')\n        run_names = [row[0] for row in cursor]\n    else:\n        run_names = sorted(self._multiplexer.Runs())\n\n        def get_first_event_timestamp(run_name):\n            try:\n                return self._multiplexer.FirstEventTimestamp(run_name)\n            except ValueError:\n                tf.logging.warning('Unable to get first event timestamp for run %s', run_name)\n                return float('inf')\n        run_names.sort(key=get_first_event_timestamp)\n    return http_util.Respond(request, run_names, 'application/json')",
    "nl": "Serve a JSON array of run names, ordered by run started time.\n\nSort order is by started time (aka first event time) with empty times sorted\nlast, and then ties are broken by sorting on the run name.",
    "original_nl": "Serve a JSON array of run names, ordered by run started time.\n\nSort order is by started time (aka first event time) with empty times sorted\nlast, and then ties are broken by sorting on the run name."
  },
  {
    "code": "@rpc_method(name='TestCaseStatus.filter')\ndef filter(query):\n    return TestCaseStatus.to_xmlrpc(query)",
    "nl": ".. function:: XML-RPC TestCaseStatus.filter(query)\n\n    Search and return the list of test case statuses.\n\n    ",
    "original_nl": ".. function:: XML-RPC TestCaseStatus.filter(query)\n\n    Search and return the list of test case statuses.\n\n    :param query: Field lookups for :class:`tcms.testcases.models.TestCaseStatus`\n    :type query: dict\n    :return: Serialized list of :class:`tcms.testcases.models.TestCaseStatus` objects\n    :rtype: list(dict)"
  },
  {
    "code": "def admins(self):\n    return self.root_admins",
    "nl": "Returns the list of admins accounts\n@rtype: List\n@return: list of admin accounts",
    "original_nl": "Returns the list of admins accounts\n@rtype: List\n@return: list of admin accounts"
  },
  {
    "code": "def setUp(self):\n    self.factory = RequestFactory()\n    ConversionRate.objects.create(from_amount=1, to_amount=5, source='etherdelta', from_currency='ETH', to_currency='USDT', timestamp=datetime(2018, 1, 1))\n    ConversionRate.objects.create(from_amount=1, to_amount=2, source='etherdelta', from_currency='ETH', to_currency='USDT')\n    ConversionRate.objects.create(from_amount=1, to_amount=3, source='etherdelta', from_currency='ETH', to_currency='USDT')",
    "nl": "Perform setup for the testcase.",
    "original_nl": "Perform setup for the testcase."
  },
  {
    "code": "def filename_to_uri(self, filename):\n    try:\n        return self._uri_cache[filename]\n    except KeyError:\n        value = self._relativeize(filename)\n        self._uri_cache[filename] = value\n        return value",
    "nl": "Convert the given filename to a uri relative to \nthis TemplateCollection.",
    "original_nl": "Convert the given filename to a uri relative to \nthis TemplateCollection."
  },
  {
    "code": "def write_text(self, text):\n    self.write('<p>')\n    self.write(text)\n    self.write('</p>')",
    "nl": "Writes a paragraph of text",
    "original_nl": "Writes a paragraph of text"
  },
  {
    "code": "def fixModelRefs(self, phrasedml_str):\n    model_ref = re.compile('^.*\\\\s*model\\\\s*\"([^\"]*)\"\\\\s*$')\n    out_str = ''\n    for line in phrasedml_str.splitlines():\n        match = model_ref.match(line)\n        if match:\n            filename = match.group(1)\n            if self.isInRootDir(filename):\n                line = line.replace(filename, self.formatResource(filename))\n        out_str += (line + '\\n')\n    return out_str",
    "nl": "Changes all references of type myModel.xml to myModel.",
    "original_nl": "Changes all references of type myModel.xml to myModel."
  },
  {
    "code": "def _adjust_width(self):\n    if (self.bar_width > self.max_iter):\n        self.bar_width = int(self.max_iter)",
    "nl": "Shrinks bar if number of iterations is less than the bar width",
    "original_nl": "Shrinks bar if number of iterations is less than the bar width"
  },
  {
    "code": "@pytest.mark.parametrize(('imsize', 'upsample_factor', 'offset'), list(itertools.product(range(11, 27), range(1, 25), ((- 1.5), (- 0.5), 0.5, 1.5))))\ndef test_gaussian_upsample_oddoffsets(imsize, upsample_factor, offset):\n    x = np.arange(imsize)\n    g = gaussian(((x - ((imsize - 1) / 2.0)) - offset))\n    (xz, z) = zoom.zoom1d(g, upsample_factor, outsize=(g.size * upsample_factor), return_xouts=True)\n    assert (((gaussian(((xz - ((imsize - 1) / 2.0)) - offset)) - z) ** 2).sum() < ((4e-05 * upsample_factor) + (0.12 * (imsize ** (- 1.83)))))",
    "nl": "Test that, when upsampled, z[::upsample_factor] = g\n\nTested for offsets in the *input gaussian function*\n\nThis doesn't imply you know what's going on, just that the behavior is consistent\n\nONLY tests for OUTSIZE=INSIZE*UPSAMPLE_FACTOR",
    "original_nl": "Test that, when upsampled, z[::upsample_factor] = g\n\nTested for offsets in the *input gaussian function*\n\nThis doesn't imply you know what's going on, just that the behavior is consistent\n\nONLY tests for OUTSIZE=INSIZE*UPSAMPLE_FACTOR"
  },
  {
    "code": "def _checkPasswordConfirm(self, editable=None, reset_status=None):\n    if reset_status:\n        return GUICheck.CHECK_OK\n    pw = self.pw.get_text()\n    confirm = self.confirm.get_text()\n    if (((not pw) and (not confirm)) and self._kickstarted):\n        result = GUICheck.CHECK_OK\n    elif (confirm and (pw != confirm)):\n        result = _(PASSWORD_CONFIRM_ERROR_GUI)\n    else:\n        result = GUICheck.CHECK_OK\n    if (result == GUICheck.CHECK_OK):\n        if (editable == self.confirm):\n            self._password_check.update_check_status(check_data=True)\n        else:\n            self._confirm_check.update_check_status(check_data=True)\n    return result",
    "nl": "Check whether the password matches the confirmation data.",
    "original_nl": "Check whether the password matches the confirmation data."
  },
  {
    "code": "def login_required(func):\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if (not g.current_user.is_active):\n            flash_notice('Bitte melde dich an.')\n            return redirect_to('authentication.login_form')\n        return func(*args, **kwargs)\n    return wrapper",
    "nl": "Ensure the current user has logged in.",
    "original_nl": "Ensure the current user has logged in."
  },
  {
    "code": "def test_read_gen1():\n    (ref_seq, vcf) = load_data()\n    nodes = rgen.create_node_list(ref_seq, ref_start_pos=1, vl=vcf[0]['v'][1])\n    assert (rgen.generate_read(1, 10, 0, 3, nodes) == (1, '4=1X3=2I', [0, 3], 'ATGATGTATT'))\n    assert (rgen.generate_read(2, 10, 0, 3, nodes) == (2, '3=1X3=3I', [0, 3], 'TGATGTATTT'))\n    assert (rgen.generate_read(3, 10, 0, 4, nodes) == (3, '2=1X3=3I1=', [0, 3], 'GATGTATTTT'))\n    assert (rgen.generate_read(4, 10, 0, 4, nodes) == (4, '1=1X3=3I2=', [0, 3], 'ATGTATTTTC'))\n    assert (rgen.generate_read(5, 10, 1, 4, nodes) == (5, '1X3=3I3=', [0, 3], 'TGTATTTTCC'))\n    assert (rgen.generate_read(6, 10, 2, 6, nodes) == (6, '3=3I3=2D1=', [3, (- 2)], 'GTATTTTCCG'))\n    assert (rgen.generate_read(7, 10, 2, 6, nodes) == (7, '2=3I3=2D2=', [3, (- 2)], 'TATTTTCCGG'))\n    assert (rgen.generate_read(8, 10, 2, 6, nodes) == (8, '1=3I3=2D3=', [3, (- 2)], 'ATTTTCCGGA'))\n    assert (rgen.generate_read(9, 10, 3, 6, nodes) == (9, '3I3=2D4=', [3, (- 2)], 'TTTTCCGGAG'))\n    assert (rgen.generate_read(10, 10, 3, 6, nodes) == (9, '2I3=2D5=', [3, (- 2)], 'TTTCCGGAGG'))\n    assert (rgen.generate_read(11, 10, 3, 6, nodes) == (9, '1I3=2D6=', [3, (- 2)], 'TTCCGGAGGC'))\n    assert (rgen.generate_read(12, 10, 4, 6, nodes) == (9, '3=2D7=', [(- 2)], 'TCCGGAGGCG'))\n    assert (rgen.generate_read(13, 10, 4, 8, nodes) == (10, '2=2D7=4D1=', [(- 2), (- 4)], 'CCGGAGGCGC'))\n    assert (rgen.generate_read(14, 9, 4, 8, nodes) == (11, '1=2D7=4D1=', [(- 2), (- 4)], 'CGGAGGCGC'))\n    assert (rgen.generate_read(15, 8, 6, 8, nodes) == (14, '7=4D1=', [(- 4)], 'GGAGGCGC'))",
    "nl": "Read gen: Read pos, cigar, v_list and seq (cpy 1)",
    "original_nl": "Read gen: Read pos, cigar, v_list and seq (cpy 1)"
  },
  {
    "code": "def test_reproject_celestial_3d():\n    header_in = fits.Header.fromtextfile(get_pkg_data_filename('../../tests/data/cube.hdr'))\n    array_in = np.ones((3, 200, 180))\n    wcs_in = WCS(header_in)\n    wcs_out = wcs_in.deepcopy()\n    wcs_out.wcs.ctype = ['GLON-SIN', 'GLAT-SIN', wcs_in.wcs.ctype[2]]\n    wcs_out.wcs.crval = [158.0501, (- 21.530282), wcs_in.wcs.crval[2]]\n    wcs_out.wcs.crpix = [50.0, 50.0, (wcs_in.wcs.crpix[2] + 0.4)]\n    (out_full, foot_full) = _reproject_full(array_in, wcs_in, wcs_out, (3, 160, 170))\n    (out_celestial, foot_celestial) = _reproject_celestial(array_in, wcs_in, wcs_out, (3, 160, 170))\n    np.testing.assert_allclose(out_full, out_celestial)\n    np.testing.assert_allclose(foot_full, foot_celestial)",
    "nl": "Test both full_reproject and slicewise reprojection. We use a case where the\nnon-celestial slices are the same and therefore where both algorithms can\nwork.",
    "original_nl": "Test both full_reproject and slicewise reprojection. We use a case where the\nnon-celestial slices are the same and therefore where both algorithms can\nwork."
  },
  {
    "code": "@dev.command()\n@click.pass_context\n@click.argument('ticket', default='')\ndef ticket(ctx, ticket):\n    issue_id = (make_issue_descriptor(ctx.obj.repo.active_branch.name).id if (not ticket) else ticket)\n    issue = verify_ticket_exists(ctx.obj.issue_tracker(), issue_id)\n    url = issue.browse_url\n    click.echo('Opening \"{}\"'.format(url))\n    webbrowser.open_new(url)",
    "nl": "Open the ticket for the current feature or the one supplied in the ticket argument.",
    "original_nl": "Open the ticket for the current feature or the one supplied in the ticket argument."
  },
  {
    "code": "def assert_is_bbox_dataset(dataset, n_fg_class, n_example=None):\n    assert (len(dataset) > 0), 'The length of dataset must be greater than zero.'\n    if n_example:\n        for _ in six.moves.range(n_example):\n            i = np.random.randint(0, len(dataset))\n            _check_example(dataset[i], n_fg_class)\n    else:\n        for i in six.moves.range(len(dataset)):\n            _check_example(dataset[i], n_fg_class)",
    "nl": "Checks if a dataset satisfies the bounding box dataset API.\n\nThis function checks if a given dataset satisfies the bounding box dataset\nAPI or not.\nIf the dataset does not satifiy the API, this function raises an\n:class:`AssertionError`.\n\nArgs:\n    dataset: A dataset to be checked.\n    n_fg_class (int): The number of foreground classes.\n    n_example (int): The number of examples to be checked.\n        If this argument is specified, this function picks\n        examples ramdomly and checks them. Otherwise,\n        this function checks all examples.",
    "original_nl": "Checks if a dataset satisfies the bounding box dataset API.\n\nThis function checks if a given dataset satisfies the bounding box dataset\nAPI or not.\nIf the dataset does not satifiy the API, this function raises an\n:class:`AssertionError`.\n\nArgs:\n    dataset: A dataset to be checked.\n    n_fg_class (int): The number of foreground classes.\n    n_example (int): The number of examples to be checked.\n        If this argument is specified, this function picks\n        examples ramdomly and checks them. Otherwise,\n        this function checks all examples."
  },
  {
    "code": "def addFromUpperLowerFile(self, fileName):\n    fileText = settings.getFileInAlterationsOrGivenDirectory(os.path.dirname(__file__), fileName)\n    fileLines = archive.getTextLines(fileText)\n    self.distanceFeedRate.addLinesSetAbsoluteDistanceMode(fileLines)",
    "nl": "Add lines of text from the fileName or the lowercase fileName, if there is no file by the original fileName in the directory.",
    "original_nl": "Add lines of text from the fileName or the lowercase fileName, if there is no file by the original fileName in the directory."
  },
  {
    "code": "def main():\n    options = _parse_args()\n    archive = download_setuptools(version=options.version, download_base=options.download_base, downloader_factory=options.downloader_factory)\n    return _install(archive, _build_install_args(options))",
    "nl": "Install or upgrade setuptools and EasyInstall.",
    "original_nl": "Install or upgrade setuptools and EasyInstall."
  },
  {
    "code": "def test_106_common_count(self):\n    self.assertEqual(self.p.common_count, 3)",
    "nl": "SELinuxPolicy: common permisison set count",
    "original_nl": "SELinuxPolicy: common permisison set count"
  },
  {
    "code": "def _check_active(self):\n    time.sleep(5)\n    return True",
    "nl": "Dummy service activity check.",
    "original_nl": "Dummy service activity check."
  },
  {
    "code": "def get_num_of_req(self):\n    if (self.num_of_req > 0):\n        return self.num_of_req\n    self.num_of_req = 0\n    if self.c_reader:\n        self.num_of_req = c_cacheReader.get_num_of_req(self.c_reader)\n    else:\n        while (self.read_one_req() is not None):\n            self.num_of_req += 1\n    self.reset()\n    return self.num_of_req",
    "nl": "count the number of requests in the trace, fast for binary type trace,\nfor plain/csv type trace, this is slow\n",
    "original_nl": "count the number of requests in the trace, fast for binary type trace,\nfor plain/csv type trace, this is slow\n:return: the number of requests in the trace"
  },
  {
    "code": "def test_correlation_decomp(self):\n    acorr_amp = (np.dot((self.mu * self.obs1), self.R) * np.dot(self.L, self.obs1))\n    acorr = np.dot(self.ev_t, acorr_amp)\n    acorrn = correlation_decomp(self.T, self.obs1, k=self.k, times=self.times)\n    assert_allclose(acorrn, acorr)\n    'Cross-correlation'\n    'k=None'\n    corr_amp = (np.dot((self.mu * self.obs1), self.R) * np.dot(self.L, self.obs2))\n    corr = np.dot(self.ev_t, corr_amp)\n    corrn = correlation_decomp(self.T, self.obs1, obs2=self.obs2, k=self.k, times=self.times)\n    assert_allclose(corrn, corr)",
    "nl": "Auto-correlation",
    "original_nl": "Auto-correlation"
  },
  {
    "code": "def add_getwork(server, username, password):\n    __patch()\n    key = get_key(server, username, password)\n    if (key not in getworks):\n        getworks[key] = 0\n    getworks[key] += 1\n    username = shorten(username)\n    logging.info(('Getwork: %s:%s@%s' % (username, password, server)))",
    "nl": "Adds a getwork to the database",
    "original_nl": "Adds a getwork to the database"
  },
  {
    "code": "def remove(self, key):\n    self.__delitem__(key)",
    "nl": "Remove object being referenced to by ident from collection\n\n",
    "original_nl": "Remove object being referenced to by ident from collection\n\n:param key: ID or serial\n:type key: str"
  },
  {
    "code": "def normalised(self, max_time_back_seconds=None, resolution=60, status_type=None):\n    if (max_time_back_seconds is None):\n        start_time = self.EPOCH_TIME\n    else:\n        start_time = (self.START_TIME - max_time_back_seconds)\n    for tick in range(start_time, self.START_TIME, resolution):\n        status_obj = self.get_status(tick)\n        if (status_type is None):\n            (yield status_obj.highest_active_status_type())\n        else:\n            (yield status_obj._status[status_type])",
    "nl": "Turns a sparse time series into a dense one, with number of seconds per bucket specified by resolution.\nIf a status_type (status, webStatus, messengerStatus etc.) is given, returns a generator of the status level (online, offline, idle) for that status type.",
    "original_nl": "Turns a sparse time series into a dense one, with number of seconds per bucket specified by resolution.\nIf a status_type (status, webStatus, messengerStatus etc.) is given, returns a generator of the status level (online, offline, idle) for that status type."
  },
  {
    "code": "@mock.patch('listenbrainz.db.user.get')\ndef test_menu_logged_in_error_show(self, mock_user_get):\n\n    @self.app.route('/page_that_returns_400')\n    def view400():\n        raise BadRequest('bad request')\n\n    @self.app.route('/page_that_returns_404')\n    def view404():\n        raise NotFound('not found')\n    user = db_user.get_or_create('iliekcomputers')\n    mock_user_get.return_value = user\n    self.temporary_login(user['id'])\n    resp = self.client.get('/page_that_returns_400')\n    data = resp.data.decode('utf-8')\n    self.assert400(resp)\n    self.assertIn('iliekcomputers', data)\n    self.assertIn('Import!', data)\n    self.assertIn('Your Listens', data)\n    mock_user_get.assert_called_with(user['id'])\n    resp = self.client.get('/page_that_returns_404')\n    data = resp.data.decode('utf-8')\n    self.assert404(resp)\n    self.assertIn('iliekcomputers', data)\n    self.assertIn('Import!', data)\n    self.assertIn('Your Listens', data)\n    mock_user_get.assert_called_with(user['id'])",
    "nl": "If the user is logged in, if we show a 400 or 404 error, show the user menu",
    "original_nl": "If the user is logged in, if we show a 400 or 404 error, show the user menu"
  },
  {
    "code": "@cached_func\ndef get_replacement_mapping():\n    mapping = {\n        \n    }\n    for (cp, repl) in iteritems(generate_re_mapping(diacritic_for_letters(regenerate=False))):\n        mapping.setdefault(cp, []).extend(repl)\n    for (cp, repl) in iteritems(get_decomps_mapping(regenerate=False)):\n        mapping.setdefault(cp, []).extend(repl)\n    for (cp, repl) in iteritems(get_punctuation_mapping(regenerate=False)):\n        mapping.setdefault(cp, []).extend(repl)\n    return mapping",
    "nl": "Returns a dict mapping a sequence of characters to another sequence\nof characters.\n\nIf a key occurs in a text, it should also match any of the characters in\nin the value.",
    "original_nl": "Returns a dict mapping a sequence of characters to another sequence\nof characters.\n\nIf a key occurs in a text, it should also match any of the characters in\nin the value."
  },
  {
    "code": "def tag_resources(ResourceARNList=None, Tags=None):\n    pass",
    "nl": "Applies one or more tags to the specified resources. Note the following:\nSee also: AWS API Documentation\n\n\n:example: response = client.tag_resources(\n    ResourceARNList=[\n        'string',\n    ],\n    Tags={\n        'string': 'string'\n    }\n)\n\n\n:type ResourceARNList: list\n",
    "original_nl": "Applies one or more tags to the specified resources. Note the following:\nSee also: AWS API Documentation\n\n\n:example: response = client.tag_resources(\n    ResourceARNList=[\n        'string',\n    ],\n    Tags={\n        'string': 'string'\n    }\n)\n\n\n:type ResourceARNList: list\n:param ResourceARNList: [REQUIRED]\n        A list of ARNs. An ARN (Amazon Resource Name) uniquely identifies a resource. You can specify a minimum of 1 and a maximum of 20 ARNs (resources) to tag. An ARN can be set to a maximum of 1600 characters. For more information, see Amazon Resource Names (ARNs) and AWS Service Namespaces in the AWS General Reference .\n        (string) --\n        \n\n:type Tags: dict\n:param Tags: [REQUIRED]\n        The tags that you want to add to the specified resources. A tag consists of a key and a value that you define.\n        (string) --\n        (string) --\n        \n\n:rtype: dict\n:return: {\n    'FailedResourcesMap': {\n        'string': {\n            'StatusCode': 123,\n            'ErrorCode': 'InternalServiceException'|'InvalidParameterException',\n            'ErrorMessage': 'string'\n        }\n    }\n}\n\n\n:returns: \nResourceARNList (list) -- [REQUIRED]\nA list of ARNs. An ARN (Amazon Resource Name) uniquely identifies a resource. You can specify a minimum of 1 and a maximum of 20 ARNs (resources) to tag. An ARN can be set to a maximum of 1600 characters. For more information, see Amazon Resource Names (ARNs) and AWS Service Namespaces in the AWS General Reference .\n\n(string) --\n\n\nTags (dict) -- [REQUIRED]\nThe tags that you want to add to the specified resources. A tag consists of a key and a value that you define.\n\n(string) --\n(string) --"
  },
  {
    "code": "def __init__(self, url, username, password, user_agent):\n    self.url = url\n    self.username = username\n    self.password = password\n    self.user_agent = user_agent",
    "nl": "This factory detects the remote Wordpress two-factor plugin type\nand returns the appropriate class",
    "original_nl": "This factory detects the remote Wordpress two-factor plugin type\nand returns the appropriate class"
  },
  {
    "code": "@property\ndef capabilities(self):\n    return [self.Capability(item) for item in self._editor.get_element_value(self._capabilities_element).split(',') if (item != 'Uploads')]",
    "nl": "Gets a list of the enabled operations (by type name) that are currently enabled for the feature service.",
    "original_nl": "Gets a list of the enabled operations (by type name) that are currently enabled for the feature service."
  },
  {
    "code": "def build_tables(target, source, env):\n    builder_attributes = {\n        'name': 'Tablefill',\n        'valid_extensions': ['.lyx', '.tex'],\n        'exec_opts': '-interaction nonstopmode -jobname',\n    }\n    builder = TableBuilder(target, source, env, **builder_attributes)\n    builder.execute_system_call()\n    return None",
    "nl": "Build a SCons target by filling a table\n\nThis function uses the tablefill function from gslab_fill to produced a \nfilled table from (i) an empty table in a LyX/Tex file and (ii) text files \ncontaining data to be used in filling the table. \n",
    "original_nl": "Build a SCons target by filling a table\n\nThis function uses the tablefill function from gslab_fill to produced a \nfilled table from (i) an empty table in a LyX/Tex file and (ii) text files \ncontaining data to be used in filling the table. \n\nParameters\n----------\ntarget: string or list \n    The target(s) of the SCons command.\nsource: string or list\n    The source(s) of the SCons command. The first source specified\n    should be the LyX/Tex file specifying the table format. The subsequent \n    sources should be the text files containing the data with which the\n    tables are to be filled. \nenv: SCons construction environment, see SCons user guide 7.2"
  },
  {
    "code": "def batch_size_from_nested_tensors(tensors):\n    for tensor in snt.nest.flatten(tensors):\n        if (tensor.get_shape().ndims > 0):\n            return tf.shape(tensor)[0]\n    return None",
    "nl": "Returns the batch dimension from the first non-scalar tensor given.",
    "original_nl": "Returns the batch dimension from the first non-scalar tensor given."
  },
  {
    "code": "def connection_lost(self, exc):\n    self._transport = None\n    super().connection_lost(exc)",
    "nl": "Called when the connection is lost or closed.",
    "original_nl": "Called when the connection is lost or closed."
  },
  {
    "code": "def __init__(self, df, category_col, parsed_col, feats_from_spacy_doc=FeatsFromSpacyDoc()):\n    self._df = df.reset_index()\n    self._category_col = category_col\n    self._parsed_col = parsed_col\n    self._category_idx_store = IndexStore()\n    self._X_factory = CSRMatrixFactory()\n    self._mX_factory = CSRMatrixFactory()\n    self._term_idx_store = IndexStore()\n    self._metadata_idx_store = IndexStore()\n    self._feats_from_spacy_doc = feats_from_spacy_doc",
    "nl": "Parameters\n----------\ndf : pd.DataFrame\n contains category_col, and parse_col, were parsed col is entirely spacy docs\ncategory_col : str\n        name of category column in df\nparsed_col : str\n        name of spacy parsed column in df\nfeats_from_spacy_doc : FeatsFromSpacyDoc",
    "original_nl": "Parameters\n----------\ndf : pd.DataFrame\n contains category_col, and parse_col, were parsed col is entirely spacy docs\ncategory_col : str\n        name of category column in df\nparsed_col : str\n        name of spacy parsed column in df\nfeats_from_spacy_doc : FeatsFromSpacyDoc"
  },
  {
    "code": "def get_bash_file(self, homefolder):\n    rc_file = os.path.abspath(('%s/.bashrc' % homefolder))\n    if os.path.exists(os.path.abspath(('%s/.bash_aliases' % homefolder))):\n        rc_file = os.path.abspath(('%s/.bash_aliases' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.bashrc' % homefolder))):\n        rc_file = os.path.abspath(('%s/.bashrc' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.bash_profile' % homefolder))):\n        rc_file = os.path.abspath(('%s/.bash_profile' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.profile' % homefolder))):\n        rc_file = os.path.abspath(('%s/.profile' % homefolder))\n    elif os.path.exists(os.path.abspath(('%s/.login' % homefolder))):\n        rc_file = os.path.abspath(('%s/.login' % homefolder))\n    return rc_file",
    "nl": "Return the path to the user's bash rc file.",
    "original_nl": "Return the path to the user's bash rc file."
  },
  {
    "code": "def parse_out_axes(codes):\n    axesCodes = 'XYZAB'\n    parsedAxes = (set(axesCodes) & set(codes))\n    return list(sorted(parsedAxes))",
    "nl": "Given a list of codes, returns a list of all present axes\n\n@param list codes: Codes parsed out of the gcode command\n@return list: List of axes in codes",
    "original_nl": "Given a list of codes, returns a list of all present axes\n\n@param list codes: Codes parsed out of the gcode command\n@return list: List of axes in codes"
  },
  {
    "code": "def load(read_file, formatting):\n    if (formatting == 'json'):\n        return json.load(read_file)\n    if (formatting == 'pkl'):\n        return pickle.load(read_file)\n    raise ValueError('Unsupported format: {}'.format(formatting))",
    "nl": "Return a writable from read_file. Support json, and pkl formats.",
    "original_nl": "Return a writable from read_file. Support json, and pkl formats."
  },
  {
    "code": "def validate_complaint_document(self, operation):\n    if ((operation == 'update') and (self.request.authenticated_role != self.context.author)):\n        self.request.errors.add('url', 'role', 'Can update document only author')\n        self.request.errors.status = 403\n        raise error_handler(self.request.errors)\n    if (self.request.validated['tender_status'] not in ['active.pre-qualification', 'active.pre-qualification.stand-still']):\n        raise_operation_error(self.request, \"Can't {} document in current ({}) tender status\".format(operation, self.request.validated['tender_status']))\n    if any([(i.status != 'active') for i in self.request.validated['tender'].lots if (i.id == self.request.validated['qualification'].lotID)]):\n        raise_operation_error(self.request, 'Can {} document only in active lot status'.format(operation))\n    if (self.request.validated['complaint'].status not in STATUS4ROLE.get(self.request.authenticated_role, [])):\n        raise_operation_error(self.request, \"Can't {} document in current ({}) complaint status\".format(operation, self.request.validated['complaint'].status))\n    return True",
    "nl": "TODO move validators\nThis class is inherited from openua package, but validate_complaint_document function has different validators.\nFor now, we have no way to use different validators on methods according to procedure type.",
    "original_nl": "TODO move validators\nThis class is inherited from openua package, but validate_complaint_document function has different validators.\nFor now, we have no way to use different validators on methods according to procedure type."
  },
  {
    "code": "def test_kml(self):\n    self._load_city_data()\n    h = City3D.objects.kml(precision=6).get(name='Houston')\n    ref_kml_regex = re.compile('^<Point><coordinates>-95.363\\\\d+,29.763\\\\d+,18</coordinates></Point>$')\n    self.assertTrue(ref_kml_regex.match(h.kml))",
    "nl": "Test GeoQuerySet.kml() with Z values.",
    "original_nl": "Test GeoQuerySet.kml() with Z values."
  },
  {
    "code": "def test_overflowBytesSentToWrappedProtocol(self):\n    factory = HAProxyWrappingFactory(Factory.forProtocol(StaticProtocol))\n    proto = factory.buildProtocol(address.IPv6Address('TCP', b'::1', 8080))\n    transport = StringTransportWithDisconnection()\n    proto.makeConnection(transport)\n    proto.dataReceived((self.IPV6HEADER + b'HTTP/1.1 / GET'))\n    self.assertEqual(proto.wrappedProtocol.data, b'HTTP/1.1 / GET')",
    "nl": "Test if non-header bytes are passed to the wrapped protocol.",
    "original_nl": "Test if non-header bytes are passed to the wrapped protocol."
  },
  {
    "code": "def __init__(self, matcher):\n    self.matcher = matcher()",
    "nl": "Constructor for Analyzer\n",
    "original_nl": "Constructor for Analyzer\n:param matcher: A class which is derived from StringMatchingBase"
  },
  {
    "code": "def test_limit(self):\n    N = 10\n    var = list(Pagination(limit=N).limit(self.variants_django))\n    self.assertEqual(len(var), N)\n    var = list(Pagination(limit=N).limit(self.variants))\n    self.assertEqual(len(var), N)",
    "nl": "Limiting to N results should return N elements.",
    "original_nl": "Limiting to N results should return N elements."
  },
  {
    "code": "def _trace_summary(self):\n    for (i, (val, args)) in enumerate(self.trace):\n        if (args is StopIteration):\n            info = 'Terminated'\n        else:\n            pprint = ','.join(((('{' + ','.join((('%s=%r' % (k, v)) for (k, v) in arg.items()))) + '}') for arg in args))\n            info = ('exploring arguments [%s]' % pprint)\n        if (i == 0):\n            print(('Step %d: Initially %s.' % (i, info)))\n        else:\n            print(('Step %d: %s after receiving input(s) %s.' % (i, info.capitalize(), val)))",
    "nl": "Summarizes the trace of values used to update the DynamicArgs\nand the arguments subsequently returned. May be used to\nimplement the summary method.",
    "original_nl": "Summarizes the trace of values used to update the DynamicArgs\nand the arguments subsequently returned. May be used to\nimplement the summary method."
  },
  {
    "code": "def on_request(self, *args):\n    return EndpointOutput(content=self.server.endpoint_sources)",
    "nl": "RPC task to get all available celery endpoints.\n",
    "original_nl": "RPC task to get all available celery endpoints.\n\nReturns:\n    ReturnMessage: A return message where the content is a list of\n    dictionaries containing information about the available endpoints."
  },
  {
    "code": "def test_serie_precedence_over_global_config():\n    chart = Line(stroke=False)\n    chart.add('1', s1, stroke=True)\n    chart.add('2', s2)\n    q = chart.render_pyquery()\n    assert (len(q('.serie-0 .line')) == 1)\n    assert (len(q('.serie-1 .line')) == 0)\n    assert (len(q('.serie-0 .dot')) == 5)\n    assert (len(q('.serie-1 .dot')) == 6)",
    "nl": "Test that per serie configuration overide global configuration",
    "original_nl": "Test that per serie configuration overide global configuration"
  },
  {
    "code": "def get_queue_limit(self, queue_name):\n    for queue_def in self._config['queue_definitions']:\n        if (queue_def['name'] == queue_name):\n            return int(queue_def['walltime_limit'])\n    return None",
    "nl": "get the maximum walltime for the queue specified",
    "original_nl": "get the maximum walltime for the queue specified"
  },
  {
    "code": "def extend_volume(self, volume, new_size):\n    return self._unify_volume(self._impl.extend_volume(volume, new_size=new_size))",
    "nl": "Extend the size of the specified volume.",
    "original_nl": "Extend the size of the specified volume."
  },
  {
    "code": "@property\ndef inverse(self):\n    return AsinhStretch(a=(1.0 / np.sinh((1.0 / self.a))))",
    "nl": "A stretch object that performs the inverse operation.",
    "original_nl": "A stretch object that performs the inverse operation."
  },
  {
    "code": "def action_visualize(args, fromshell=False, path=None, title='', theme='', verbose=False):\n    if (not args):\n        ontouri = ontospy_actions.action_listlocal(all_details=False)\n        if ontouri:\n            islocal = True\n        else:\n            raise SystemExit(1)\n    elif fromshell:\n        ontouri = args\n        islocal = True\n    else:\n        ontouri = args[0]\n        islocal = False\n    viztype = ask_visualization()\n    if (viztype == ''):\n        return None\n    USE_CACHE = False\n    if (islocal and USE_CACHE):\n        g = get_pickled_ontology(ontouri)\n        if (not g):\n            g = do_pickle_ontology(ontouri)\n    else:\n        printDebug('Loading graph...', dim=True)\n        if islocal:\n            g = Ontospy(os.path.join(ontospy_manager.get_home_location(), ontouri), verbose=verbose)\n        else:\n            g = Ontospy(ontouri, verbose=verbose)\n    if (not path):\n        from os.path import expanduser\n        home = expanduser('~')\n        onto_path = slugify(unicode(ontouri))\n        viz_path = slugify(unicode(VISUALIZATIONS_LIST[viztype]['Title']))\n        path = os.path.join(home, ((('ontospy-viz/' + onto_path) + '/') + viz_path))\n        if (not os.path.exists(path)):\n            os.makedirs(path)\n    printDebug('Building visualization...', dim=True)\n    url = build_visualization(ontouri, g, viztype, path, title, theme)\n    return url",
    "nl": "export model into another format eg html, d3 etc...\n<fromshell> : the local name is being passed from ontospy shell",
    "original_nl": "export model into another format eg html, d3 etc...\n<fromshell> : the local name is being passed from ontospy shell"
  },
  {
    "code": "def __init__(self, indent=None, fmt=None, datefmt=None, epoch_time=False, **kwargs):\n    if ((fmt is not None) or (datefmt is not None)):\n        msg = \"JsonFormatter doesn't allow to set 'format' and 'datefmt'!\"\n        raise ValueError(msg)\n    super(SimpleJsonFormatter, self).__init__(fmt='%(uber_json)s', **kwargs)\n    self.indent = (indent or 0)\n    self.epoch = epoch_time",
    "nl": "Initialize the handler.\nIf stream is not specified, sys.stderr is used.",
    "original_nl": "Initialize the handler.\nIf stream is not specified, sys.stderr is used."
  },
  {
    "code": "def capitalize_first_letter(string):\n    if string:\n        string = (string[0].upper() + string[1:])\n    return string",
    "nl": "Same as capitalize() but leaves the other letter untouched.\n\nE.g.: 'abCd Efg' -> 'AbDd Efg'",
    "original_nl": "Same as capitalize() but leaves the other letter untouched.\n\nE.g.: 'abCd Efg' -> 'AbDd Efg'"
  },
  {
    "code": "def get_cons_enc(cons):\n    res = list()\n    for c in list(set(cons)):\n        (name, form) = c.strip(')').split(' ')[1:]\n        if (form == 'Bool'):\n            res.append(z3.Bool(name))\n        elif (form == 'Real'):\n            res.append(z3.Real(name))\n    return res",
    "nl": "Returns z3 instance for each string that declares a constant.",
    "original_nl": "Returns z3 instance for each string that declares a constant."
  },
  {
    "code": "def chr(_: int) -> str:\n    pass",
    "nl": "Return a Unicode string of one character with ordinal i; 0 <= i <= 0x10ffff.",
    "original_nl": "Return a Unicode string of one character with ordinal i; 0 <= i <= 0x10ffff."
  },
  {
    "code": "@site_name.setter\ndef site_name(self, site_name):\n    if (site_name in SITE_LIST):\n        self.__site_name = site_name\n        self.__site_url = SITE_LIST[site_name]['url']\n        if ('api_version' and ('hashed_string' in SITE_LIST[site_name])):\n            self.api_version = SITE_LIST[site_name]['api_version']\n            self.hash_string = SITE_LIST[site_name]['hashed_string']\n    else:\n        raise PybooruError(\"The 'site_name' is not valid, specify a valid 'site_name'.\")",
    "nl": "Function that sets and checks the site name and set url.\n",
    "original_nl": "Function that sets and checks the site name and set url.\n\nParameters:\n    site_name (str): The site name in 'SITE_LIST', default sites.\n\nRaises:\n    PybooruError: When 'site_name' isn't valid."
  },
  {
    "code": "def __init__(self, constructed_object):\n    self.constructed_object = constructed_object\n    self.build_methods = dict()",
    "nl": "Initialize a new Builder.\n\nConcrete Builders should call this method from within their own __init__ method.\nThe concrete __init__ method should also register all build options to build methods,\nby using the _register method.\n\n@param constructed_object: An instance of an object this builder is responsible for.",
    "original_nl": "Initialize a new Builder.\n\nConcrete Builders should call this method from within their own __init__ method.\nThe concrete __init__ method should also register all build options to build methods,\nby using the _register method.\n\n@param constructed_object: An instance of an object this builder is responsible for."
  },
  {
    "code": "def __getitem__(self, index):\n    assert isinstance(index, int)\n    return self.__data[index]",
    "nl": "@rtype: float",
    "original_nl": "@rtype: float"
  },
  {
    "code": "def find_cluster(fa_list, thr=0.5):\n    fa0 = fa_list[0]\n    fa0_group = [fa0]\n    fa_other = fa_list[1:]\n    for fa_o in fa_list[1:]:\n        tm_d = jchem.calc_tm_dist_int(fa0, fa_o)\n        if (tm_d > thr):\n            fa0_group.append(fa_o)\n            fa_other.remove(fa_o)\n    return (fa0_group, fa_other)",
    "nl": "find similar pattern with \nthe first element: fa0",
    "original_nl": "find similar pattern with \nthe first element: fa0"
  },
  {
    "code": "def read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()",
    "nl": "Return file content.",
    "original_nl": "Return file content."
  },
  {
    "code": "@mock.patch('know_me.profile.models.ProfileItem.has_object_read_permission')\ndef test_has_object_read_permission(mock_parent_permission, api_rf, list_entry_factory):\n    entry = list_entry_factory()\n    request = api_rf.get('/')\n    expected = mock_parent_permission.return_value\n    assert (entry.has_object_read_permission(request) == expected)\n    assert (mock_parent_permission.call_count == 1)\n    assert (mock_parent_permission.call_args[0] == (request,))",
    "nl": "List entries should delegate the read permission check to their\nparent profile item.",
    "original_nl": "List entries should delegate the read permission check to their\nparent profile item."
  },
  {
    "code": "def verification(url=None, path=None, lang='chi_sim', clean=False, engine='pytesseract'):\n    if (url is not None):\n        image = read_url_img(url, decode=True)\n    if (path is not None):\n        if os.path.exists(path):\n            try:\n                image = Image.open(path)\n            except:\n                traceback.print_exc()\n                return ''\n        else:\n            return ''\n    if clean:\n        img = np.array(image.convert('L'))\n        meanImg = img.mean()\n        img[(img > meanImg)] = 255\n        img[(img <= meanImg)] = 0\n        image = Image.fromarray(img)\n    if (engine == 'pytesseract'):\n        return tesseract(image, lang)\n    else:\n        return crnn(image)",
    "nl": "tesseract ocr chinses\\english verification\n@@param:url ,if url is not None,it will get image from url with function read_url_img\n@@oarm:path,if path is not None,it with get image from file\n@@param:lang,language choose in ['chi_sim','eng','chi_sim']\n@@param:clean,whether simple clean ths image\n@@ engine:pytesseract,crnn engine\n@@return :uft-8 string",
    "original_nl": "tesseract ocr chinses\\english verification\n@@param:url ,if url is not None,it will get image from url with function read_url_img\n@@oarm:path,if path is not None,it with get image from file\n@@param:lang,language choose in ['chi_sim','eng','chi_sim']\n@@param:clean,whether simple clean ths image\n@@ engine:pytesseract,crnn engine\n@@return :uft-8 string"
  },
  {
    "code": "def median1d(self, param, return_errors=False):\n    v = [self.mergers[m].median1d(param, return_errors=return_errors) for m in self.mergers]\n    if return_errors:\n        (value, merror, perror) = zip(*v)\n        return (numpy.array(value), numpy.array(merror), numpy.array(perror))\n    else:\n        return numpy.array(v)",
    "nl": "Return median 1d marginalized parameters\n",
    "original_nl": "Return median 1d marginalized parameters\n\nParameters\n----------\nname: str\n    The name of the parameter requested\nreturn_errors: Optional, {bool, False}\n    If true, return a second and third parameter that represents the\n    lower and upper 90% error on the parameter.\n\nReturns\n-------\nparam: nump.ndarray or tuple\n    The requested parameter"
  },
  {
    "code": "def __init__(self, node=None):\n    self.type = 'NAF/KAF'\n    if (node is None):\n        self.node = etree.Element('factVal')\n    else:\n        self.node = node",
    "nl": "Constructor of the object\n@type node: xml Element or None (to create and empty one)\n@param node:  this is the node of the element. If it is None it will create a new object",
    "original_nl": "Constructor of the object\n@type node: xml Element or None (to create and empty one)\n@param node:  this is the node of the element. If it is None it will create a new object"
  },
  {
    "code": "def test_031_role_match_regex(self):\n    q = ConstraintQuery(self.p, role='test31r.', role_regex=True)\n    constraint = sorted((c.tclass for c in q.results()))\n    self.assertListEqual(['test31a', 'test31b'], constraint)",
    "nl": "Constraint query with regex role match.",
    "original_nl": "Constraint query with regex role match."
  },
  {
    "code": "def generate_batch_pvdm(doc_ids, word_ids, batch_size, window_size):\n    data_index = 0\n    assert ((batch_size % window_size) == 0)\n    batch = np.ndarray(shape=(batch_size, (window_size + 1)), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = (window_size + 1)\n    buffer = collections.deque(maxlen=span)\n    buffer_doc = collections.deque(maxlen=span)\n    mask = ([1] * span)\n    mask[(- 1)] = 0\n    i = 0\n    doc_id = 0\n    while (data_index < len(word_ids)):\n        if ((len(set(buffer_doc)) == 1) and (len(buffer_doc) == span)):\n            doc_id = buffer_doc[(- 1)]\n            batch[i, :] = (list(compress(buffer, mask)) + [doc_id])\n            labels[(i, 0)] = buffer[(- 1)]\n            i += 1\n        buffer.append(word_ids[data_index])\n        buffer_doc.append(doc_ids[data_index])\n        data_index = (data_index + 1)\n        if (i == batch_size):\n            (yield (batch, labels, doc_id))\n            batch = np.ndarray(shape=(batch_size, (window_size + 1)), dtype=np.int32)\n            labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n            i = 0",
    "nl": "batch generator for PV-DM (Distribbuted Memory Model of Paragraph Vectors)\n",
    "original_nl": "batch generator for PV-DM (Distribbuted Memory Model of Paragraph Vectors)\n:param doc_ids: list of document indices\n:param word_ids: list of word indices\n:param batch_size: number of words in each mini-batch\n:param window_size: number of words before the target word\n:return: tuple of (batch, labels)"
  },
  {
    "code": "def get_room_id(self):\n    return self.lesson_details[self.lesson_id][2]",
    "nl": "Return the ID of the room Eric's will be expected to show up in at\nsome point during the current period, or else the name of the period\n(such as PLAYTIME).",
    "original_nl": "Return the ID of the room Eric's will be expected to show up in at\nsome point during the current period, or else the name of the period\n(such as PLAYTIME)."
  },
  {
    "code": "def convertColumns(self, columns):\n    new = []\n    for desc in columns:\n        if (not isinstance(desc, dict)):\n            desc = {\n                'column': desc,\n            }\n        if ('expression' in desc):\n            assert ('column' not in desc), ('You cannot provide both an expression and a column (for %s in index %s in %s)' % (desc, self.name, self.soClass))\n            assert ('length' not in desc), ('length does not apply to expressions (for %s in index %s in %s)' % (desc, self.name, self.soClass))\n            new.append(desc)\n            continue\n        columnName = desc['column']\n        if (not isinstance(columnName, str)):\n            columnName = columnName.name\n        colDict = self.soClass.sqlmeta.columns\n        if (columnName not in colDict):\n            for possible in colDict.values():\n                if (possible.origName == columnName):\n                    column = possible\n                    break\n            else:\n                raise ValueError(('The column by the name %r was not found in the class %r' % (columnName, self.soClass)))\n        else:\n            column = colDict[columnName]\n        desc['column'] = column\n        new.append(desc)\n    return new",
    "nl": "Converts all the columns to dictionary descriptors;\ndereferences string column names.",
    "original_nl": "Converts all the columns to dictionary descriptors;\ndereferences string column names."
  },
  {
    "code": "def Kill(self):\n    VimCommunicator.Kill(self)\n    if os.path.exists(self.args.servername):\n        os.remove(self.args.servername)",
    "nl": "Kills the Neovim process and removes the socket",
    "original_nl": "Kills the Neovim process and removes the socket"
  },
  {
    "code": "def render_widget(self, name, context, options):\n    dbsys = DashboardSystem(self.env)\n    params = ('layout', 'schema', 'show_captions', 'title')\n    (layout, schema, show_captions, title) = self.bind_params(name, options, *params)\n    lp = dbsys.resolve_layout(layout)\n    dbmod = DashboardModule(self.env)\n    layout_data = lp.expand_layout(layout, context, {\n        'schema': schema,\n        'embed': True,\n    })\n    widgets = dbmod.expand_widget_data(context, schema)\n    return (layout_data['template'], {\n        'title': title,\n        'data': dict(context=context, layout=schema, widgets=widgets, title='', default={\n            'height': (dbmod.default_widget_height or None),\n        }),\n    }, context)",
    "nl": "Count ocurrences of values assigned to given ticket field.",
    "original_nl": "Count ocurrences of values assigned to given ticket field."
  },
  {
    "code": "def test_get_raw_device_name_from_alias():\n    di = no_datastore_interface.NoDatastoreInterface()\n    assert (di.get_raw_device_name_from_alias(api_key, 'Alias') == '')",
    "nl": "This function checks if get_raw_device_name_from_alias returns nothing.",
    "original_nl": "This function checks if get_raw_device_name_from_alias returns nothing."
  },
  {
    "code": "def is_feature_enabled(course):\n    return EdxNotesTab.is_enabled(course)",
    "nl": "Returns True if Student Notes feature is enabled for the course, False otherwise.",
    "original_nl": "Returns True if Student Notes feature is enabled for the course, False otherwise."
  },
  {
    "code": "@property\ndef node_id(self):\n    return self._node_id",
    "nl": "The node ID of the device on the bus.",
    "original_nl": "The node ID of the device on the bus."
  },
  {
    "code": "def pairwise(iterable):\n    (a, b) = it.tee(iterable)\n    next(b, None)\n    return zip(a, b)",
    "nl": "s -> (s0,s1), (s1,s2), (s2, s3), ...",
    "original_nl": "s -> (s0,s1), (s1,s2), (s2, s3), ..."
  },
  {
    "code": "@property\ndef loc_data(self):\n    return (json.loads(self.loc_payload) if self.loc_payload else None)",
    "nl": "The loc_data property is used to specify localization paramaters within the 'alert' aps key.\nhttps://developer.apple.com/library/ios/documentation/NetworkingInternet/Conceptual/RemoteNotificationsPG/Chapters/ApplePushService.html",
    "original_nl": "The loc_data property is used to specify localization paramaters within the 'alert' aps key.\nhttps://developer.apple.com/library/ios/documentation/NetworkingInternet/Conceptual/RemoteNotificationsPG/Chapters/ApplePushService.html"
  },
  {
    "code": "def show_password_dialog(self):\n    dlg = xbmcgui.Dialog()\n    dialog = dlg.input(heading=self.get_local_string(string_id=30004), type=xbmcgui.INPUT_ALPHANUM, option=xbmcgui.ALPHANUM_HIDE_INPUT)\n    return dialog",
    "nl": "Asks the user for its Netflix password\n\n",
    "original_nl": "Asks the user for its Netflix password\n\n:returns: str - Netflix password"
  },
  {
    "code": "def gametime_to_realtime(format=False, **kwargs):\n    rtime = 0\n    for (name, value) in kwargs.items():\n        if ((name not in UNITS) and name.endswith('s')):\n            name = name[:(- 1)]\n        if (name not in UNITS):\n            raise ValueError(\"the unit {} isn't defined as a valid game time unit\".format(name))\n        rtime += (value * UNITS[name])\n    rtime /= TIMEFACTOR\n    if format:\n        return time_to_tuple(rtime, 31536000, 2628000, 604800, 86400, 3600, 60)\n    return rtime",
    "nl": "This method helps to figure out the real-world time it will take until an\nin-game time has passed. E.g. if an event should take place a month later\nin-game, you will be able to find the number of real-world seconds this\ncorresponds to (hint: Interval events deal with real life seconds).\n\nKwargs:\n    format (bool): Formatting the output.\n    days, month etc (int): These are the names of time units that must\n        match the `settings.TIME_UNITS` dict keys.\n",
    "original_nl": "This method helps to figure out the real-world time it will take until an\nin-game time has passed. E.g. if an event should take place a month later\nin-game, you will be able to find the number of real-world seconds this\ncorresponds to (hint: Interval events deal with real life seconds).\n\nKwargs:\n    format (bool): Formatting the output.\n    days, month etc (int): These are the names of time units that must\n        match the `settings.TIME_UNITS` dict keys.\n\nReturns:\n    time (float or tuple): The realtime difference or the same\n        time split up into time units.\n\nExample:\n     gametime_to_realtime(days=2) -> number of seconds in real life from\n                    now after which 2 in-game days will have passed."
  },
  {
    "code": "def test_invalid_source(self):\n    with self.assertRaises(ValueError):\n        util.parse_filename('invalid_package_name.tar.gz')",
    "nl": "Parse fails on invalid package name",
    "original_nl": "Parse fails on invalid package name"
  },
  {
    "code": "def on_typecode_inconsistency(match, state, logger):\n    logger.error(('RS found two different types with the same name: %s' % match[0]))",
    "nl": "It happens when RS detects two different types with same name.",
    "original_nl": "It happens when RS detects two different types with same name."
  },
  {
    "code": "def test_cleanedFailure(self):\n\n    def failing_func():\n        (1 / 0)\n    try:\n        failing_func()\n    except ZeroDivisionError:\n        failure = Failure()\n    failure.cleanFailure()\n    event = dict(log_format='Hi mom', who='me', log_failure=failure)\n    (records, output) = self.logEvent(event)\n    self.assertEqual(len(records), 1)\n    self.assertIn('Hi mom', output)\n    self.assertIn('in failing_func', output)\n    self.assertIn('ZeroDivisionError', output)",
    "nl": "A cleaned Failure object has a fake traceback object; make sure that\nlogging such a failure still results in the exception details being\nlogged.",
    "original_nl": "A cleaned Failure object has a fake traceback object; make sure that\nlogging such a failure still results in the exception details being\nlogged."
  },
  {
    "code": "def get_model():\n    (ch, row, col) = (3, 45, 160)\n    model = Sequential()\n    model.add(Lambda((lambda x: ((x / 255.0) - 0.5)), input_shape=(row, col, ch), output_shape=(row, col, ch)))\n    model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode='same'))\n    model.add(ELU())\n    model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode='same'))\n    model.add(ELU())\n    model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode='same'))\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(ELU())\n    model.add(Dense(512))\n    model.add(Dropout(0.5))\n    model.add(ELU())\n    model.add(Dense(1))\n    return model",
    "nl": "Get the model, this is a slight modification of comma.ai model",
    "original_nl": "Get the model, this is a slight modification of comma.ai model"
  },
  {
    "code": "def getNewRepository():\n    return WidenRepository()",
    "nl": "Get the repository constructor.",
    "original_nl": "Get the repository constructor."
  },
  {
    "code": "def test_tcp4(self):\n    self.onePrefix('haproxy:tcp:8080', TCP4ServerEndpoint)",
    "nl": "Test if the parser generates a wrapped TCP4 endpoint.",
    "original_nl": "Test if the parser generates a wrapped TCP4 endpoint."
  },
  {
    "code": "def update_list(client, list_id, revision, title=None, public=None):\n    if (title is not None):\n        _check_title_length(title, client.api)\n    data = {\n        'revision': revision,\n        'title': title,\n        'public': public,\n    }\n    data = {key: value for (key, value) in data.iteritems() if (value is not None)}\n    endpoint = '/'.join([client.api.Endpoints.LISTS, str(list_id)])\n    response = client.authenticated_request(endpoint, 'PATCH', data=data)\n    return response.json()",
    "nl": "Updates the list with the given ID to have the given properties\n\nSee https://developer.wunderlist.com/documentation/endpoints/list for detailed parameter information",
    "original_nl": "Updates the list with the given ID to have the given properties\n\nSee https://developer.wunderlist.com/documentation/endpoints/list for detailed parameter information"
  },
  {
    "code": "def app_settings(request):\n    settings_dict = dict()\n    settings = dict()\n    for obj in Setting.objects.all():\n        settings[obj.name] = obj.value\n    settings_dict['settings'] = json.dumps(settings)\n    return settings_dict",
    "nl": "Global values to pass to templates",
    "original_nl": "Global values to pass to templates"
  },
  {
    "code": "@task\ndef install():\n    local('pip install -r requirements.txt')",
    "nl": "Install requirements packages",
    "original_nl": "Install requirements packages"
  },
  {
    "code": "def __init__(self, num_input_states, num_memory_states, num_output_states, random_genome_length=10000, seed_num_markov_gates=4, probabilistic=True, genome=None):\n    self.num_input_states = num_input_states\n    self.num_memory_states = num_memory_states\n    self.num_output_states = num_output_states\n    self.states = np.zeros(((num_input_states + num_memory_states) + num_output_states), dtype=np.bool)\n    self.markov_gates = []\n    self.markov_gate_input_ids = []\n    self.markov_gate_output_ids = []\n    if (genome is None):\n        self.genome = np.random.randint(0, 256, random_genome_length).astype(np.uint8)\n        for _ in range(seed_num_markov_gates):\n            start_index = np.random.randint(0, int((len(self.genome) * 0.8)))\n            self.genome[start_index] = 42\n            self.genome[(start_index + 1)] = 213\n    else:\n        self.genome = np.array(genome, dtype=np.uint8)\n    self._setup_markov_network(probabilistic)",
    "nl": "Sets up a Markov Network\n",
    "original_nl": "Sets up a Markov Network\n\nParameters\n----------\nnum_input_states: int\n    The number of input states in the Markov Network\nnum_memory_states: int\n    The number of internal memory states in the Markov Network\nnum_output_states: int\n    The number of output states in the Markov Network\nrandom_genome_length: int (default: 10000)\n    Length of the genome if it is being randomly generated\n    This parameter is ignored if \"genome\" is not None\nseed_num_markov_gates: int (default: 4)\n    The number of Markov Gates with which to seed the Markov Network\n    It is important to ensure that randomly-generated Markov Networks have at least a few Markov Gates to begin with\n    May sometimes result in fewer Markov Gates if the Markov Gates are randomly seeded in the same location\n    This parameter is ignored if \"genome\" is not None\nprobabilistic: bool (default: True)\n    Flag indicating whether the Markov Gates are probabilistic or deterministic\ngenome: array-like (default: None)\n    An array representation of the Markov Network to construct\n    All values in the array must be integers in the range [0, 255]\n    If None, then a random Markov Network will be generated\n\nReturns\n-------\nNone"
  },
  {
    "code": "def finalize_options(self):\n    _build_ext.finalize_options(self)\n    __builtins__.__NUMPY_SETUP__ = False\n    import numpy\n    self.include_dirs.append(numpy.get_include())\n    for extension in self.extensions:\n        extension.include_dirs = self.include_dirs\n    try:\n        from Cython.Build import cythonize\n        _needs_stub = [ext._needs_stub for ext in self.extensions]\n        self.extensions = cythonize(self.extensions)\n        for (ns, ext) in zip(_needs_stub, self.extensions):\n            ext._needs_stub = ns\n    except ImportError:\n        print('Cython not imported')\n        print('If C sources exist in the build directory, they will be used')\n        for ext in self.extensions:\n            ext.sources = [f.replace('.pyx', '.c') for f in ext.sources]\n    for ext in self.extensions:\n        for src in ext.sources:\n            if (not exists(src)):\n                raise Exception('Extension source code not found ({0})\\nCython required for install'.format(src))\n    return",
    "nl": "Unsets __NUMPY_SETUP__ so that the get_include function can be used",
    "original_nl": "Unsets __NUMPY_SETUP__ so that the get_include function can be used"
  },
  {
    "code": "def test_noZipFile(self):\n    with open('./test_data/test.txt', 'wb') as f:\n        f.write('test')\n    testcat.unzipFiles()\n    l = os.listdir('./test_out')\n    self.assertEqual(l, [])",
    "nl": "Test that an empty list is returned when there is no zip file in the input directory.",
    "original_nl": "Test that an empty list is returned when there is no zip file in the input directory."
  },
  {
    "code": "def test_writePID(self):\n    pid = 1995\n    pidFile = PIDFile(DummyFilePath())\n    pidFile._write(pid)\n    self.assertEqual(pidFile.read(), pid)",
    "nl": "L{PIDFile._write} stores the given PID.",
    "original_nl": "L{PIDFile._write} stores the given PID."
  },
  {
    "code": "def check_get_datafile(self):\n    datafile = xml_Utils.getChildTextbyParentTag(self.filepath, 'Details', 'InputDataFile')\n    if ((datafile is None) or (datafile is False) or (str(datafile).strip() == '')):\n        if (self.filetype == 'tc'):\n            datafile = get_default_xml_datafile(self.filepath)\n        if (self.filetype == 'ts'):\n            datatype = self.check_get_datatype(False)\n            if str(datatype).lower().startswith('iterative'):\n                datafile = get_default_xml_datafile(self.filepath)\n            else:\n                datafile = False\n        elif (self.filetype == 'proj'):\n            datafile = False\n    elif (str(datafile).strip().upper() == 'DEFAULT'):\n        print_info('This testcase will be executed using the default InputDataFile')\n        datafile = get_default_xml_datafile(self.filepath)\n    elif (str(datafile).strip().upper() == 'NO_DATA'):\n        print_info('This test case will be run without any InputDataFile')\n        datafile = 'NO_DATA'\n    elif ((datafile is not None) and (datafile is not False)):\n        datafile_rel = str(datafile).strip()\n        datafile = file_Utils.getAbsPath(datafile_rel, os.path.dirname(self.filepath))\n    if ((str(datafile).strip().upper() != 'NO_DATA') and (datafile is not False)):\n        if (not file_Utils.fileExists(datafile)):\n            print_info('\\n')\n            print_error('!!! *** InputDataFile does not exist in provided path:{0} *** !!!'.format(datafile))\n    return datafile",
    "nl": "Check InputDatFile tag in the xml file and\nbased on the values return the datafile to be used for the testcase/testsuite\n    - If user provided a datafile, will use it.\n    - If user specified 'Default' will use the default datafile\n    - If user did not provide any value will use default datafile\n    - If user specified 'NODATA' will print a msg saying so.",
    "original_nl": "Check InputDatFile tag in the xml file and\nbased on the values return the datafile to be used for the testcase/testsuite\n    - If user provided a datafile, will use it.\n    - If user specified 'Default' will use the default datafile\n    - If user did not provide any value will use default datafile\n    - If user specified 'NODATA' will print a msg saying so."
  },
  {
    "code": "def test_mult_ds_area(self):\n    from satpy.composites import CompositeBase\n    ds1 = self._get_test_ds()\n    ds2 = self._get_test_ds()\n    comp = CompositeBase('test_comp')\n    ret_datasets = comp.check_areas((ds1, ds2))\n    self.assertIs(ret_datasets[0], ds1)\n    self.assertIs(ret_datasets[1], ds2)",
    "nl": "Test multiple datasets successfully pass.",
    "original_nl": "Test multiple datasets successfully pass."
  },
  {
    "code": "def adjust_recursive_directory_permissions(pre_existing_dir, new_directory_list, module, directory_args, changed):\n    if (len(new_directory_list) > 0):\n        working_dir = os.path.join(pre_existing_dir, new_directory_list.pop(0))\n        directory_args['path'] = working_dir\n        changed = module.set_fs_attributes_if_different(directory_args, changed)\n        changed = adjust_recursive_directory_permissions(working_dir, new_directory_list, module, directory_args, changed)\n    return changed",
    "nl": "Walk the new directories list and make sure that permissions are as we would expect",
    "original_nl": "Walk the new directories list and make sure that permissions are as we would expect"
  },
  {
    "code": "def num_unique_categories(venue_collection):\n    unique_categories = set()\n    venues = venue_collection.find()\n    for venue in venues:\n        unique_categories.add(venue_primary_category_extractor(venue)[1][0])\n    return unique_categories",
    "nl": "Counts total number of unique categories.\n",
    "original_nl": "Counts total number of unique categories.\n:param venue_collection: MongoDB collection that contains the venues\n:return: a set of containing unique categories"
  },
  {
    "code": "def biselect(table, *args, **kwargs):\n    kwargs['complement'] = False\n    t1 = select(table, *args, **kwargs)\n    kwargs['complement'] = True\n    t2 = select(table, *args, **kwargs)\n    return (t1, t2)",
    "nl": "Return two tables, the first containing selected rows, the second\ncontaining remaining rows. E.g.::\n\n    >>> import petl as etl\n    >>> table1 = [['foo', 'bar', 'baz'],\n    ...           ['a', 4, 9.3],\n    ...           ['a', 2, 88.2],\n    ...           ['b', 1, 23.3],\n    ...           ['c', 8, 42.0],\n    ...           ['d', 7, 100.9],\n    ...           ['c', 2]]\n    >>> table2, table3 = etl.biselect(table1, lambda rec: rec.foo == 'a')\n    >>> table2\n    +-----+-----+------+\n    | foo | bar | baz  |\n    +=====+=====+======+\n    | 'a' |   4 |  9.3 |\n    +-----+-----+------+\n    | 'a' |   2 | 88.2 |\n    +-----+-----+------+\n    >>> table3\n    +-----+-----+-------+\n    | foo | bar | baz   |\n    +=====+=====+=======+\n    | 'b' |   1 |  23.3 |\n    +-----+-----+-------+\n    | 'c' |   8 |  42.0 |\n    +-----+-----+-------+\n    | 'd' |   7 | 100.9 |\n    +-----+-----+-------+\n    | 'c' |   2 |       |\n    +-----+-----+-------+\n\n.. versionadded:: 1.1.0",
    "original_nl": "Return two tables, the first containing selected rows, the second\ncontaining remaining rows. E.g.::\n\n    >>> import petl as etl\n    >>> table1 = [['foo', 'bar', 'baz'],\n    ...           ['a', 4, 9.3],\n    ...           ['a', 2, 88.2],\n    ...           ['b', 1, 23.3],\n    ...           ['c', 8, 42.0],\n    ...           ['d', 7, 100.9],\n    ...           ['c', 2]]\n    >>> table2, table3 = etl.biselect(table1, lambda rec: rec.foo == 'a')\n    >>> table2\n    +-----+-----+------+\n    | foo | bar | baz  |\n    +=====+=====+======+\n    | 'a' |   4 |  9.3 |\n    +-----+-----+------+\n    | 'a' |   2 | 88.2 |\n    +-----+-----+------+\n    >>> table3\n    +-----+-----+-------+\n    | foo | bar | baz   |\n    +=====+=====+=======+\n    | 'b' |   1 |  23.3 |\n    +-----+-----+-------+\n    | 'c' |   8 |  42.0 |\n    +-----+-----+-------+\n    | 'd' |   7 | 100.9 |\n    +-----+-----+-------+\n    | 'c' |   2 |       |\n    +-----+-----+-------+\n\n.. versionadded:: 1.1.0"
  },
  {
    "code": "def get_path_to_toplevel_modules(filename):\n    curr_dir = os.path.dirname(os.path.abspath(filename))\n    pattern = '__init__.py'\n    try:\n        for i in range(10):\n            files = set(os.listdir(curr_dir))\n            if (pattern in files):\n                curr_dir = os.path.dirname(curr_dir)\n            else:\n                return curr_dir\n    except IOError:\n        pass\n    return None",
    "nl": "Return the path to top-level directory that contains Python modules.\n\nIt will look in parent directories for __init__.py files. The first parent\ndirectory without __init__.py is the top-level directory.\n",
    "original_nl": "Return the path to top-level directory that contains Python modules.\n\nIt will look in parent directories for __init__.py files. The first parent\ndirectory without __init__.py is the top-level directory.\n\nReturned directory might be used to extend the PYTHONPATH."
  },
  {
    "code": "@property\ndef accountable_date(self):\n    fecha_transaccion = self.data['TBK_FECHA_CONTABLE']\n    m = int(fecha_transaccion[:2])\n    d = int(fecha_transaccion[2:])\n    santiago = pytz.timezone('America/Santiago')\n    today = santiago.localize(datetime.datetime.today())\n    year = today.year\n    if ((self.paid_at.month == 12) and (m == 1)):\n        year += 1\n    santiago_dt = santiago.localize(datetime.datetime(year, m, d))\n    return santiago_dt",
    "nl": "Accountable date of transaction, localized as America/Santiago",
    "original_nl": "Accountable date of transaction, localized as America/Santiago"
  },
  {
    "code": "def updateControlItem(self, cgi):\n    s = self.settings\n    pointsX = list(s.xPos)\n    pointsY = list(s.yPos)\n    ind = cgi.index\n    (xpos, ypos) = self._getGraphCoords(cgi.widgetposn, (cgi.deltacrosspos[0] + cgi.posn[0]), (cgi.deltacrosspos[1] + cgi.posn[1]))\n    (xposd, yposd) = self._getGraphCoords(cgi.widgetposn, ((cgi.deltacrosspos[0] + cgi.posn[0]) + 1), ((cgi.deltacrosspos[1] + cgi.posn[1]) + 1))\n    if ((xpos is None) or (ypos is None)):\n        return\n    roundx = utils.round2delt(xpos, xposd)\n    roundy = utils.round2delt(ypos, yposd)\n    (pointsX[ind], pointsY[ind]) = (roundx, roundy)\n    operations = (document.OperationSettingSet(s.get('xPos'), pointsX), document.OperationSettingSet(s.get('yPos'), pointsY))\n    self.document.applyOperation(document.OperationMultiple(operations, descr=_('move label')))",
    "nl": "Update position of point given new name and vals.",
    "original_nl": "Update position of point given new name and vals."
  },
  {
    "code": "def __init__(self, *args, **kwargs):\n    super(EnrollmentTransaction, self).__init__(*args, **kwargs)\n    self.Type = TransactionType.EnrollmentTransaction",
    "nl": "Create an instance.\n\nArgs:\n    *args:\n    **kwargs:",
    "original_nl": "Create an instance.\n\nArgs:\n    *args:\n    **kwargs:"
  },
  {
    "code": "@classmethod\ndef sort_menus(c):\n    for name in c.items:\n        if (not c.sorted[name]):\n            c.items[name].sort(key=(lambda x: x.weight))\n            c.sorted[name] = True",
    "nl": "sort_menus goes through the items and sorts them based on\ntheir weight",
    "original_nl": "sort_menus goes through the items and sorts them based on\ntheir weight"
  },
  {
    "code": "def test_default_config(self):\n    cfg_manager = ConfigurationManager.ConfigurationManager()\n    vmexp = VMExperiment.VMExperiment(None, None, cfg_manager)\n    self.assertNotEqual(vmexp.url, None)\n    self.assertTrue(vmexp.url.__contains__('localhost'))\n    self.assertTrue(vmexp.should_store_image, True)\n    self.assertEqual(vmexp.vm_type, 'VirtualMachineDummy')\n    self.assertEqual(vmexp.user_manager_type, 'DummyUserManager')\n    vm = vmexp.vm\n    vmexp.load_user_manager()\n    um = vmexp.user_manager\n    self.assertIs(type(vm), VirtualMachineDummy.VirtualMachineDummy)\n    self.assertIs(type(um), user_manager.DummyUserManager.DummyUserManager)\n    self.assertFalse(vmexp.is_ready)\n    self.assertFalse(vmexp.is_error)",
    "nl": "Tests that the ctor works and sets proper defaults with a blank configuration manager.",
    "original_nl": "Tests that the ctor works and sets proper defaults with a blank configuration manager."
  },
  {
    "code": "@mock.patch('django.core.files.storage.Storage.save')\ndef test_check_fails(self, m_mock):\n    m_mock.side_effect = Exception('Boom')\n    informer = StorageInformer()\n    self.assertRaises(InformerException, informer.check_availability)",
    "nl": "Test if with 'broken scenario', all goes bad",
    "original_nl": "Test if with 'broken scenario', all goes bad"
  },
  {
    "code": "def create_def_exec_dir(self):\n    if (self.ws_execution is None):\n        execdir = self.create_ws_execution()\n    else:\n        execdir = self.ws_execution\n    return execdir",
    "nl": "Create the default result execution directory",
    "original_nl": "Create the default result execution directory"
  },
  {
    "code": "def _has_undefined_space():\n    return VLAN.objects.filter(space__isnull=True).exists()",
    "nl": "Returns True if the undefined space contains at least one VLAN.",
    "original_nl": "Returns True if the undefined space contains at least one VLAN."
  },
  {
    "code": "def start(self):\n    assert (not self._sock_service)\n    service = Gio.SocketService.new()\n    try:\n        service.add_inet_port(self._port, None)\n    except GLib.GError as e:\n        raise ServerError(e)\n    except OverflowError as e:\n        raise ServerError(('port: %s' % e))\n    self._id = service.connect('incoming', self._incoming_connection_cb)\n    service.start()\n    self._sock_service = service",
    "nl": "Start accepting connections.\n\nMay raise ServerError.",
    "original_nl": "Start accepting connections.\n\nMay raise ServerError."
  },
  {
    "code": "def get_meta(self, table_name, constraints, column_to_field_name):\n    unique_together = []\n    for (index, params) in constraints.items():\n        if params['unique']:\n            columns = params['columns']\n            if (len(columns) > 1):\n                tup = (('(' + ', '.join(((\"'%s'\" % column_to_field_name[c]) for c in columns))) + ')')\n                unique_together.append(tup)\n    meta = ['', '    class Meta:', '        managed = False', (\"        db_table = '%s'\" % table_name)]\n    if unique_together:\n        tup = (('(' + ', '.join(unique_together)) + ',)')\n        meta += [('        unique_together = %s' % tup)]\n    return meta",
    "nl": "Return a sequence comprising the lines of code necessary\nto construct the inner Meta class for the model corresponding\nto the given database table name.",
    "original_nl": "Return a sequence comprising the lines of code necessary\nto construct the inner Meta class for the model corresponding\nto the given database table name."
  },
  {
    "code": "def test_strReturnsStr(self):\n    self.assertEqual(type(self.path.__str__()), str)",
    "nl": "Calling C{str()} with a L{URLPath} will always return a L{str}.",
    "original_nl": "Calling C{str()} with a L{URLPath} will always return a L{str}."
  },
  {
    "code": "@api.multi\ndef get_hold_gifts(self):\n    return reduce((lambda x, y: (x and y)), self.mapped('project_id.hold_gifts'))",
    "nl": ":return: True if all children's gift are held.",
    "original_nl": ":return: True if all children's gift are held."
  },
  {
    "code": "def insideout(ds):\n    ds = list(ds)\n    result = dict([(k, []) for k in ds[0].keys()])\n    for d in ds:\n        for (k, v) in d.items():\n            result[k].append(v)\n    return result",
    "nl": "Transform a list of dictionaries to a dictionary of lists.",
    "original_nl": "Transform a list of dictionaries to a dictionary of lists."
  },
  {
    "code": "def _get_from_context(self, ctx):\n    return (ctx['active_model'], ctx['active_ids'])",
    "nl": "Gets the active_model and active_ids attributes from context\n\n",
    "original_nl": "Gets the active_model and active_ids attributes from context\n\n:param ctx (dict): context that will be processed\n:return: active_model, active_ids"
  },
  {
    "code": "def add_requested_columns(args, update_cursor, col_names, col_types=None):\n    if (args.anno_type in ['count', 'boolean']):\n        col_name = col_names[0]\n        col_type = 'integer'\n        try:\n            alter_qry = ((((('ALTER TABLE variants ADD COLUMN ' + col_name) + ' ') + col_type) + ' ') + 'DEFAULT NULL')\n            update_cursor.execute(sql.text(alter_qry))\n        except sql.exc.OperationalError:\n            sys.stderr.write((('WARNING: Column \"(' + col_name) + ')\" already exists in variants table. Overwriting values.\\n'))\n            update_cursor.execute((('UPDATE variants SET ' + col_name) + ' = NULL WHERE 1'))\n    elif (args.anno_type == 'extract'):\n        for (col_name, col_type) in zip(col_names, col_types):\n            try:\n                alter_qry = ((((('ALTER TABLE variants ADD COLUMN ' + col_name) + ' ') + col_type) + ' ') + 'DEFAULT NULL')\n                update_cursor.execute(alter_qry)\n            except sql.exc.OperationalError:\n                sys.stderr.write((('WARNING: Column \"(' + col_name) + ')\" already exists in variants table. Overwriting values.\\n'))\n    else:\n        raise ValueError(('Unknown annotation type: %s\\n' % args.anno_type))",
    "nl": "Attempt to add new, user-defined columns to the\nvariants table.  Warn if the column already exists.",
    "original_nl": "Attempt to add new, user-defined columns to the\nvariants table.  Warn if the column already exists."
  },
  {
    "code": "def __init__(self, column_names=None, title=None):\n    super(BaseTableView, self).__init__()\n    self._columns = (column_names or [])\n    self._number_of_columns = len(self._columns)\n    self._rows = []\n    self._title = title",
    "nl": "Initializes a table view.\n\nArgs:\n  column_names (Optional[list[str]]): column names.\n  title (Optional[str]): title.",
    "original_nl": "Initializes a table view.\n\nArgs:\n  column_names (Optional[list[str]]): column names.\n  title (Optional[str]): title."
  },
  {
    "code": "def balance(slack_user_id, synapse_user, params):\n    user = User.from_slack_id(slack_user_id)\n    savings_node = Node.by_id(user=synapse_user, id=user.savings_node_id)\n    if savings_node:\n        balance = format_currency(savings_node.balance)\n        return '```Savings: {0}```'.format(balance)\n    else:\n        return '*No savings node found for user.*'",
    "nl": "Return balance on SYNAPSE-US savings account.",
    "original_nl": "Return balance on SYNAPSE-US savings account."
  },
  {
    "code": "def get_start_index_of_section(self, start_content):\n    matches = list(self.bracket_expression.finditer(start_content))\n    if (len(matches) > 0):\n        last_match = matches[(- 1)]\n        return last_match.start()\n    else:\n        return 0",
    "nl": "Return the index of the section.\n\nIf no section is found the first index (0) is returned",
    "original_nl": "Return the index of the section.\n\nIf no section is found the first index (0) is returned"
  },
  {
    "code": "def write_to_cache(self, data, filename):\n    json_data = self.json_format_dict(data, True)\n    cache = open(filename, 'w')\n    cache.write(json_data)\n    cache.close()",
    "nl": "Writes data in JSON format to a file",
    "original_nl": "Writes data in JSON format to a file"
  },
  {
    "code": "def pytest_addoption(parser):\n    group = parser.getgroup('Test duration', 'plugin test duration')\n    group.addoption('--test_duration', action='store', default=None, help=\"Set time to control long-run tests where is used 'duration' fixture.E.g. --test_duration=30s, --test_duration=1.5H\")",
    "nl": "Plugin specific options.",
    "original_nl": "Plugin specific options."
  },
  {
    "code": "def _create_template(self, num_instances, num_replace=0, template_version=('heat_template_version', '2015-04-30')):\n    return super(AutoScalingResourceGroup, self)._create_template(num_instances, num_replace, template_version=template_version)",
    "nl": "Create a template in the HOT format for the nested stack.",
    "original_nl": "Create a template in the HOT format for the nested stack."
  },
  {
    "code": "def vertical_table(data, headers, sep_title='{n}. row', sep_character='*', sep_length=27):\n    header_len = max([len(x) for x in headers])\n    padded_headers = [x.ljust(header_len) for x in headers]\n    formatted_rows = [_format_row(padded_headers, row) for row in data]\n    output = []\n    for (i, result) in enumerate(formatted_rows):\n        (yield (_get_separator(i, sep_title, sep_character, sep_length) + result))",
    "nl": "Format *data* and *headers* as an vertical table.\n\nThe values in *data* and *headers* must be strings.\n\n",
    "original_nl": "Format *data* and *headers* as an vertical table.\n\nThe values in *data* and *headers* must be strings.\n\n:param iterable data: An :term:`iterable` (e.g. list) of rows.\n:param iterable headers: The column headers.\n:param str sep_title: The title given to each row separator. Defaults to\n                      ``'{n}. row'``. Any instance of ``'{n}'`` is\n                      replaced by the record number.\n:param str sep_character: The character used to separate rows. Defaults to\n                          ``'*'``.\n:param int/tuple sep_length: The number of separator characters that should\n                             appear on each side of the *sep_title*. Use\n                             a tuple to specify the left and right values\n                             separately.\n:return: The formatted data.\n:rtype: str"
  },
  {
    "code": "def split_pow_tgh(self, text):\n    return [n for n in re.split('/(?=([^{}]*{[^{}]*})*[^{}]*$)', text) if (n is not None)][:2]",
    "nl": "Split a power/toughness string on the correct slash.\n\nCorrectly accounts for curly braces to denote fractions.\nE.g., '2/2' --> ['2', '2']\n'3{1/2}/3{1/2}' --> ['3{1/2}', '3{1/2}']",
    "original_nl": "Split a power/toughness string on the correct slash.\n\nCorrectly accounts for curly braces to denote fractions.\nE.g., '2/2' --> ['2', '2']\n'3{1/2}/3{1/2}' --> ['3{1/2}', '3{1/2}']"
  },
  {
    "code": "def test_array_coordinates_distances():\n    ICRS(ra=(np.array([1, 2]) * u.deg), dec=(np.array([3, 4]) * u.deg), distance=([0.1, 0.2] * u.kpc))\n    with pytest.raises(ValueError):\n        ICRS(ra=(np.array([1, 2, 3]) * u.deg), dec=(np.array([[3, 4], [5, 6]]) * u.deg), distance=(2.0 * u.kpc))\n    with pytest.raises(ValueError):\n        ICRS(ra=(np.array([1, 2]) * u.deg), dec=(np.array([3, 4]) * u.deg), distance=([0.1, 0.2, 3.0] * u.kpc))",
    "nl": "Test creating coordinates from arrays and distances.",
    "original_nl": "Test creating coordinates from arrays and distances."
  },
  {
    "code": "def post(self, request, bot_id, format=None):\n    return super(MessengerChatStateList, self).post(request, bot_id, format)",
    "nl": "Add a new chat state\n---\nserializer: MessengerChatStateSerializer\nresponseMessages:\n    - code: 401\n      message: Not authenticated\n    - code: 400\n      message: Not valid request",
    "original_nl": "Add a new chat state\n---\nserializer: MessengerChatStateSerializer\nresponseMessages:\n    - code: 401\n      message: Not authenticated\n    - code: 400\n      message: Not valid request"
  },
  {
    "code": "def _get_tags_used_in_content(app_label=None, model=None):\n    tag_parser = HTMLTagParser()\n    queryset = FieldSanitizer.objects.all()\n    if (app_label and model):\n        queryset = queryset.filter(content_type__app_label=app_label, content_type__model=model)\n    for fs in queryset:\n        model_class = fs.content_type.model_class()\n        for content in model_class.objects.values_list(fs.field_name, flat=True):\n            tag_parser.feed(content)\n    return sorted(list(tag_parser.tags))",
    "nl": "Use html5lib's parser to get a list of HTML tags used in content\nassociated with a FieldSanitizer.\n\nThis can be useful when determining what to include in a whitelist,\nand is used in the `list_html_elements` and\n`list_html_elements_for_model` management commands.",
    "original_nl": "Use html5lib's parser to get a list of HTML tags used in content\nassociated with a FieldSanitizer.\n\nThis can be useful when determining what to include in a whitelist,\nand is used in the `list_html_elements` and\n`list_html_elements_for_model` management commands."
  },
  {
    "code": "def show(tournament, match, attachment):\n    return api.fetch_and_parse('GET', ('tournaments/%s/matches/%s/attachments/%s' % (tournament, match, attachment)))",
    "nl": "Retrieve a single match attachment record.",
    "original_nl": "Retrieve a single match attachment record."
  },
  {
    "code": "def add_model_score_header(head):\n    add_metadata(head, 'info', 'ModelScore', annotation_number='1', entry_type='Integer', description='PHRED score for genotype models.')\n    return",
    "nl": "Add Model Score to vcf header",
    "original_nl": "Add Model Score to vcf header"
  },
  {
    "code": "def get_view_url(self):\n    return self.get_categorization_object().get_absolute_url()",
    "nl": "Return object view url. Used in `get_translated_url` templatetag from parler.",
    "original_nl": "Return object view url. Used in `get_translated_url` templatetag from parler."
  },
  {
    "code": "def write(file_or_filename, pymm_element):\n    if (not isinstance(pymm_element, element.BaseElement)):\n        raise ValueError('pymm.write requires file/filename, then pymm element')\n    et_elem = encode(pymm_element)\n    xmltree = ET.ElementTree(et_elem)\n    xmltree.write(file_or_filename)",
    "nl": "Writes mindmap/element to file. Element must be pymm element.\nWill write element and children hierarchy to file.\nWriting any element to file works, but in order to be opened\nin Freeplane, the Mindmap element should be passed.\n\n",
    "original_nl": "Writes mindmap/element to file. Element must be pymm element.\nWill write element and children hierarchy to file.\nWriting any element to file works, but in order to be opened\nin Freeplane, the Mindmap element should be passed.\n\n:param mm_element: Mindmap or other pymm element\n:param file_or_filename: string path to file or file instance\n    of mindmap (.mm)\n:return:"
  },
  {
    "code": "def render_sparkline(self, **kwargs):\n    spark_options = dict(width=200, height=50, show_dots=False, show_legend=False, show_x_labels=False, show_y_labels=False, spacing=0, margin=5, min_scale=1, max_scale=2, explicit_size=True, no_data_text='', js=(), classes=(_ellipsis, 'pygal-sparkline'))\n    spark_options.update(kwargs)\n    return self.render(**spark_options)",
    "nl": "Render a sparkline",
    "original_nl": "Render a sparkline"
  },
  {
    "code": "def isActive(self, index):\n    return self._iget_active(index)",
    "nl": "@rtype: bool",
    "original_nl": "@rtype: bool"
  },
  {
    "code": "def __repr__(self) -> str:\n    return self.to_json()",
    "nl": "Get the debug representation of the robot model.\n\n",
    "original_nl": "Get the debug representation of the robot model.\n\n:return:"
  },
  {
    "code": "def _get_anchor_data(anchor_data, ufo, components, anchor_name):\n    anchors = []\n    for component in components:\n        for anchor in ufo[component.baseGlyph].anchors:\n            if (anchor.name == anchor_name):\n                anchors.append((anchor, component))\n                break\n    if (len(anchors) > 1):\n        for (i, (anchor, component)) in enumerate(anchors):\n            t = Transform(*component.transformation)\n            name = ('%s_%d' % (anchor.name, (i + 1)))\n            anchor_data[name] = t.transformPoint((anchor.x, anchor.y))\n    elif anchors:\n        (anchor, component) = anchors[0]\n        t = Transform(*component.transformation)\n        anchor_data[anchor.name] = t.transformPoint((anchor.x, anchor.y))",
    "nl": "Get data for an anchor from a list of components.",
    "original_nl": "Get data for an anchor from a list of components."
  },
  {
    "code": "def testParseOptions(self):\n    options = cli_test_lib.TestOptions()\n    options.filter = 'event.timestamp == 0'\n    test_tool = tools.CLITool()\n    event_filters.EventFiltersArgumentsHelper.ParseOptions(options, test_tool)\n    self.assertEqual(test_tool._event_filter_expression, options.filter)\n    self.assertIsNotNone(test_tool._event_filter)\n    with self.assertRaises(errors.BadConfigObject):\n        event_filters.EventFiltersArgumentsHelper.ParseOptions(options, None)",
    "nl": "Tests the ParseOptions function.",
    "original_nl": "Tests the ParseOptions function."
  },
  {
    "code": "def t_paren(self, s):\n    self.add_token(BRACKET2NAME[s], s)",
    "nl": "[(){}[\\]]",
    "original_nl": "[(){}[\\]]"
  },
  {
    "code": "def getCubicPathByBeginEnd(begin, controlPoints, elementNode, end):\n    return svg_reader.getCubicPoints(begin, controlPoints, end, lineation.getNumberOfBezierPoints(begin, elementNode, end))",
    "nl": "Get the cubic path by begin and end.",
    "original_nl": "Get the cubic path by begin and end."
  },
  {
    "code": "def SetNodesInfos(self, values):\n    ida_idaapi.pygc_set_nodes_infos(self, values)",
    "nl": "Set the properties for the given nodes.\n\nExample usage (set first three nodes's bg color to purple):\n  inst = ...\n  p = idaapi.node_info_t()\n  p.bg_color = 0x00ff00ff\n  inst.SetNodesInfos({0 : p, 1 : p, 2 : p})\n\n@param values: A dictionary of 'int -> node_info_t' objects.",
    "original_nl": "Set the properties for the given nodes.\n\nExample usage (set first three nodes's bg color to purple):\n  inst = ...\n  p = idaapi.node_info_t()\n  p.bg_color = 0x00ff00ff\n  inst.SetNodesInfos({0 : p, 1 : p, 2 : p})\n\n@param values: A dictionary of 'int -> node_info_t' objects."
  }
]